{"guide": {"name": "creating-a-chatbot-fast", "category": "chatbots", "pretty_category": "Chatbots", "guide_index": 1, "absolute_index": 13, "pretty_name": "Creating A Chatbot Fast", "content": "# How to Create a Chatbot with Gradio Fast\n\nTags: NLP, TEXT, CHAT\n\n## Introduction\n\nChatbots are a popular application of large language models. Using `gradio`, you can easily build a demo of your chatbot model and share that with your users, or try it yourself using an intuitive chatbot UI.\n\nThis tutorial uses `gr.ChatInterface()`, which is a high-level abstraction that allows you to create your chatbot UI fast, often with a single line of code. The chatbot interface that we create will look something like this:\n\n<gradio-app space='gradio/chatinterface_streaming_echo'></gradio-app>\n\nWe'll start with a couple of simple examples, and then show how to use `gr.ChatInterface()` with real language models from several popular APIs and libraries, including `langchain`, `openai`, and Hugging Face. \n\n**Prerequisites**: please make sure you are using the **latest version** version of Gradio:  \n\n```bash\n$ pip install --upgrade gradio\n```\n\n## Defining a chat function\n\nWhen working with `gr.ChatInterface()`, the first thing you should do is define your chat function. Your chat function should take two arguments: `message` and then `history` (the arguments can be named anything, but must be in this order)\n\n* `message`: a `str` representing the user's input\n* `history`: a `list` of `list` representing the conversations up until that point. Each inner list consists of two `str` representing a pair: `[user input, bot response]`. \n\nYour function should return a single string response, which is the bot's response to the particular user input `message`. Your function can take into account the `history` of messages, as well as the current message.\n\nLet's take a look at a few examples.\n\n## Example: a chatbot that responds yes or no\n\nLet's write a chat function that responds `Yes` or `No` randomly.\n\nHere's our chat function:\n\n```py\nimport random\n\ndef random_response(message, history):\n    return random.choice([\"Yes\", \"No\"])\n```\n\nNow, we can plug this into `gr.ChatInterface()` and call the `.launch()` method to create the web interface:\n\n```py\nimport gradio as gr\n\ngr.ChatInterface(random_response).launch()\n```\n\nThat's it! Here's our running demo, try it out:\n\n<gradio-app space='gradio/chatinterface_random_response'></gradio-app>\n\n## Another example using the user's input and history\n\nOf course, the previous example was very simplistic, it didn't even take user input or the previous history into account! Here's a pretty simple example showing how to incorporate a user's input as well as the history.\n\n```py\nimport random\nimport gradio as gr\n\ndef alternatingly_agree(message, history):\n    if len(history) % 2 == 0:\n        return f\"Yes, I do think that '{message}'\"\n    else:\n        return \"I don't think so\"\n\ngr.ChatInterface(alternatingly_agree).launch()\n```\n\n## Streaming chatbots \n\nIf in your chat function, if you use `yield` to generate a sequence of responses, you'll end up with a streaming chatbot. It's that simple!\n\n```py\nimport time\nimport gradio as gr\n\ndef slow_echo(message, history):\n    for i in range(len(message)):\n        time.sleep(0.3)\n        yield \"You typed: \" + message[: i+1]\n\ngr.ChatInterface(slow_echo).queue().launch()\n```\n\nNotice that we've [enabled queuing](), which is required to use generator functions.\n\n## Customizing your chatbot\n\nIf you're familiar with Gradio's `Interface` class, the `gr.ChatInterface` includes many of the same arguments that you can use to customize the look and feel of your Chatbot. For example, you can:\n\n* add a `title` and `description` above your chatbot using `title` and `description` arguments\n* add a theme or custom css using `theme` and `css` arguments respectively\n* add `examples` and even enable `cache_examples`, which make it easier for users to try it out \n* You can change the text or disable each of the buttons that appear in the chatbot interface: `submit_btn`, `retry_btn`, `delete_last_btn`, `clear_btn`\n\nIf you want to customize the `gr.Chatbot` or `gr.Textbox` that compose the `ChatInterface`, then you can pass in your own chatbot or textbox as well. Here's an example of how we can use these parameters:\n\nIf you need to create something even more custom, then its best to construct the chatbot UI using the low-level `gr.Blocks()` API. We have [a dedicated guide for that here](/).\n\nHere's a complete example using the parameters above:\n\n```py\nimport gradio as gr\n\ndef yes_man(message, history):\n    if messages.endswith(\"?\"):\n        return \"Yes\"\n    else:\n        return \"Ask me anything!\"\n\ngr.ChatInterface(\n    yes_man,\n    chatbot=gr.Chatbot(height=300),\n    textbox=gr.Textbox(placeholder=\"Ask me a yes or no question\")\n    title=\"Yes Man\",\n    description=\"Ask Yes Man any question\",\n    theme=\"soft\",\n    examples=[\"Hello\", \"Am I cool?\", \"Are tomatoes vegetables?\"],\n    cache_examples=True,\n    retry_btn=False,\n    delete_last_btn=\"Delete\",\n    clear_btn=\"Clear\",\n).launch()\n```\n\n## Using your chatbot via an API\n\nOnce you've built your Gradio chatbot and are hosting it on [Hugging Face Spaces](https://hf.space) or somewhere else, then you can query it with a simple API at the `/chat` endpoint. The endpoint just expects the user's message, and will return the response, internally keeping track of the messages sent so far.\n\nINSERT SCREENSHOT\n\nTo use the endpoint, you should use either the Gradio Python Client or the Gradio JS client.\n\n## A `langchain` example\n\nNow, let's actually use the `gr.ChatInterface` with some real large language models. We'll start by using `langchain` on top of `openai` to build a general-purpose streaming chatbot application in 19 lines of code. You'll need to have an OpenAI key for this example (keep reading for the free, open-source equivalent!)\n\n```py\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage\nimport openai\nimport gradio as gr\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nllm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-0613')\n\ndef predict(message, history):\n    history_langchain_format = []\n    for human, ai in history:\n        history_langchain_format.append(HumanMessage(content=human))\n        history_langchain_format.append(AIMessage(content=ai))\n    history_langchain_format.append(HumanMessage(content=message))\n    gpt_response = chat(history_langchain_format)\n    return gpt_response.content\n\ngr.ChatInterface(predict).launch() \n```\n\n## A streaming example using `openai`\n\nOf course, we could also use the `openai` library directy. Here a similar example, but this time with streaming results as well:\n\n\n```py\nimport openai\nimport gradio as gr\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ndef predict(message, history):\n    history_openai_format = []\n    for human, assistant in history:\n        history_openai_format.append({\"role\": \"user\", \"content\": human })\n        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n    history_openai_format.append({\"role\": \"user\", \"content\": message})\n\n    response = openai.ChatCompletion.create(\n        model='gpt-3.5-turbo',\n        messages= history_openai_format,         \n        temperature=1.0,\n        stream=True\n    )\n    \n    partial_message = \"\"\n    for chunk in response:\n        if len(chunk['choices'][0]['delta']) != 0:\n            partial_message = partial_message + chunk['choices'][0]['delta']['content']\n            yield partial_message \n\ngr.ChatInterface(predict).queue().launch() \n```\n\n## Examples using open-source LLMs with Hugging Face!\n\nOf course, why use a closed-source model when you can use an open-source one instead? Here's the equivalent example using Hugging Face's StarChat model, which is primarily designed as a coding assistant.\n\n```py\nfrom text_generation import Client, DialogueTemplate\n\nsystem_message = \"Below is a conversation between a human user and a helpful AI coding assistant.\"\nclient = Client(\"https://api-inference.huggingface.co/models/HuggingFaceH4/starchat-beta\")\n\ndef predict(message, history):    \n    history_starchat_format = []\n    for user, assistant in history:\n        history_starchat_format.append({\"role\": \"user\", \"content\": user})\n        history_starchat_format.append({\"role\": \"assistant\", \"content\": assistant.rstrip()})        \n    history_starchat_format.append({\"role\": \"user\", \"content\": message})\n    \n    dialogue_template = DialogueTemplate(system=system_message, messages=history_starchat_format)\n    generate_kwargs = dict(\n        temperature=1.0,\n        max_new_tokens=1024,\n        top_p=0.95,\n        repetition_penalty=1.2,\n        do_sample=True,\n        truncate=4096,\n        seed=42,\n        stop_sequences=[\"<|end|>\"],\n    )\n    stream = client.generate_stream(\n        prompt,\n        **generate_kwargs,\n    )\n    \n    output = \"\"\n    for idx, response in enumerate(stream):\n        if response.token.special:\n            continue\n        output += response.token.text\n        yield output\n\ngr.ChatInterface(predict).queue().launch()\n```\n\nWith those examples, you should be all set to create your own Gradio Chatbot demos soon! For building more custom Chabot UI, check out [a dedicated guide](/) using the low-level `gr.Blocks()` API.", "html": "<h1 id=\"how-to-create-a-chatbot-with-gradio-fast\">How to Create a Chatbot with Gradio Fast</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Chatbots are a popular application of large language models. Using <code>gradio</code>, you can easily build a demo of your chatbot model and share that with your users, or try it yourself using an intuitive chatbot UI.</p>\n\n<p>This tutorial uses <code>gr.ChatInterface()</code>, which is a high-level abstraction that allows you to create your chatbot UI fast, often with a single line of code. The chatbot interface that we create will look something like this:</p>\n\n<p><gradio-app space='gradio/chatinterface<em>streaming</em>echo'></gradio-app></p>\n\n<p>We'll start with a couple of simple examples, and then show how to use <code>gr.ChatInterface()</code> with real language models from several popular APIs and libraries, including <code>langchain</code>, <code>openai</code>, and Hugging Face. </p>\n\n<p><strong>Prerequisites</strong>: please make sure you are using the <strong>latest version</strong> version of Gradio:  </p>\n\n<div class='codeblock'><pre><code class='lang-bash'>$ pip install --upgrade gradio\n</code></pre></div>\n\n<h2 id=\"defining-a-chat-function\">Defining a chat function</h2>\n\n<p>When working with <code>gr.ChatInterface()</code>, the first thing you should do is define your chat function. Your chat function should take two arguments: <code>message</code> and then <code>history</code> (the arguments can be named anything, but must be in this order)</p>\n\n<ul>\n<li><code>message</code>: a <code>str</code> representing the user's input</li>\n<li><code>history</code>: a <code>list</code> of <code>list</code> representing the conversations up until that point. Each inner list consists of two <code>str</code> representing a pair: <code>[user input, bot response]</code>. </li>\n</ul>\n\n<p>Your function should return a single string response, which is the bot's response to the particular user input <code>message</code>. Your function can take into account the <code>history</code> of messages, as well as the current message.</p>\n\n<p>Let's take a look at a few examples.</p>\n\n<h2 id=\"example-a-chatbot-that-responds-yes-or-no\">Example: a chatbot that responds yes or no</h2>\n\n<p>Let's write a chat function that responds <code>Yes</code> or <code>No</code> randomly.</p>\n\n<p>Here's our chat function:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import random\n\ndef random_response(message, history):\n    return random.choice([\"Yes\", \"No\"])\n</code></pre></div>\n\n<p>Now, we can plug this into <code>gr.ChatInterface()</code> and call the <code>.launch()</code> method to create the web interface:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import gradio as gr\n\ngr.ChatInterface(random_response).launch()\n</code></pre></div>\n\n<p>That's it! Here's our running demo, try it out:</p>\n\n<p><gradio-app space='gradio/chatinterface<em>random</em>response'></gradio-app></p>\n\n<h2 id=\"another-example-using-the-users-input-and-history\">Another example using the user's input and history</h2>\n\n<p>Of course, the previous example was very simplistic, it didn't even take user input or the previous history into account! Here's a pretty simple example showing how to incorporate a user's input as well as the history.</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import random\nimport gradio as gr\n\ndef alternatingly_agree(message, history):\n    if len(history) % 2 == 0:\n        return f\"Yes, I do think that '{message}'\"\n    else:\n        return \"I don't think so\"\n\ngr.ChatInterface(alternatingly_agree).launch()\n</code></pre></div>\n\n<h2 id=\"streaming-chatbots\">Streaming chatbots</h2>\n\n<p>If in your chat function, if you use <code>yield</code> to generate a sequence of responses, you'll end up with a streaming chatbot. It's that simple!</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import time\nimport gradio as gr\n\ndef slow_echo(message, history):\n    for i in range(len(message)):\n        time.sleep(0.3)\n        yield \"You typed: \" + message[: i+1]\n\ngr.ChatInterface(slow_echo).queue().launch()\n</code></pre></div>\n\n<p>Notice that we've <a href=\"\">enabled queuing</a>, which is required to use generator functions.</p>\n\n<h2 id=\"customizing-your-chatbot\">Customizing your chatbot</h2>\n\n<p>If you're familiar with Gradio's <code>Interface</code> class, the <code>gr.ChatInterface</code> includes many of the same arguments that you can use to customize the look and feel of your Chatbot. For example, you can:</p>\n\n<ul>\n<li>add a <code>title</code> and <code>description</code> above your chatbot using <code>title</code> and <code>description</code> arguments</li>\n<li>add a theme or custom css using <code>theme</code> and <code>css</code> arguments respectively</li>\n<li>add <code>examples</code> and even enable <code>cache_examples</code>, which make it easier for users to try it out </li>\n<li>You can change the text or disable each of the buttons that appear in the chatbot interface: <code>submit_btn</code>, <code>retry_btn</code>, <code>delete_last_btn</code>, <code>clear_btn</code></li>\n</ul>\n\n<p>If you want to customize the <code>gr.Chatbot</code> or <code>gr.Textbox</code> that compose the <code>ChatInterface</code>, then you can pass in your own chatbot or textbox as well. Here's an example of how we can use these parameters:</p>\n\n<p>If you need to create something even more custom, then its best to construct the chatbot UI using the low-level <code>gr.Blocks()</code> API. We have <a rel=\"noopener\" target=\"_blank\" href=\"/\">a dedicated guide for that here</a>.</p>\n\n<p>Here's a complete example using the parameters above:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import gradio as gr\n\ndef yes_man(message, history):\n    if messages.endswith(\"?\"):\n        return \"Yes\"\n    else:\n        return \"Ask me anything!\"\n\ngr.ChatInterface(\n    yes_man,\n    chatbot=gr.Chatbot(height=300),\n    textbox=gr.Textbox(placeholder=\"Ask me a yes or no question\")\n    title=\"Yes Man\",\n    description=\"Ask Yes Man any question\",\n    theme=\"soft\",\n    examples=[\"Hello\", \"Am I cool?\", \"Are tomatoes vegetables?\"],\n    cache_examples=True,\n    retry_btn=False,\n    delete_last_btn=\"Delete\",\n    clear_btn=\"Clear\",\n).launch()\n</code></pre></div>\n\n<h2 id=\"using-your-chatbot-via-an-api\">Using your chatbot via an API</h2>\n\n<p>Once you've built your Gradio chatbot and are hosting it on <a rel=\"noopener\" target=\"_blank\" href=\"https://hf.space\">Hugging Face Spaces</a> or somewhere else, then you can query it with a simple API at the <code>/chat</code> endpoint. The endpoint just expects the user's message, and will return the response, internally keeping track of the messages sent so far.</p>\n\n<p>INSERT SCREENSHOT</p>\n\n<p>To use the endpoint, you should use either the Gradio Python Client or the Gradio JS client.</p>\n\n<h2 id=\"a-langchain-example\">A <code>langchain</code> example</h2>\n\n<p>Now, let's actually use the <code>gr.ChatInterface</code> with some real large language models. We'll start by using <code>langchain</code> on top of <code>openai</code> to build a general-purpose streaming chatbot application in 19 lines of code. You'll need to have an OpenAI key for this example (keep reading for the free, open-source equivalent!)</p>\n\n<div class='codeblock'><pre><code class='lang-py'>from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage\nimport openai\nimport gradio as gr\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nllm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-0613')\n\ndef predict(message, history):\n    history_langchain_format = []\n    for human, ai in history:\n        history_langchain_format.append(HumanMessage(content=human))\n        history_langchain_format.append(AIMessage(content=ai))\n    history_langchain_format.append(HumanMessage(content=message))\n    gpt_response = chat(history_langchain_format)\n    return gpt_response.content\n\ngr.ChatInterface(predict).launch() \n</code></pre></div>\n\n<h2 id=\"a-streaming-example-using-openai\">A streaming example using <code>openai</code></h2>\n\n<p>Of course, we could also use the <code>openai</code> library directy. Here a similar example, but this time with streaming results as well:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import openai\nimport gradio as gr\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ndef predict(message, history):\n    history_openai_format = []\n    for human, assistant in history:\n        history_openai_format.append({\"role\": \"user\", \"content\": human })\n        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n    history_openai_format.append({\"role\": \"user\", \"content\": message})\n\n    response = openai.ChatCompletion.create(\n        model='gpt-3.5-turbo',\n        messages= history_openai_format,         \n        temperature=1.0,\n        stream=True\n    )\n\n    partial_message = \"\"\n    for chunk in response:\n        if len(chunk['choices'][0]['delta']) != 0:\n            partial_message = partial_message + chunk['choices'][0]['delta']['content']\n            yield partial_message \n\ngr.ChatInterface(predict).queue().launch() \n</code></pre></div>\n\n<h2 id=\"examples-using-open-source-llms-with-hugging-face\">Examples using open-source LLMs with Hugging Face!</h2>\n\n<p>Of course, why use a closed-source model when you can use an open-source one instead? Here's the equivalent example using Hugging Face's StarChat model, which is primarily designed as a coding assistant.</p>\n\n<div class='codeblock'><pre><code class='lang-py'>from text_generation import Client, DialogueTemplate\n\nsystem_message = \"Below is a conversation between a human user and a helpful AI coding assistant.\"\nclient = Client(\"https://api-inference.huggingface.co/models/HuggingFaceH4/starchat-beta\")\n\ndef predict(message, history):    \n    history_starchat_format = []\n    for user, assistant in history:\n        history_starchat_format.append({\"role\": \"user\", \"content\": user})\n        history_starchat_format.append({\"role\": \"assistant\", \"content\": assistant.rstrip()})        \n    history_starchat_format.append({\"role\": \"user\", \"content\": message})\n\n    dialogue_template = DialogueTemplate(system=system_message, messages=history_starchat_format)\n    generate_kwargs = dict(\n        temperature=1.0,\n        max_new_tokens=1024,\n        top_p=0.95,\n        repetition_penalty=1.2,\n        do_sample=True,\n        truncate=4096,\n        seed=42,\n        stop_sequences=[\"<|end|>\"],\n    )\n    stream = client.generate_stream(\n        prompt,\n        **generate_kwargs,\n    )\n\n    output = \"\"\n    for idx, response in enumerate(stream):\n        if response.token.special:\n            continue\n        output += response.token.text\n        yield output\n\ngr.ChatInterface(predict).queue().launch()\n</code></pre></div>\n\n<p>With those examples, you should be all set to create your own Gradio Chatbot demos soon! For building more custom Chabot UI, check out <a rel=\"noopener\" target=\"_blank\" href=\"/\">a dedicated guide</a> using the low-level <code>gr.Blocks()</code> API.</p>\n", "tags": ["NLP", "TEXT", "CHAT"], "spaces": [], "url": "/guides/creating-a-chatbot-fast/", "contributor": null}}