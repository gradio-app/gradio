[{"category": "\ud83d\udd8a\ufe0f Text & Natural Language Processing", "demos": [{"name": "Hello World", "dir": "hello_world", "code": "import gradio as gr\n\n# defining the core function\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\n# defining a text-to-text interface \ndemo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n    \n# launching the interface \ndemo.launch()   ", "space_url": "https://huggingface.co/spaces/gradio/hello_world", "text": "The simplest possible Gradio demo. It wraps a 'Hello {name}!' function in an Interface that accepts and returns text."}, {"name": "Text Generation", "dir": "text_generation", "code": "# imports\nimport gradio as gr\nfrom transformers import pipeline\n\n# loading the model\ngenerator = pipeline('text-generation', model='gpt2')\n\n\n# defining the core function\ndef generate(text):\n    result = generator(text, max_length=30, num_return_sequences=1)\n    return result[0][\"generated_text\"]\n\n\n# defining examples\nexamples = [\n    [\"The Moon's orbit around Earth has\"],\n    [\"The smooth Borealis basin in the Northern Hemisphere covers 40%\"],\n]\n\n# defining the interface\ndemo = gr.Interface(\n    fn=generate,\n    inputs=gr.inputs.Textbox(lines=5, label=\"Input Text\"),\n    outputs=gr.outputs.Textbox(label=\"Generated Text\"),\n    examples=examples\n)\n\n# launching\ndemo.launch()\n", "space_url": "https://huggingface.co/spaces/gradio/text_generation", "text": "This text generation demo takes in input text and returns generated text. It uses the Transformers library to set up the model and has two examples."}, {"name": "Autocomplete", "dir": "autocomplete", "code": "# imports\nimport gradio as gr\n\n# load a model from https://hf.co/models as an interface, then use it as an api \napi = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\")\n\n# define the core function\ndef complete_with_gpt(text):\n    # Use the last 50 characters of the text as context\n    return text[:-50] + api(text[-50:])\n\n# start a block\nwith gr.Blocks() as demo:\n    # define the UI: one textbox, and a button\n    textbox = gr.Textbox(placeholder=\"Type here...\", lines=4)\n    btn = gr.Button(\"Autocomplete\")\n    \n    # define what will run when the button is clicked\n    # the textbox is used as both an input and an output\n    btn.click(fn=complete_with_gpt, inputs=textbox, outputs=textbox, queue=False)\n\n# launch    \ndemo.launch()", "space_url": "https://huggingface.co/spaces/gradio/autocomplete", "text": "This text generation demo works like autocomplete. There's only one textbox and it's used for both the input and the output. The demo loads the model as an interface, and uses that interface as an API. It then uses blocks to create the UI. All of this is done in less than 10 lines of code."}, {"name": "Sentiment Analysis", "dir": "sentiment_analysis", "code": "# imports\nimport gradio as gr\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# load and set up model\nnltk.download(\"vader_lexicon\")\nsid = SentimentIntensityAnalyzer()\n\n# define core function \ndef sentiment_analysis(text):\n    scores = sid.polarity_scores(text)\n    del scores[\"compound\"]\n    return scores\n\n# define an interface with a textbox input, a label output, and interpretation\ndemo = gr.Interface(\n    fn=sentiment_analysis, \n    inputs=gr.Textbox(placeholder=\"Enter a positive or negative sentence here...\"), \n    outputs=\"label\", \n    interpretation=\"default\",\n    examples=[[\"This is wonderful!\"]])\n\n# launch\ndemo.launch()", "space_url": "https://huggingface.co/spaces/gradio/sentiment-analysis", "text": "This sentiment analaysis demo takes in input text and returns its classification for either positive, negative or neutral using Gradio's Label output. It also uses the default interpretation method so users can click the Interpret button after a submission and see which words had the biggest effect on the output."}, {"name": "Named Entity Recognition", "dir": "text_analysis", "code": "# imports\nimport gradio as gr\nimport os\nos.system('python -m spacy download en_core_web_sm')\nimport spacy\nfrom spacy import displacy\n\n# load the model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# define the core function\ndef text_analysis(text):\n    doc = nlp(text)\n    html = displacy.render(doc, style=\"dep\", page=True)\n    html = (\n        \"<div style='max-width:100%; max-height:360px; overflow:auto'>\"\n        + html\n        + \"</div>\"\n    )\n    pos_count = {\n        \"char_count\": len(text),\n        \"token_count\": 0,\n    }\n    pos_tokens = []\n\n    for token in doc:\n        pos_tokens.extend([(token.text, token.pos_), (\" \", None)])\n\n    return pos_tokens, pos_count, html\n\n# define the interface, with a textbox input, and three outputs: HighlightedText, JSON, and HTML\ndemo = gr.Interface(\n    text_analysis,\n    gr.Textbox(placeholder=\"Enter sentence here...\"),\n    [\"highlight\", \"json\", \"html\"],\n    examples=[\n        [\"What a beautiful morning for a walk!\"],\n        [\"It was the best of times, it was the worst of times.\"],\n    ],\n)\n\n# launch\ndemo.launch()\n", "space_url": "https://huggingface.co/spaces/gradio/text_analysis", "text": "This simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear NER segmentation."}, {"name": "Multilingual Translation", "dir": "translation", "code": "# imports \nimport gradio as gr\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\nimport torch\n\n# loading the model (from https://hf.co/models) and defining constants\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\ndevice = 0 if torch.cuda.is_available() else -1\nLANGS = [\"ace_Arab\", \"eng_Latn\", \"fra_Latn\", \"spa_Latn\"]\n\n# defining the core function\ndef translate(text, src_lang, tgt_lang):\n    \"\"\"\n    Translate the text from source lang to target lang\n    \"\"\"\n    translation_pipeline = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=src_lang, tgt_lang=tgt_lang, max_length=400, device=device)\n    result = translation_pipeline(text)\n    return result[0]['translation_text']\n\n# defining an interface that accepts text, the source and target languages, and returns the translation \ndemo = gr.Interface(\n    fn=translate,\n    inputs=[\n        gr.components.Textbox(label=\"Text\"),\n        gr.components.Dropdown(label=\"Source Language\", choices=LANGS),\n        gr.components.Dropdown(label=\"Target Language\", choices=LANGS),\n    ],\n    outputs=[\"text\"],\n    examples=[[\"Building a translation demo with Gradio is so easy!\", \"eng_Latn\", \"spa_Latn\"]],\n    cache_examples=False,\n    title=\"Translation Demo\",\n    description=\"This demo is a simplified version of the original [NLLB-Translator](https://huggingface.co/spaces/Narrativaai/NLLB-Translator) space\"\n)\n\n# launching the demo\ndemo.launch()", "space_url": "https://huggingface.co/spaces/gradio/translation", "text": "This translation demo takes in the text, source and target languages, and returns the translation. It uses the Transformers library to set up the model and has a title, description, and example."}]}, {"category": "\ud83d\uddbc\ufe0f Images & Computer Vision", "demos": [{"name": "Image Classification", "dir": "image_classification", "code": "# imports\nimport gradio as gr\nimport torch\nimport requests\nfrom torchvision import transforms\n\n# load the model \nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n\n# download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n# define core function\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}    \n  return confidences\n\n# define the interface \ndemo = gr.Interface(fn=predict, \n             inputs=gr.inputs.Image(type=\"pil\"),\n             outputs=gr.outputs.Label(num_top_classes=3),\n             examples=[[\"cheetah.jpg\"]],\n             )\n             \n# launch\ndemo.launch()", "space_url": "https://huggingface.co/spaces/abidlabs/image_classification", "text": "Simple image classification in Pytorch with Gradio's Image input and Label output."}, {"name": "Image Segmentation", "dir": "image_segmentation", "code": "# imports\nimport gradio as gr\nimport torch\nimport random\nimport numpy as np\nfrom transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\n\n\n# load model\ndevice = torch.device(\"cpu\")\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-tiny-ade\").to(device)\nmodel.eval()\npreprocessor = MaskFormerFeatureExtractor.from_pretrained(\"facebook/maskformer-swin-tiny-ade\")\n\n# define core and helper fns\ndef visualize_instance_seg_mask(mask):\n    image = np.zeros((mask.shape[0], mask.shape[1], 3))\n    labels = np.unique(mask)\n    label2color = {label: (random.randint(0, 1), random.randint(0, 255), random.randint(0, 255)) for label in labels}\n    for i in range(image.shape[0]):\n      for j in range(image.shape[1]):\n        image[i, j, :] = label2color[mask[i, j]]\n    image = image / 255\n    return image\n\ndef query_image(img):\n    target_size = (img.shape[0], img.shape[1])\n    inputs = preprocessor(images=img, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n    outputs.class_queries_logits = outputs.class_queries_logits.cpu()\n    outputs.masks_queries_logits = outputs.masks_queries_logits.cpu()\n    results = preprocessor.post_process_segmentation(outputs=outputs, target_size=target_size)[0].cpu().detach()\n    results = torch.argmax(results, dim=0).numpy()\n    results = visualize_instance_seg_mask(results)\n    return results\n\n# define interface \n\ndemo = gr.Interface(\n    query_image, \n    inputs=[gr.Image()], \n    outputs=\"image\",\n    title=\"MaskFormer Demo\",\n    examples=[[\"example_2.png\"]]\n)\n\n# launch \ndemo.launch()", "space_url": "https://huggingface.co/spaces/gradio/image_segmentation/", "text": "Image segmentation using DETR. Takes in both an inputu image and the desired confidence, and returns a segmented image."}, {"name": "Image Transformation with AnimeGAN", "dir": "animeganv2", "code": "# imports\nimport gradio as gr\nfrom PIL import Image\nimport torch\n\n# load the models\nmodel2 = torch.hub.load(\n    \"AK391/animegan2-pytorch:main\",\n    \"generator\",\n    pretrained=True,\n    progress=False\n)\nmodel1 = torch.hub.load(\"AK391/animegan2-pytorch:main\", \"generator\", pretrained=\"face_paint_512_v1\")\nface2paint = torch.hub.load(\n    'AK391/animegan2-pytorch:main', 'face2paint', \n    size=512,side_by_side=False\n)\n\n# define the core function\ndef inference(img, ver):\n    if ver == 'version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)':\n        out = face2paint(model2, img)\n    else:\n        out = face2paint(model1, img)\n    return out\n\n# define the title, description and examples\ntitle = \"AnimeGANv2\"\ndescription = \"Gradio Demo for AnimeGanv2 Face Portrait. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below. Please use a cropped portrait picture for best results similar to the examples below.\"\narticle = \"<p style='text-align: center'><a href='https://github.com/bryandlee/animegan2-pytorch' target='_blank'>Github Repo Pytorch</a></p> <center><img src='https://visitor-badge.glitch.me/badge?page_id=akhaliq_animegan' alt='visitor badge'></center></p>\"\nexamples=[['groot.jpeg','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'],['gongyoo.jpeg','version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)']]\n\n# define the interface\ndemo = gr.Interface(\n    fn=inference, \n    inputs=[gr.inputs.Image(type=\"pil\"),gr.inputs.Radio(['version 1 (\ud83d\udd3a stylization, \ud83d\udd3b robustness)','version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)'], type=\"value\", default='version 2 (\ud83d\udd3a robustness,\ud83d\udd3b stylization)', label='version')], \n    outputs=gr.outputs.Image(type=\"pil\"),\n    title=title,\n    description=description,\n    article=article,\n    examples=examples)\n\n# launch    \ndemo.launch()", "space_url": "https://huggingface.co/spaces/gradio/animeganv2", "text": "Recreate the viral AnimeGAN image transformation demo."}, {"name": "Image Generation with Stable Diffusion", "dir": "stable-diffusion", "external_space": "stabilityai/stable-diffusion", "code": "# imports\nimport gradio as gr\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\nfrom datasets import load_dataset\nfrom PIL import Image  \nimport re\nimport os\n\nauth_token = os.getenv(\"auth_token\")\n\n# load the model\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cpu\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=auth_token, revision=\"fp16\", torch_dtype=torch.float16)\npipe = pipe.to(device)\n\n# define the core function\ndef infer(prompt, samples, steps, scale, seed):        \n    generator = torch.Generator(device=device).manual_seed(seed)\n    images_list = pipe(\n        [prompt] * samples,\n        num_inference_steps=steps,\n        guidance_scale=scale,\n        generator=generator,\n    )\n    images = []\n    safe_image = Image.open(r\"unsafe.png\")\n    for i, image in enumerate(images_list[\"sample\"]):\n        if(images_list[\"nsfw_content_detected\"][i]):\n            images.append(safe_image)\n        else:\n            images.append(image)\n    return images\n    \n# define a block\nblock = gr.Blocks()\n\n\n# start the block\nwith block:\n    # define the layout\n    with gr.Group():\n        with gr.Box():\n            with gr.Row().style(mobile_collapse=False, equal_height=True):\n                # define the input textbox\n                text = gr.Textbox(\n                    label=\"Enter your prompt\",\n                    show_label=False,\n                    max_lines=1,\n                    placeholder=\"Enter your prompt\",\n                ).style(\n                    border=(True, False, True, True),\n                    rounded=(True, False, False, True),\n                    container=False,\n                )\n                # define the button\n                btn = gr.Button(\"Generate image\").style(\n                    margin=False,\n                    rounded=(False, True, True, False),\n                )\n        # define the output gallery\n        gallery = gr.Gallery(\n            label=\"Generated images\", show_label=False, elem_id=\"gallery\"\n        ).style(grid=[2], height=\"auto\")\n\n        # define the advanced button\n        advanced_button = gr.Button(\"Advanced options\", elem_id=\"advanced-btn\")\n\n        # define the advanced section sliders\n        with gr.Row(elem_id=\"advanced-options\"):\n            samples = gr.Slider(label=\"Images\", minimum=1, maximum=4, value=4, step=1)\n            steps = gr.Slider(label=\"Steps\", minimum=1, maximum=50, value=45, step=1)\n            scale = gr.Slider(\n                label=\"Guidance Scale\", minimum=0, maximum=50, value=7.5, step=0.1\n            )\n            seed = gr.Slider(\n                label=\"Seed\",\n                minimum=0,\n                maximum=2147483647,\n                step=1,\n                randomize=True,\n            )\n\n        # define what will happen when the buttons are selected\n        text.submit(infer, inputs=[text, samples, steps, scale, seed], outputs=gallery)\n        btn.click(infer, inputs=[text, samples, steps, scale, seed], outputs=gallery)\n        advanced_button.click(\n            None,\n            [],\n            text,\n        )\n        \n# launch\nblock.launch()", "space_url": "https://huggingface.co/spaces/gradio/stable-diffusion", "text": "Note: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main"}, {"name": "Iterative Output", "dir": "fake_diffusion", "code": "# imports\nimport gradio as gr\nimport numpy as np\nimport time\n\n# define core fn, which returns a generator {steps} times before returning the image\ndef fake_diffusion(steps):\n    for _ in range(steps):\n        time.sleep(1)\n        image = np.random.random((600, 600, 3))\n        yield image\n\n    image = \"https://i.picsum.photos/id/867/600/600.jpg?hmac=qE7QFJwLmlE_WKI7zMH6SgH5iY5fx8ec6ZJQBwKRT44\" \n    yield image\n\n# define Interface\ndemo = gr.Interface(fake_diffusion, \n                    inputs=gr.Slider(1, 10, 3), \n                    outputs=\"image\")\n\n# define queue - required for generators\ndemo.queue()\n\n# launch\ndemo.launch()", "space_url": "https://huggingface.co/spaces/gradio/fake_diffusion", "text": "This demo uses a fake model to showcase iterative output. The Image output will update every time a generator is returned until the final image."}, {"name": "3D Models", "dir": "depth_estimation", "code": "# imports\nimport gradio as gr\nfrom transformers import DPTFeatureExtractor, DPTForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport open3d as o3d\nfrom pathlib import Path\nimport os\n\n# load the model\nfeature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-large\")\nmodel = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n\n# define the core and helper functions\ndef process_image(image_path):\n    image_path = Path(image_path)\n    image_raw = Image.open(image_path)\n    image = image_raw.resize(\n        (800, int(800 * image_raw.size[1] / image_raw.size[0])),\n        Image.Resampling.LANCZOS)\n\n    # prepare image for the model\n    encoding = feature_extractor(image, return_tensors=\"pt\")\n\n    # forward pass\n    with torch.no_grad():\n        outputs = model(**encoding)\n        predicted_depth = outputs.predicted_depth\n\n    # interpolate to original size\n    prediction = torch.nn.functional.interpolate(\n        predicted_depth.unsqueeze(1),\n        size=image.size[::-1],\n        mode=\"bicubic\",\n        align_corners=False,\n    ).squeeze()\n    output = prediction.cpu().numpy()\n    depth_image = (output * 255 / np.max(output)).astype('uint8')\n    try:\n        gltf_path = create_3d_obj(np.array(image), depth_image, image_path)\n        img = Image.fromarray(depth_image)\n        return [img, gltf_path, gltf_path]\n    except Exception as e:\n        gltf_path = create_3d_obj(\n            np.array(image), depth_image, image_path, depth=8)\n        img = Image.fromarray(depth_image)\n        return [img, gltf_path, gltf_path]\n    except:\n        print(\"Error reconstructing 3D model\")\n        raise Exception(\"Error reconstructing 3D model\")\n\n\ndef create_3d_obj(rgb_image, depth_image, image_path, depth=10):\n    depth_o3d = o3d.geometry.Image(depth_image)\n    image_o3d = o3d.geometry.Image(rgb_image)\n    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n        image_o3d, depth_o3d, convert_rgb_to_intensity=False)\n    w = int(depth_image.shape[1])\n    h = int(depth_image.shape[0])\n\n    camera_intrinsic = o3d.camera.PinholeCameraIntrinsic()\n    camera_intrinsic.set_intrinsics(w, h, 500, 500, w/2, h/2)\n\n    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\n        rgbd_image, camera_intrinsic)\n\n    print('normals')\n    pcd.normals = o3d.utility.Vector3dVector(\n        np.zeros((1, 3)))  # invalidate existing normals\n    pcd.estimate_normals(\n        search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.01, max_nn=30))\n    pcd.orient_normals_towards_camera_location(\n        camera_location=np.array([0., 0., 1000.]))\n    pcd.transform([[1, 0, 0, 0],\n                   [0, -1, 0, 0],\n                   [0, 0, -1, 0],\n                   [0, 0, 0, 1]])\n    pcd.transform([[-1, 0, 0, 0],\n                   [0, 1, 0, 0],\n                   [0, 0, 1, 0],\n                   [0, 0, 0, 1]])\n\n    print('run Poisson surface reconstruction')\n    with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Debug) as cm:\n        mesh_raw, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\n            pcd, depth=depth, width=0, scale=1.1, linear_fit=True)\n\n    voxel_size = max(mesh_raw.get_max_bound() - mesh_raw.get_min_bound()) / 256\n    print(f'voxel_size = {voxel_size:e}')\n    mesh = mesh_raw.simplify_vertex_clustering(\n        voxel_size=voxel_size,\n        contraction=o3d.geometry.SimplificationContraction.Average)\n\n    # vertices_to_remove = densities < np.quantile(densities, 0.001)\n    # mesh.remove_vertices_by_mask(vertices_to_remove)\n    bbox = pcd.get_axis_aligned_bounding_box()\n    mesh_crop = mesh.crop(bbox)\n    gltf_path = f'./{image_path.stem}.gltf'\n    o3d.io.write_triangle_mesh(\n        gltf_path, mesh_crop, write_triangle_uvs=True)\n    return gltf_path\n\n# define the title, description and examples\ntitle = \"Demo: zero-shot depth estimation with DPT + 3D Point Cloud\"\ndescription = \"This demo is a variation from the original <a href='https://huggingface.co/spaces/nielsr/dpt-depth-estimation' target='_blank'>DPT Demo</a>. It uses the DPT model to predict the depth of an image and then uses 3D Point Cloud to create a 3D object.\"\nexamples = [[\"examples/1-jonathan-borba-CgWTqYxHEkg-unsplash.jpg\"]]\n\n# define an interface with one Image input and 3 outputs: Image, Model3D and File\niface = gr.Interface(fn=process_image,\n                     inputs=[gr.Image(\n                         type=\"filepath\", label=\"Input Image\")],\n                     outputs=[gr.Image(label=\"predicted depth\", type=\"pil\"),\n                              gr.Model3D(label=\"3d mesh reconstruction\", clear_color=[\n                                                 1.0, 1.0, 1.0, 1.0]),\n                              gr.File(label=\"3d gLTF\")],\n                     title=title,\n                     description=description,\n                     examples=examples,\n                     allow_flagging=\"never\",\n                     cache_examples=False)\n\n# launch\niface.launch(debug=True, enable_queue=False)", "space_url": "https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-obj", "text": "A demo for predicting the depth of an image and generating a 3D model of it."}]}, {"category": "\ud83d\udcc8 Tabular Data & Plots", "demos": [{"name": "Outbreak Forecast", "dir": "outbreak_forecast", "code": "# imports\nimport gradio as gr\nfrom math import sqrt\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.express as px\nimport pandas as pd\n\n\n# define core fn\ndef outbreak(plot_type, r, month, countries, social_distancing):\n    months = [\"January\", \"February\", \"March\", \"April\", \"May\"]\n    m = months.index(month)\n    start_day = 30 * m\n    final_day = 30 * (m + 1)\n    x = np.arange(start_day, final_day + 1)\n    pop_count = {\"USA\": 350, \"Canada\": 40, \"Mexico\": 300, \"UK\": 120}\n    if social_distancing:\n        r = sqrt(r)\n    df = pd.DataFrame({'day': x})\n    for country in countries:\n        df[country] = ( x ** (r) * (pop_count[country] + 1))\n        \n\n    if plot_type == \"Matplotlib\":\n        fig = plt.figure()\n        plt.plot(df['day'], df[countries].to_numpy())\n        plt.title(\"Outbreak in \" + month)\n        plt.ylabel(\"Cases\")\n        plt.xlabel(\"Days since Day 0\")\n        plt.legend(countries)\n        return fig\n    elif plot_type == \"Plotly\":\n        fig = px.line(df, x='day', y=countries)\n        fig.update_layout(title=\"Outbreak in \" + month,\n                   xaxis_title=\"Cases\",\n                   yaxis_title=\"Days Since Day 0\")\n        return fig\n    else:\n        raise ValueError(\"A plot type must be selected\")\n\n# define inputs\ninputs = [\n        gr.Dropdown([\"Matplotlib\", \"Plotly\"], label=\"Plot Type\"),\n        gr.Slider(1, 4, 3.2, label=\"R\"),\n        gr.Dropdown([\"January\", \"February\", \"March\", \"April\", \"May\"], label=\"Month\"),\n        gr.CheckboxGroup([\"USA\", \"Canada\", \"Mexico\", \"UK\"], label=\"Countries\", \n                         value=[\"USA\", \"Canada\"]),\n        gr.Checkbox(label=\"Social Distancing?\"),\n    ]\n# define output \noutputs = gr.Plot()\n\n# define interface\ndemo = gr.Interface(fn=outbreak, inputs=inputs, outputs=outputs, examples=[\n        [\"Matplotlib\", 2, \"March\", [\"Mexico\", \"UK\"], True],\n        [\"Plotly\", 3.6, \"February\", [\"Canada\", \"Mexico\", \"UK\"], False],\n    ], cache_examples=True)\n\n# launch\ndemo.launch()\n", "space_url": "https://huggingface.co/spaces/gradio/outbreak_forecast", "text": "Generate a plot based on 5 inputs."}, {"name": "Clustering with Scikit-Learn", "dir": "clustering", "code": "# imports\nimport gradio as gr\nimport math\nfrom functools import partial\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import (\n    AgglomerativeClustering, Birch, DBSCAN, KMeans, MeanShift, OPTICS, SpectralClustering, estimate_bandwidth\n)\nfrom sklearn.datasets import make_blobs, make_circles, make_moons\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\n# loading models and setting up \nplt.style.use('seaborn')\nSEED = 0\nMAX_CLUSTERS = 10\nN_SAMPLES = 1000\nN_COLS = 3\nFIGSIZE = 7, 7  # does not affect size in webpage\nCOLORS = [\n    'blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan'\n]\nassert len(COLORS) >= MAX_CLUSTERS, \"Not enough different colors for all clusters\"\nnp.random.seed(SEED)\n\n# defining core fns\n\ndef normalize(X):\n    return StandardScaler().fit_transform(X)\n\n\ndef get_regular(n_clusters):\n    # spiral pattern\n    centers = [\n        [0, 0],\n        [1, 0],\n        [1, 1],\n        [0, 1],\n        [-1, 1],\n        [-1, 0],\n        [-1, -1],\n        [0, -1],\n        [1, -1],\n        [2, -1],\n    ][:n_clusters]\n    assert len(centers) == n_clusters\n    X, labels = make_blobs(n_samples=N_SAMPLES, centers=centers, cluster_std=0.25, random_state=SEED)\n    return normalize(X), labels\n\n\ndef get_circles(n_clusters):\n    X, labels = make_circles(n_samples=N_SAMPLES, factor=0.5, noise=0.05, random_state=SEED)\n    return normalize(X), labels\n\n\ndef get_moons(n_clusters):\n    X, labels = make_moons(n_samples=N_SAMPLES, noise=0.05, random_state=SEED)\n    return normalize(X), labels\n\n\ndef get_noise(n_clusters):\n    np.random.seed(SEED)\n    X, labels = np.random.rand(N_SAMPLES, 2), np.random.randint(0, n_clusters, size=(N_SAMPLES,))\n    return normalize(X), labels\n\n\ndef get_anisotropic(n_clusters):\n    X, labels = make_blobs(n_samples=N_SAMPLES, centers=n_clusters, random_state=170)\n    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n    X = np.dot(X, transformation)\n    return X, labels\n\n\ndef get_varied(n_clusters):\n    cluster_std = [1.0, 2.5, 0.5, 1.0, 2.5, 0.5, 1.0, 2.5, 0.5, 1.0][:n_clusters]\n    assert len(cluster_std) == n_clusters\n    X, labels = make_blobs(\n        n_samples=N_SAMPLES, centers=n_clusters, cluster_std=cluster_std, random_state=SEED\n    )\n    return normalize(X), labels\n\n\ndef get_spiral(n_clusters):\n    # from https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering.html\n    np.random.seed(SEED)\n    t = 1.5 * np.pi * (1 + 3 * np.random.rand(1, N_SAMPLES))\n    x = t * np.cos(t)\n    y = t * np.sin(t)\n    X = np.concatenate((x, y))\n    X += 0.7 * np.random.randn(2, N_SAMPLES)\n    X = np.ascontiguousarray(X.T)\n\n    labels = np.zeros(N_SAMPLES, dtype=int)\n    return normalize(X), labels\n\n\nDATA_MAPPING = {\n    'regular': get_regular,\n    'circles': get_circles,\n    'moons': get_moons,\n    'spiral': get_spiral,\n    'noise': get_noise,\n    'anisotropic': get_anisotropic,\n    'varied': get_varied,\n}\n\n\ndef get_groundtruth_model(X, labels, n_clusters, **kwargs):\n    # dummy model to show true label distribution\n    class Dummy:\n        def __init__(self, y):\n            self.labels_ = labels\n\n    return Dummy(labels)\n\n\ndef get_kmeans(X, labels, n_clusters, **kwargs):\n    model = KMeans(init=\"k-means++\", n_clusters=n_clusters, n_init=10, random_state=SEED)\n    model.set_params(**kwargs)\n    return model.fit(X)\n\n\ndef get_dbscan(X, labels, n_clusters, **kwargs):\n    model = DBSCAN(eps=0.3)\n    model.set_params(**kwargs)\n    return model.fit(X)\n\n\ndef get_agglomerative(X, labels, n_clusters, **kwargs):\n    connectivity = kneighbors_graph(\n        X, n_neighbors=n_clusters, include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n    model = AgglomerativeClustering(\n        n_clusters=n_clusters, linkage=\"ward\", connectivity=connectivity\n    )\n    model.set_params(**kwargs)\n    return model.fit(X)\n\n\ndef get_meanshift(X, labels, n_clusters, **kwargs):\n    bandwidth = estimate_bandwidth(X, quantile=0.25)\n    model = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    model.set_params(**kwargs)\n    return model.fit(X)\n\n\ndef get_spectral(X, labels, n_clusters, **kwargs):\n    model = SpectralClustering(\n        n_clusters=n_clusters,\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n    )\n    model.set_params(**kwargs)\n    return model.fit(X)\n\n\ndef get_optics(X, labels, n_clusters, **kwargs):\n    model = OPTICS(\n        min_samples=7,\n        xi=0.05,\n        min_cluster_size=0.1,\n    )\n    model.set_params(**kwargs)\n    return model.fit(X)\n\n\ndef get_birch(X, labels, n_clusters, **kwargs):\n    model = Birch(n_clusters=n_clusters)\n    model.set_params(**kwargs)\n    return model.fit(X)\n\n\ndef get_gaussianmixture(X, labels, n_clusters, **kwargs):\n    model = GaussianMixture(\n        n_components=n_clusters, covariance_type=\"full\", random_state=SEED,\n    )\n    model.set_params(**kwargs)\n    return model.fit(X)\n\n\nMODEL_MAPPING = {\n    'True labels': get_groundtruth_model,\n    'KMeans': get_kmeans,\n    'DBSCAN': get_dbscan,\n    'MeanShift': get_meanshift,\n    'SpectralClustering': get_spectral,\n    'OPTICS': get_optics,\n    'Birch': get_birch,\n    'GaussianMixture': get_gaussianmixture,\n    'AgglomerativeClustering': get_agglomerative,\n}\n\n\ndef plot_clusters(ax, X, labels):\n    set_clusters = set(labels)\n    set_clusters.discard(-1)  # -1 signifiies outliers, which we plot separately\n    for label, color in zip(sorted(set_clusters), COLORS):\n        idx = labels == label\n        if not sum(idx):\n            continue\n        ax.scatter(X[idx, 0], X[idx, 1], color=color)\n\n    # show outliers (if any)\n    idx = labels == -1\n    if sum(idx):\n        ax.scatter(X[idx, 0], X[idx, 1], c='k', marker='x')\n\n    ax.grid(None)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    return ax\n\n\ndef cluster(dataset: str, n_clusters: int, clustering_algorithm: str):\n    if isinstance(n_clusters, dict):\n        n_clusters = n_clusters['value']\n    else:\n        n_clusters = int(n_clusters)\n\n    X, labels = DATA_MAPPING[dataset](n_clusters)\n    model = MODEL_MAPPING[clustering_algorithm](X, labels, n_clusters=n_clusters)\n    if hasattr(model, \"labels_\"):\n        y_pred = model.labels_.astype(int)\n    else:\n        y_pred = model.predict(X)\n\n    fig, ax = plt.subplots(figsize=FIGSIZE)\n\n    plot_clusters(ax, X, y_pred)\n    ax.set_title(clustering_algorithm, fontsize=16)\n\n    return fig\n\n\ntitle = \"Clustering with Scikit-learn\"\ndescription = (\n    \"This example shows how different clustering algorithms work. Simply pick \"\n    \"the dataset and the number of clusters to see how the clustering algorithms work. \"\n    \"Colored cirles are (predicted) labels and black x are outliers.\"\n)\n\n\ndef iter_grid(n_rows, n_cols):\n    # create a grid using gradio Block\n    for _ in range(n_rows):\n        with gr.Row():\n            for _ in range(n_cols):\n                with gr.Column():\n                    yield\n\n# starting a block\n\nwith gr.Blocks(title=title) as demo:\n    # adding text as HTML and Markdown\n    gr.HTML(f\"<b>{title}</b>\")\n    gr.Markdown(description)\n\n    # setting up the inputs\n    input_models = list(MODEL_MAPPING)\n    input_data = gr.Radio(\n        list(DATA_MAPPING),\n        value=\"regular\",\n        label=\"dataset\"\n    )\n    input_n_clusters = gr.Slider(\n        minimum=1,\n        maximum=MAX_CLUSTERS,\n        value=4,\n        step=1,\n        label='Number of clusters'\n    )\n    n_rows = int(math.ceil(len(input_models) / N_COLS))\n    counter = 0\n    for _ in iter_grid(n_rows, N_COLS):\n        if counter >= len(input_models):\n            break\n\n        input_model = input_models[counter]\n        # defining the output\n        plot = gr.Plot(label=input_model)\n        fn = partial(cluster, clustering_algorithm=input_model)\n        input_data.change(fn=fn, inputs=[input_data, input_n_clusters], outputs=plot)\n        input_n_clusters.change(fn=fn, inputs=[input_data, input_n_clusters], outputs=plot)\n        counter += 1\n\n# launch\ndemo.launch()\n", "space_url": "https://huggingface.co/spaces/gradio/clustering", "text": "This demo built with Blocks generates 9 plots based on the input."}, {"name": "Time Series Forecasting", "dir": "timeseries-forecasting-with-prophet", "code": "# imports\nimport gradio as gr\nimport pypistats\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\nimport pandas as pd\nfrom prophet import Prophet\npd.options.plotting.backend = \"plotly\"\n\n# defining the core fn\ndef get_forecast(lib, time):\n\n    data = pypistats.overall(lib, total=True, format=\"pandas\")\n    data = data.groupby(\"category\").get_group(\"with_mirrors\").sort_values(\"date\")\n    start_date = date.today() - relativedelta(months=int(time.split(\" \")[0]))\n    df = data[(data['date'] > str(start_date))] \n\n    df1 = df[['date','downloads']]\n    df1.columns = ['ds','y']\n\n    m = Prophet()\n    m.fit(df1)\n    future = m.make_future_dataframe(periods=90)\n    forecast = m.predict(future)\n    fig1 = m.plot(forecast)\n    return fig1 \n\n# starting a block\nwith gr.Blocks() as demo:\n    # defining text on the page\n    gr.Markdown(\n    \"\"\"\n    **Pypi Download Stats \ud83d\udcc8 with Prophet Forecasting**: see live download stats for popular open-source libraries \ud83e\udd17 along with a 3 month forecast using Prophet. The [ source code for this Gradio demo is here](https://huggingface.co/spaces/gradio/timeseries-forecasting-with-prophet/blob/main/app.py).\n    \"\"\")\n    # defining layout\n    with gr.Row():\n        # defining inputs\n        lib = gr.Dropdown([\"pandas\", \"scikit-learn\", \"torch\", \"prophet\"], label=\"Library\", value=\"pandas\")\n        time = gr.Dropdown([\"3 months\", \"6 months\", \"9 months\", \"12 months\"], label=\"Downloads over the last...\", value=\"12 months\")\n\n    # defining output\n    plt = gr.Plot()\n\n    # defining when the output will update, and core fn will run: when either input is changed, or demo loads\n    lib.change(get_forecast, [lib, time], plt, queue=False)\n    time.change(get_forecast, [lib, time], plt, queue=False)    \n    demo.load(get_forecast, [lib, time], plt, queue=False)    \n\n# launch\ndemo.launch()", "space_url": "https://huggingface.co/spaces/gradio/timeseries-forecasting-with-prophet", "text": "A simple dashboard showing pypi stats for python libraries. Updates on load, and has no buttons!"}, {"name": "Income Classification with XGBoost", "dir": "xgboost-income-prediction-with-explainability", "code": "# imports\nimport gradio as gr\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport shap\nimport xgboost as xgb\nfrom datasets import load_dataset\n\n\n# loading the model and setting up\nmatplotlib.use(\"Agg\")\ndataset = load_dataset(\"scikit-learn/adult-census-income\")\nX_train = dataset[\"train\"].to_pandas()\n_ = X_train.pop(\"fnlwgt\")\n_ = X_train.pop(\"race\")\ny_train = X_train.pop(\"income\")\ny_train = (y_train == \">50K\").astype(int)\ncategorical_columns = [\n    \"workclass\",\n    \"education\",\n    \"marital.status\",\n    \"occupation\",\n    \"relationship\",\n    \"sex\",\n    \"native.country\",\n]\nX_train = X_train.astype({col: \"category\" for col in categorical_columns})\ndata = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\nmodel = xgb.train(params={\"objective\": \"binary:logistic\"}, dtrain=data)\nexplainer = shap.TreeExplainer(model)\n\n# defining the two core fns\n\ndef predict(*args):\n    df = pd.DataFrame([args], columns=X_train.columns)\n    df = df.astype({col: \"category\" for col in categorical_columns})\n    pos_pred = model.predict(xgb.DMatrix(df, enable_categorical=True))\n    return {\">50K\": float(pos_pred[0]), \"<=50K\": 1 - float(pos_pred[0])}\n\n\ndef interpret(*args):\n    df = pd.DataFrame([args], columns=X_train.columns)\n    df = df.astype({col: \"category\" for col in categorical_columns})\n    shap_values = explainer.shap_values(xgb.DMatrix(df, enable_categorical=True))\n    scores_desc = list(zip(shap_values[0], X_train.columns))\n    scores_desc = sorted(scores_desc)\n    fig_m = plt.figure(tight_layout=True)\n    plt.barh([s[1] for s in scores_desc], [s[0] for s in scores_desc])\n    plt.title(\"Feature Shap Values\")\n    plt.ylabel(\"Shap Value\")\n    plt.xlabel(\"Feature\")\n    plt.tight_layout()\n    return fig_m\n\n\nunique_class = sorted(X_train[\"workclass\"].unique())\nunique_education = sorted(X_train[\"education\"].unique())\nunique_marital_status = sorted(X_train[\"marital.status\"].unique())\nunique_relationship = sorted(X_train[\"relationship\"].unique())\nunique_occupation = sorted(X_train[\"occupation\"].unique())\nunique_sex = sorted(X_train[\"sex\"].unique())\nunique_country = sorted(X_train[\"native.country\"].unique())\n\n# starting the block \n\nwith gr.Blocks() as demo:\n    # defining text on the page\n    gr.Markdown(\"\"\"\n    **Income Classification with XGBoost \ud83d\udcb0**:  This demo uses an XGBoost classifier predicts income based on demographic factors, along with Shapley value-based *explanations*. The [source code for this Gradio demo is here](https://huggingface.co/spaces/gradio/xgboost-income-prediction-with-explainability/blob/main/app.py).\n    \"\"\")\n    # defining the layout\n    with gr.Row():\n        with gr.Column():\n            # defining the inputs\n            age = gr.Slider(label=\"Age\", minimum=17, maximum=90, step=1, randomize=True)\n            work_class = gr.Dropdown(\n                label=\"Workclass\",\n                choices=unique_class,\n                value=lambda: random.choice(unique_class),\n            )\n            education = gr.Dropdown(\n                label=\"Education Level\",\n                choices=unique_education,\n                value=lambda: random.choice(unique_education),\n            )\n            years = gr.Slider(\n                label=\"Years of schooling\",\n                minimum=1,\n                maximum=16,\n                step=1,\n                randomize=True,\n            )\n            marital_status = gr.Dropdown(\n                label=\"Marital Status\",\n                choices=unique_marital_status,\n                value=lambda: random.choice(unique_marital_status),\n            )\n            occupation = gr.Dropdown(\n                label=\"Occupation\",\n                choices=unique_occupation,\n                value=lambda: random.choice(unique_occupation),\n            )\n            relationship = gr.Dropdown(\n                label=\"Relationship Status\",\n                choices=unique_relationship,\n                value=lambda: random.choice(unique_relationship),\n            )\n            sex = gr.Dropdown(\n                label=\"Sex\", choices=unique_sex, value=lambda: random.choice(unique_sex)\n            )\n            capital_gain = gr.Slider(\n                label=\"Capital Gain\",\n                minimum=0,\n                maximum=100000,\n                step=500,\n                randomize=True,\n            )\n            capital_loss = gr.Slider(\n                label=\"Capital Loss\", minimum=0, maximum=10000, step=500, randomize=True\n            )\n            hours_per_week = gr.Slider(\n                label=\"Hours Per Week Worked\", minimum=1, maximum=99, step=1\n            )\n            country = gr.Dropdown(\n                label=\"Native Country\",\n                choices=unique_country,\n                value=lambda: random.choice(unique_country),\n            )\n        with gr.Column():\n            # defining the outputs\n            label = gr.Label()\n            plot = gr.Plot()\n            with gr.Row():\n                # defining the buttons\n                predict_btn = gr.Button(value=\"Predict\")\n                interpret_btn = gr.Button(value=\"Explain\")\n            # defining the fn that will run when predict is clicked, what it will get as inputs, and which output it will update \n            predict_btn.click(\n                predict,\n                inputs=[\n                    age,\n                    work_class,\n                    education,\n                    years,\n                    marital_status,\n                    occupation,\n                    relationship,\n                    sex,\n                    capital_gain,\n                    capital_loss,\n                    hours_per_week,\n                    country,\n                ],\n                outputs=[label],\n            )\n            # defining the fn that will run when interpret is clicked, what it will get as inputs, and which output it will update \n            interpret_btn.click(\n                interpret,\n                inputs=[\n                    age,\n                    work_class,\n                    education,\n                    years,\n                    marital_status,\n                    occupation,\n                    relationship,\n                    sex,\n                    capital_gain,\n                    capital_loss,\n                    hours_per_week,\n                    country,\n                ],\n                outputs=[plot],\n            )\n\n# launch\ndemo.launch()\n", "space_url": "https://huggingface.co/spaces/gradio/xgboost-income-prediction-with-explainability", "text": "This demo takes in 12 inputs from the user in dropdowns and sliders and predicts income. It also has a separate button for explaining the prediction."}, {"name": "Leaderboard", "dir": "leaderboard", "code": "# imports \nimport gradio as gr\nimport requests\nimport pandas as pd\nfrom huggingface_hub.hf_api import SpaceInfo\npath = f\"https://huggingface.co/api/spaces\"\n\n\n# defining the core fn\ndef get_blocks_party_spaces():\n    r = requests.get(path)\n    d = r.json()\n    spaces = [SpaceInfo(**x) for x in d]\n    blocks_spaces = {}\n    for i in range(0,len(spaces)):\n        if spaces[i].id.split('/')[0] == 'Gradio-Blocks' and hasattr(spaces[i], 'likes') and spaces[i].id != 'Gradio-Blocks/Leaderboard' and spaces[i].id != 'Gradio-Blocks/README':\n            blocks_spaces[spaces[i].id]=spaces[i].likes\n    df = pd.DataFrame(\n    [{\"Spaces_Name\": Spaces, \"likes\": likes} for Spaces,likes in blocks_spaces.items()])\n    df = df.sort_values(by=['likes'],ascending=False)\n    return df\n\n# defining the block\nblock = gr.Blocks()\n\n# starting the block\nwith block:    \n    # setting up text on the page\n    gr.Markdown(\"\"\"Leaderboard for the most popular Blocks Event Spaces. To learn more and join, see <a href=\"https://huggingface.co/Gradio-Blocks\" target=\"_blank\" style=\"text-decoration: underline\">Blocks Party Event</a>\"\"\")\n    # defining the layout\n    with gr.Tabs():\n        with gr.TabItem(\"Blocks Party Leaderboard\"):\n            with gr.Row():\n                # defining the output\n                data = gr.outputs.Dataframe(type=\"pandas\")\n            with gr.Row():\n                # defining the button\n                data_run = gr.Button(\"Refresh\")\n                # naming the fn that will run when the button is clicked, and update the output \n                data_run.click(get_blocks_party_spaces, inputs=None, outputs=data)\n    # running the function on page load in addition to when the button is clicked\n    block.load(get_blocks_party_spaces, inputs=None, outputs=data)               \n\n # launch    \nblock.launch()\n\n", "space_url": "https://huggingface.co/spaces/gradio/leaderboard", "text": "A simple dashboard ranking spaces by number of likes."}, {"name": "Tax Calculator", "dir": "tax_calculator", "code": "# imports\nimport gradio as gr\n\n# define core fn\ndef tax_calculator(income, marital_status, assets):\n    tax_brackets = [(10, 0), (25, 8), (60, 12), (120, 20), (250, 30)]\n    total_deductible = sum(assets[\"Cost\"])\n    taxable_income = income - total_deductible\n\n    total_tax = 0\n    for bracket, rate in tax_brackets:\n        if taxable_income > bracket:\n            total_tax += (taxable_income - bracket) * rate / 100\n\n    if marital_status == \"Married\":\n        total_tax *= 0.75\n    elif marital_status == \"Divorced\":\n        total_tax *= 0.8\n\n    return round(total_tax)\n\n# define interface\ndemo = gr.Interface(\n    tax_calculator,\n    [\n        \"number\",\n        gr.Radio([\"Single\", \"Married\", \"Divorced\"]),\n        gr.Dataframe(\n            headers=[\"Item\", \"Cost\"],\n            datatype=[\"str\", \"number\"],\n            label=\"Assets Purchased this Year\",\n        ),\n    ],\n    \"number\",\n    examples=[\n        [10000, \"Married\", [[\"Suit\", 5000], [\"Laptop\", 800], [\"Car\", 1800]]],\n        [80000, \"Single\", [[\"Suit\", 800], [\"Watch\", 1800], [\"Car\", 800]]],\n    ],\n)\n\n# launch\ndemo.launch()\n", "space_url": "https://huggingface.co/spaces/gradio/tax_calculator", "text": "Calculate taxes using Textbox, Radio, and Dataframe components"}]}, {"category": "\ud83c\udfa4 Audio & Speech", "demos": [{"name": "Text to Speech", "dir": "neon-tts-plugin-coqui", "code": "# imports\nimport tempfile\nimport gradio as gr\nfrom neon_tts_plugin_coqui import CoquiTTS\n\n# load the model and set up constants\nLANGUAGES = list(CoquiTTS.langs.keys())\ncoquiTTS = CoquiTTS()\n\n# define core fn \ndef tts(text: str, language: str):\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n        coquiTTS.get_tts(text, fp, speaker = {\"language\" : language})\n        return fp.name\n\n# define inputs and outputs\ninputs = [gr.Textbox(label=\"Input\", value=CoquiTTS.langs[\"en\"][\"sentence\"], max_lines=3), \n            gr.Radio(label=\"Language\", choices=LANGUAGES, value=\"en\")]\noutputs = gr.Audio(label=\"Output\")\n\n# define interface\ndemo = gr.Interface(fn=tts, inputs=inputs, outputs=outputs)\n\n# launch \ndemo.launch()", "space_url": "https://huggingface.co/spaces/gradio/neon-tts-plugin-coqui", "text": "This  demo converts text to speech in 14 languages."}, {"name": "Speech to Text (ASR)", "dir": "automatic-speech-recognition", "code": "# imports\nimport gradio as gr\n\n# automatically load the interface from a HF model \ndemo = gr.Interface.load(\n    \"huggingface/facebook/wav2vec2-base-960h\",\n    title=\"Speech-to-text\",\n    inputs=\"mic\",\n    description=\"Let me try to guess what you're saying!\",\n)\n\n# launch\ndemo.launch()\n", "space_url": "https://huggingface.co/spaces/gradio/automatic-speech-recognition", "text": "Automatic speech recognition in Chinese and English. Uses a tabbed layout to give the user the choice between uploading audio or recording from a microphone."}, {"name": "Musical Instrument Identification", "dir": "musical_instrument_identification", "code": "# imports \nimport gradio as gr\nimport torch, torchaudio\nfrom timeit import default_timer as timer\nfrom data_setups import audio_preprocess, resample\nimport gdown\n\n# download the model and define constants\nurl = 'https://drive.google.com/uc?id=1X5CR18u0I-ZOi_8P0cNptCe5JGk9Ro0C'\noutput = 'piano.wav'\ngdown.download(url, output, quiet=False)\nurl = 'https://drive.google.com/uc?id=1W-8HwmGR5SiyDbUcGAZYYDKdCIst07__'\noutput= 'torch_efficientnet_fold2_CNN.pth'\ngdown.download(url, output, quiet=False)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSAMPLE_RATE = 44100\nAUDIO_LEN = 2.90\nmodel = torch.load(\"torch_efficientnet_fold2_CNN.pth\", map_location=torch.device('cpu'))\nLABELS = [\n    \"Cello\", \"Clarinet\", \"Flute\", \"Acoustic Guitar\", \"Electric Guitar\", \"Organ\", \"Piano\", \"Saxophone\", \"Trumpet\", \"Violin\", \"Voice\"\n]\nexample_list = [\n    [\"piano.wav\"]\n]\n\n\n# define core fn\ndef predict(audio_path):\n    start_time = timer()\n    wavform, sample_rate = torchaudio.load(audio_path)\n    wav = resample(wavform, sample_rate, SAMPLE_RATE)\n    if len(wav) > int(AUDIO_LEN * SAMPLE_RATE):\n        wav = wav[:int(AUDIO_LEN * SAMPLE_RATE)]\n    else:\n        print(f\"input length {len(wav)} too small!, need over {int(AUDIO_LEN * SAMPLE_RATE)}\")\n        return\n    # input Preprocessing\n    img = audio_preprocess(wav, SAMPLE_RATE).unsqueeze(0)\n    model.eval()\n    with torch.inference_mode():\n        pred_probs = torch.softmax(model(img), dim=1)\n    pred_labels_and_probs = {LABELS[i]: float(pred_probs[0][i]) for i in range(len(LABELS))}\n    pred_time = round(timer() - start_time, 5)\n    return pred_labels_and_probs, pred_time\n\n# define interface\ndemo = gr.Interface(fn=predict,\n                    inputs=gr.Audio(type=\"filepath\"),\n                    outputs=[gr.Label(num_top_classes=11, label=\"Predictions\"), \n                             gr.Number(label=\"Prediction time (s)\")],\n                    examples=example_list,\n                    cache_examples=False\n                    )\n\n# launch\ndemo.launch(debug=False)", "space_url": "https://huggingface.co/spaces/gradio/musical_instrument_identification", "text": "This demo identifies musical instruments from an audio file. It uses Gradio's Audio and Label components."}, {"name": "Speaker Verification", "dir": "same-person-or-different", "code": "# imports\nimport gradio as gr\nimport torch\nfrom torchaudio.sox_effects import apply_effects_file\nfrom transformers import AutoFeatureExtractor, AutoModelForAudioXVector\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# define outputs for HTML component\nOUTPUT_OK = (\n    \"\"\"\n    <div class=\"container\">\n        <div class=\"row\"><h1 style=\"text-align: center\">The speakers are</h1></div>\n        <div class=\"row\"><h1 class=\"display-1 text-success\" style=\"text-align: center\">{:.1f}%</h1></div>\n        <div class=\"row\"><h1 style=\"text-align: center\">similar</h1></div>\n        <div class=\"row\"><h1 class=\"text-success\" style=\"text-align: center\">Welcome, human!</h1></div>\n        <div class=\"row\"><small style=\"text-align: center\">(You must get at least 85% to be considered the same person)</small><div class=\"row\">\n    </div>\n\"\"\"\n)\nOUTPUT_FAIL = (\n    \"\"\"\n    <div class=\"container\">\n        <div class=\"row\"><h1 style=\"text-align: center\">The speakers are</h1></div>\n        <div class=\"row\"><h1 class=\"display-1 text-danger\" style=\"text-align: center\">{:.1f}%</h1></div>\n        <div class=\"row\"><h1 style=\"text-align: center\">similar</h1></div>\n        <div class=\"row\"><h1 class=\"text-danger\" style=\"text-align: center\">You shall not pass!</h1></div>\n        <div class=\"row\"><small style=\"text-align: center\">(You must get at least 85% to be considered the same person)</small><div class=\"row\">\n    </div>\n\"\"\"\n)\n\n# load model and define constants\n\nEFFECTS = [\n    [\"remix\", \"-\"],\n    [\"channels\", \"1\"],\n    [\"rate\", \"16000\"],\n    [\"gain\", \"-1.0\"],\n    [\"silence\", \"1\", \"0.1\", \"0.1%\", \"-1\", \"0.1\", \"0.1%\"],\n    [\"trim\", \"0\", \"10\"],\n]\n\nTHRESHOLD = 0.85\n\nmodel_name = \"microsoft/unispeech-sat-base-plus-sv\"\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\nmodel = AutoModelForAudioXVector.from_pretrained(model_name).to(device)\ncosine_sim = torch.nn.CosineSimilarity(dim=-1)\n\n\n# define core fn\ndef similarity_fn(path1, path2):\n    if not (path1 and path2):\n        return '<b style=\"color:red\">ERROR: Please record audio for *both* speakers!</b>'\n\n    wav1, _ = apply_effects_file(path1, EFFECTS)\n    wav2, _ = apply_effects_file(path2, EFFECTS)\n    print(wav1.shape, wav2.shape)\n\n    input1 = feature_extractor(wav1.squeeze(0), return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n    input2 = feature_extractor(wav2.squeeze(0), return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n\n    with torch.no_grad():\n        emb1 = model(input1).embeddings\n        emb2 = model(input2).embeddings\n    emb1 = torch.nn.functional.normalize(emb1, dim=-1).cpu()\n    emb2 = torch.nn.functional.normalize(emb2, dim=-1).cpu()\n    similarity = cosine_sim(emb1, emb2).numpy()[0]\n\n    if similarity >= THRESHOLD:\n        output = OUTPUT_OK.format(similarity * 100)\n    else:\n        output = OUTPUT_FAIL.format(similarity * 100)\n\n    return output\n\n#define inputs, outputs, description, article and examples\ninputs = [\n    gr.inputs.Audio(source=\"microphone\", type=\"filepath\", optional=True, label=\"Speaker #1\"),\n    gr.inputs.Audio(source=\"microphone\", type=\"filepath\", optional=True, label=\"Speaker #2\"),\n]\noutput = gr.outputs.HTML(label=\"\")\n\n\ndescription = (\n    \"This demo from Microsoft will compare two speech samples and determine if they are from the same speaker. \"\n    \"Try it with your own voice!\"\n)\narticle = (\n    \"<p style='text-align: center'>\"\n    \"<a href='https://huggingface.co/microsoft/unispeech-sat-large-sv' target='_blank'>\ud83c\udf99\ufe0f Learn more about UniSpeech-SAT</a> | \"\n    \"<a href='https://arxiv.org/abs/2110.05752' target='_blank'>\ud83d\udcda UniSpeech-SAT paper</a> | \"\n    \"<a href='https://www.danielpovey.com/files/2018_icassp_xvectors.pdf' target='_blank'>\ud83d\udcda X-Vector paper</a>\"\n    \"</p>\"\n)\nexamples = [\n    [\"samples/cate_blanch.mp3\", \"samples/cate_blanch_2.mp3\"],\n    [\"samples/cate_blanch.mp3\", \"samples/heath_ledger.mp3\"],\n]\n# define interface \ninterface = gr.Interface(\n    fn=similarity_fn,\n    inputs=inputs,\n    outputs=output,\n    layout=\"horizontal\",\n    theme=\"huggingface\",\n    allow_flagging=False,\n    live=False,\n    examples=examples,\n    cache_examples=False\n)\ninterface.launch()\n", "space_url": "https://huggingface.co/spaces/gradio/same-person-or-different", "text": "This demo identifies if two speakers are the same person using Gradio's Audio and HTML components."}]}]