{"docs": {"building": {"simplecsvlogger": {"class": null, "name": "SimpleCSVLogger", "description": "A simplified implementation of the FlaggingCallback abstract class provided for illustrative purposes.  Each flagged sample (both the input and output data) is logged to a CSV file on the machine running the gradio app.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}], "returns": {"annotation": null}, "example": "import gradio as gr\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\",\n                    flagging_callback=SimpleCSVLogger())", "fns": [], "parent": "gradio"}, "csvlogger": {"class": null, "name": "CSVLogger", "description": "The default implementation of the FlaggingCallback abstract class. Each flagged sample (both the input and output data) is logged to a CSV file with headers on the machine running the gradio app.", "tags": {"guides": "using-flagging"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}], "returns": {"annotation": null}, "example": "import gradio as gr\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\",\n                    flagging_callback=CSVLogger())", "fns": [], "guides": [{"name": "using-flagging", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 36, "pretty_name": "Using Flagging", "content": "# Using Flagging\n\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n\n## Introduction\n\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.\n\nGradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n\n## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.\n\n* `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.                 \n    * `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n    * `auto`: users will not see a button to flag, but every sample will be flagged automatically. \n    * `never`: users will not see a button to flag, and no sample will be flagged. \n* `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n    * If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n    * If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n    * The chosen option is then logged along with the input and output.\n* `flagging_dir`: this parameter takes a string.\n    * It represents what to name the directory where flagged data is stored.\n* `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n    * Using this parameter allows you to write custom code that gets run when the flag button is clicked\n    * By default, this is set to an instance of `gr.CSVLogger`\n    * One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)\n\n## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CSV file will log the flagged data. \n\nHere's an example: The code below creates the calculator interface embedded below it:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\n\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged. \n\n```directory\n+-- flagged/\n|   +-- logs.csv\n```\n_flagged/logs.csv_\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\n\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n|   +-- image/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n_flagged/logs.csv_\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.\n\nIf we go back to the calculator example, the following code will create the interface embedded below it.  \n```python\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n```\n<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\n\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\n\n_flagged/logs.csv_\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n\n## The HuggingFaceDatasetSaver Callback\n\nSometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data.\n\nWe've made this super easy with the `flagging_callback` parameter.\n\nFor example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:\n\n\n```python\nimport os\n\nHF_TOKEN = os.getenv('HF_TOKEN')\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n```\n\nNotice that we define our own \ninstance of  `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\n\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo! \n\n## Flagging with Blocks\n\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.\n\nAt the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\nThis requires two steps:\n\n1. You have to run your callback's `.setup()` somewhere in the code prior to the \nfirst time you flag data\n2. When the flagging button is clicked, then you trigger the callback's `.flag()` method,\nmaking sure to collect the arguments correctly and disabling the typical preprocessing. \n\nHere is an example with an image sepia filter Blocks demo that lets you flag\ndata using the default `CSVLogger`:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img, strength):\n    sepia_filter = strength * np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]\n    ) + (1-strength) * np.identity(3)\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ncallback = gr.CSVLogger()\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            img_input = gr.Image()\n            strength = gr.Slider(0, 1, 0.5)\n        img_output = gr.Image()\n    with gr.Row():\n        btn = gr.Button(\"Flag\")\n        \n    # This needs to be called at some point prior to the first call to callback.flag()\n    callback.setup([img_input, strength, img_output], \"flagged_data_points\")\n\n    img_input.change(sepia, [img_input, strength], img_output)\n    strength.change(sepia, [img_input, strength], img_output)\n    \n    # We can choose which components to flag -- in this case, we'll flag all of them\n    btn.click(lambda *args: callback.flag(args), [img_input, strength, img_output], None, preprocess=False)\n\ndemo.launch()\n\n```\n<gradio-app space='gradio/blocks_flag'></gradio-app>\n\n## Privacy\n\nImportant Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use `allow_flagging=auto` (when all of the data submitted through the demo is being flagged)\n\n### That's all! Happy building :) \n", "html": "<h1 id=\"using-flagging\">Using Flagging</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>When you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.</p>\n\n<p>Gradio simplifies the collection of this data by including a <strong>Flag</strong> button with every <code>Interface</code>. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with <code>gradio.Interface</code> as well as with <code>gradio.Blocks</code>.</p>\n\n<h2 id=\"the-flag-button-in-gradiointerface\">The <strong>Flag</strong> button in <code>gradio.Interface</code></h2>\n\n<p>Flagging with Gradio's <code>Interface</code> is especially easy. By default, underneath the output components, there is a button marked <strong>Flag</strong>. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.</p>\n\n<p>There are <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#interface-header\">four parameters</a> in <code>gradio.Interface</code> that control how flagging works. We will go over them in greater detail.</p>\n\n<ul>\n<li><code>allow_flagging</code>: this parameter can be set to either <code>\"manual\"</code> (default), <code>\"auto\"</code>, or <code>\"never\"</code>. <br />\n<ul>\n<li><code>manual</code>: users will see a button to flag, and samples are only flagged when the button is clicked.</li>\n<li><code>auto</code>: users will not see a button to flag, but every sample will be flagged automatically. </li>\n<li><code>never</code>: users will not see a button to flag, and no sample will be flagged. </li>\n</ul></li>\n<li><code>flagging_options</code>: this parameter can be either <code>None</code> (default) or a list of strings.\n<ul>\n<li>If <code>None</code>, then the user simply clicks on the <strong>Flag</strong> button and no additional options are shown.</li>\n<li>If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is <code>[\"Incorrect\", \"Ambiguous\"]</code>, then buttons labeled <strong>Flag as Incorrect</strong> and <strong>Flag as Ambiguous</strong> appear. This only applies if <code>allow_flagging</code> is <code>\"manual\"</code>.</li>\n<li>The chosen option is then logged along with the input and output.</li>\n</ul></li>\n<li><code>flagging_dir</code>: this parameter takes a string.\n<ul>\n<li>It represents what to name the directory where flagged data is stored.</li>\n</ul></li>\n<li><code>flagging_callback</code>: this parameter takes an instance of a subclass of the <code>FlaggingCallback</code> class\n<ul>\n<li>Using this parameter allows you to write custom code that gets run when the flag button is clicked</li>\n<li>By default, this is set to an instance of <code>gr.CSVLogger</code></li>\n<li>One example is setting it to an instance of <code>gr.HuggingFaceDatasetSaver</code> which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)</li>\n</ul></li>\n</ul>\n\n<h2 id=\"what-happens-to-flagged-data\">What happens to flagged data?</h2>\n\n<p>Within the directory provided by the <code>flagging_dir</code> argument, a CSV file will log the flagged data. </p>\n\n<p>Here's an example: The code below creates the calculator interface embedded below it:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n</code></pre></div>\n\n<p><gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app></p>\n\n<p>When you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged. </p>\n\n<div class='codeblock'><pre><code class='lang-directory'>+-- flagged/\n|   +-- logs.csv\n</code></pre></div>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>num1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n</code></pre></div>\n\n<p>If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an <code>image</code> input to <code>image</code> output interface will create the following structure.</p>\n\n<div class='codeblock'><pre><code class='lang-directory'>+-- flagged/\n|   +-- logs.csv\n|   +-- image/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n</code></pre></div>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>im,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n</code></pre></div>\n\n<p>If you wish for the user to provide a reason for flagging, you can pass a list of strings to the <code>flagging_options</code> argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.</p>\n\n<p>If we go back to the calculator example, the following code will create the interface embedded below it.  </p>\n\n<div class='codeblock'><pre><code class='lang-python'>iface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n</code></pre></div>\n\n<p><gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app></p>\n\n<p>When users click the flag button, the csv file will now include a column indicating the selected option.</p>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>num1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n</code></pre></div>\n\n<h2 id=\"the-huggingfacedatasetsaver-callback\">The HuggingFaceDatasetSaver Callback</h2>\n\n<p>Sometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data.</p>\n\n<p>We've made this super easy with the <code>flagging_callback</code> parameter.</p>\n\n<p>For example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import os\n\nHF_TOKEN = os.getenv('HF_TOKEN')\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n</code></pre></div>\n\n<p>Notice that we define our own \ninstance of  <code>gradio.HuggingFaceDatasetSaver</code> using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set <code>allow_flagging=\"manual\"</code>\nbecause on Hugging Face Spaces, <code>allow_flagging</code> is set to <code>\"never\"</code> by default. Here's our demo:</p>\n\n<p><gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app></p>\n\n<p>You can now see all the examples flagged above in this <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo\">public Hugging Face dataset</a>.</p>\n\n<p><img src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true\" alt=\"flagging callback hf\" /></p>\n\n<p>We created the <code>gradio.HuggingFaceDatasetSaver</code> class, but you can pass your own custom class as long as it inherits from <code>FLaggingCallback</code> defined in <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py\">this file</a>. If you create a cool callback, contribute it to the repo! </p>\n\n<h2 id=\"flagging-with-blocks\">Flagging with Blocks</h2>\n\n<p>What about if you are using <code>gradio.Blocks</code>? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.</p>\n\n<p>At the same time, you might want to use an existing <code>FlaggingCallback</code> to avoid writing extra code.\nThis requires two steps:</p>\n\n<ol>\n<li>You have to run your callback's <code>.setup()</code> somewhere in the code prior to the \nfirst time you flag data</li>\n<li>When the flagging button is clicked, then you trigger the callback's <code>.flag()</code> method,\nmaking sure to collect the arguments correctly and disabling the typical preprocessing. </li>\n</ol>\n\n<p>Here is an example with an image sepia filter Blocks demo that lets you flag\ndata using the default <code>CSVLogger</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import numpy as np\nimport gradio as gr\n\ndef sepia(input_img, strength):\n    sepia_filter = strength * np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]\n    ) + (1-strength) * np.identity(3)\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ncallback = gr.CSVLogger()\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            img_input = gr.Image()\n            strength = gr.Slider(0, 1, 0.5)\n        img_output = gr.Image()\n    with gr.Row():\n        btn = gr.Button(\"Flag\")\n\n    # This needs to be called at some point prior to the first call to callback.flag()\n    callback.setup([img_input, strength, img_output], \"flagged_data_points\")\n\n    img_input.change(sepia, [img_input, strength], img_output)\n    strength.change(sepia, [img_input, strength], img_output)\n\n    # We can choose which components to flag -- in this case, we'll flag all of them\n    btn.click(lambda *args: callback.flag(args), [img_input, strength, img_output], None, preprocess=False)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_flag'></gradio-app></p>\n\n<h2 id=\"privacy\">Privacy</h2>\n\n<p>Important Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use <code>allow_flagging=auto</code> (when all of the data submitted through the demo is being flagged)</p>\n\n<h3 id=\"thats-all-happy-building\">That's all! Happy building :)</h3>\n", "tags": ["FLAGGING", "DATA"], "spaces": ["https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced", "https://huggingface.co/spaces/gradio/calculator-flagging-options", "https://huggingface.co/spaces/gradio/calculator-flag-basic"], "url": "/guides/using-flagging/", "contributor": null}], "parent": "gradio"}, "huggingfacedatasetsaver": {"class": null, "name": "HuggingFaceDatasetSaver", "description": "A callback that saves each flagged sample (both the input and output data) to a HuggingFace dataset.", "tags": {"guides": "using-flagging"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "hf_token", "annotation": "str", "doc": "The HuggingFace token to use to create (and write the flagged sample to) the HuggingFace dataset."}, {"name": "dataset_name", "annotation": "str", "doc": "The name of the dataset to save the data to, e.g. \"image-classifier-1\""}, {"name": "organization", "annotation": "str | None", "doc": "The organization to save the dataset under. The hf_token must provide write access to this organization. If not provided, saved under the name of the user corresponding to the hf_token.", "default": "None"}, {"name": "private", "annotation": "bool", "doc": "Whether the dataset should be private (defaults to False).", "default": "False"}], "returns": {"annotation": null}, "example": "import gradio as gr\nhf_writer = gr.HuggingFaceDatasetSaver(HF_API_TOKEN, \"image-classification-mistakes\")\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\",\n                    allow_flagging=\"manual\", flagging_callback=hf_writer)", "fns": [], "guides": [{"name": "using-flagging", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 36, "pretty_name": "Using Flagging", "content": "# Using Flagging\n\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n\n## Introduction\n\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.\n\nGradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n\n## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.\n\n* `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.                 \n    * `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n    * `auto`: users will not see a button to flag, but every sample will be flagged automatically. \n    * `never`: users will not see a button to flag, and no sample will be flagged. \n* `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n    * If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n    * If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n    * The chosen option is then logged along with the input and output.\n* `flagging_dir`: this parameter takes a string.\n    * It represents what to name the directory where flagged data is stored.\n* `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n    * Using this parameter allows you to write custom code that gets run when the flag button is clicked\n    * By default, this is set to an instance of `gr.CSVLogger`\n    * One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)\n\n## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CSV file will log the flagged data. \n\nHere's an example: The code below creates the calculator interface embedded below it:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\n\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged. \n\n```directory\n+-- flagged/\n|   +-- logs.csv\n```\n_flagged/logs.csv_\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\n\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n|   +-- image/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n_flagged/logs.csv_\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.\n\nIf we go back to the calculator example, the following code will create the interface embedded below it.  \n```python\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n```\n<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\n\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\n\n_flagged/logs.csv_\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n\n## The HuggingFaceDatasetSaver Callback\n\nSometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data.\n\nWe've made this super easy with the `flagging_callback` parameter.\n\nFor example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:\n\n\n```python\nimport os\n\nHF_TOKEN = os.getenv('HF_TOKEN')\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n```\n\nNotice that we define our own \ninstance of  `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\n\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo! \n\n## Flagging with Blocks\n\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.\n\nAt the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\nThis requires two steps:\n\n1. You have to run your callback's `.setup()` somewhere in the code prior to the \nfirst time you flag data\n2. When the flagging button is clicked, then you trigger the callback's `.flag()` method,\nmaking sure to collect the arguments correctly and disabling the typical preprocessing. \n\nHere is an example with an image sepia filter Blocks demo that lets you flag\ndata using the default `CSVLogger`:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img, strength):\n    sepia_filter = strength * np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]\n    ) + (1-strength) * np.identity(3)\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ncallback = gr.CSVLogger()\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            img_input = gr.Image()\n            strength = gr.Slider(0, 1, 0.5)\n        img_output = gr.Image()\n    with gr.Row():\n        btn = gr.Button(\"Flag\")\n        \n    # This needs to be called at some point prior to the first call to callback.flag()\n    callback.setup([img_input, strength, img_output], \"flagged_data_points\")\n\n    img_input.change(sepia, [img_input, strength], img_output)\n    strength.change(sepia, [img_input, strength], img_output)\n    \n    # We can choose which components to flag -- in this case, we'll flag all of them\n    btn.click(lambda *args: callback.flag(args), [img_input, strength, img_output], None, preprocess=False)\n\ndemo.launch()\n\n```\n<gradio-app space='gradio/blocks_flag'></gradio-app>\n\n## Privacy\n\nImportant Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use `allow_flagging=auto` (when all of the data submitted through the demo is being flagged)\n\n### That's all! Happy building :) \n", "html": "<h1 id=\"using-flagging\">Using Flagging</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>When you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.</p>\n\n<p>Gradio simplifies the collection of this data by including a <strong>Flag</strong> button with every <code>Interface</code>. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with <code>gradio.Interface</code> as well as with <code>gradio.Blocks</code>.</p>\n\n<h2 id=\"the-flag-button-in-gradiointerface\">The <strong>Flag</strong> button in <code>gradio.Interface</code></h2>\n\n<p>Flagging with Gradio's <code>Interface</code> is especially easy. By default, underneath the output components, there is a button marked <strong>Flag</strong>. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.</p>\n\n<p>There are <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#interface-header\">four parameters</a> in <code>gradio.Interface</code> that control how flagging works. We will go over them in greater detail.</p>\n\n<ul>\n<li><code>allow_flagging</code>: this parameter can be set to either <code>\"manual\"</code> (default), <code>\"auto\"</code>, or <code>\"never\"</code>. <br />\n<ul>\n<li><code>manual</code>: users will see a button to flag, and samples are only flagged when the button is clicked.</li>\n<li><code>auto</code>: users will not see a button to flag, but every sample will be flagged automatically. </li>\n<li><code>never</code>: users will not see a button to flag, and no sample will be flagged. </li>\n</ul></li>\n<li><code>flagging_options</code>: this parameter can be either <code>None</code> (default) or a list of strings.\n<ul>\n<li>If <code>None</code>, then the user simply clicks on the <strong>Flag</strong> button and no additional options are shown.</li>\n<li>If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is <code>[\"Incorrect\", \"Ambiguous\"]</code>, then buttons labeled <strong>Flag as Incorrect</strong> and <strong>Flag as Ambiguous</strong> appear. This only applies if <code>allow_flagging</code> is <code>\"manual\"</code>.</li>\n<li>The chosen option is then logged along with the input and output.</li>\n</ul></li>\n<li><code>flagging_dir</code>: this parameter takes a string.\n<ul>\n<li>It represents what to name the directory where flagged data is stored.</li>\n</ul></li>\n<li><code>flagging_callback</code>: this parameter takes an instance of a subclass of the <code>FlaggingCallback</code> class\n<ul>\n<li>Using this parameter allows you to write custom code that gets run when the flag button is clicked</li>\n<li>By default, this is set to an instance of <code>gr.CSVLogger</code></li>\n<li>One example is setting it to an instance of <code>gr.HuggingFaceDatasetSaver</code> which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)</li>\n</ul></li>\n</ul>\n\n<h2 id=\"what-happens-to-flagged-data\">What happens to flagged data?</h2>\n\n<p>Within the directory provided by the <code>flagging_dir</code> argument, a CSV file will log the flagged data. </p>\n\n<p>Here's an example: The code below creates the calculator interface embedded below it:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n</code></pre></div>\n\n<p><gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app></p>\n\n<p>When you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged. </p>\n\n<div class='codeblock'><pre><code class='lang-directory'>+-- flagged/\n|   +-- logs.csv\n</code></pre></div>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>num1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n</code></pre></div>\n\n<p>If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an <code>image</code> input to <code>image</code> output interface will create the following structure.</p>\n\n<div class='codeblock'><pre><code class='lang-directory'>+-- flagged/\n|   +-- logs.csv\n|   +-- image/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n</code></pre></div>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>im,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n</code></pre></div>\n\n<p>If you wish for the user to provide a reason for flagging, you can pass a list of strings to the <code>flagging_options</code> argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.</p>\n\n<p>If we go back to the calculator example, the following code will create the interface embedded below it.  </p>\n\n<div class='codeblock'><pre><code class='lang-python'>iface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n</code></pre></div>\n\n<p><gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app></p>\n\n<p>When users click the flag button, the csv file will now include a column indicating the selected option.</p>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>num1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n</code></pre></div>\n\n<h2 id=\"the-huggingfacedatasetsaver-callback\">The HuggingFaceDatasetSaver Callback</h2>\n\n<p>Sometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data.</p>\n\n<p>We've made this super easy with the <code>flagging_callback</code> parameter.</p>\n\n<p>For example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import os\n\nHF_TOKEN = os.getenv('HF_TOKEN')\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n</code></pre></div>\n\n<p>Notice that we define our own \ninstance of  <code>gradio.HuggingFaceDatasetSaver</code> using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set <code>allow_flagging=\"manual\"</code>\nbecause on Hugging Face Spaces, <code>allow_flagging</code> is set to <code>\"never\"</code> by default. Here's our demo:</p>\n\n<p><gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app></p>\n\n<p>You can now see all the examples flagged above in this <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo\">public Hugging Face dataset</a>.</p>\n\n<p><img src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true\" alt=\"flagging callback hf\" /></p>\n\n<p>We created the <code>gradio.HuggingFaceDatasetSaver</code> class, but you can pass your own custom class as long as it inherits from <code>FLaggingCallback</code> defined in <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py\">this file</a>. If you create a cool callback, contribute it to the repo! </p>\n\n<h2 id=\"flagging-with-blocks\">Flagging with Blocks</h2>\n\n<p>What about if you are using <code>gradio.Blocks</code>? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.</p>\n\n<p>At the same time, you might want to use an existing <code>FlaggingCallback</code> to avoid writing extra code.\nThis requires two steps:</p>\n\n<ol>\n<li>You have to run your callback's <code>.setup()</code> somewhere in the code prior to the \nfirst time you flag data</li>\n<li>When the flagging button is clicked, then you trigger the callback's <code>.flag()</code> method,\nmaking sure to collect the arguments correctly and disabling the typical preprocessing. </li>\n</ol>\n\n<p>Here is an example with an image sepia filter Blocks demo that lets you flag\ndata using the default <code>CSVLogger</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import numpy as np\nimport gradio as gr\n\ndef sepia(input_img, strength):\n    sepia_filter = strength * np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]\n    ) + (1-strength) * np.identity(3)\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ncallback = gr.CSVLogger()\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            img_input = gr.Image()\n            strength = gr.Slider(0, 1, 0.5)\n        img_output = gr.Image()\n    with gr.Row():\n        btn = gr.Button(\"Flag\")\n\n    # This needs to be called at some point prior to the first call to callback.flag()\n    callback.setup([img_input, strength, img_output], \"flagged_data_points\")\n\n    img_input.change(sepia, [img_input, strength], img_output)\n    strength.change(sepia, [img_input, strength], img_output)\n\n    # We can choose which components to flag -- in this case, we'll flag all of them\n    btn.click(lambda *args: callback.flag(args), [img_input, strength, img_output], None, preprocess=False)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_flag'></gradio-app></p>\n\n<h2 id=\"privacy\">Privacy</h2>\n\n<p>Important Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use <code>allow_flagging=auto</code> (when all of the data submitted through the demo is being flagged)</p>\n\n<h3 id=\"thats-all-happy-building\">That's all! Happy building :)</h3>\n", "tags": ["FLAGGING", "DATA"], "spaces": ["https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced", "https://huggingface.co/spaces/gradio/calculator-flagging-options", "https://huggingface.co/spaces/gradio/calculator-flag-basic"], "url": "/guides/using-flagging/", "contributor": null}], "parent": "gradio"}, "queue": {"class": null, "name": "queue", "description": "You can control the rate of processed requests by creating a queue. This will allow you to set the number of requests to be processed at one time, and will let users know their position in the queue.", "tags": {"parameters": "concurrency_count: Number of worker threads that will be processing requests from the queue concurrently. Increasing this number will increase the rate at which requests are processed, but will also increase the memory usage of the queue.<br>status_update_rate: If \"auto\", Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.<br>client_position_to_load_data: DEPRECATED. This parameter is deprecated and has no effect.<br>default_enabled: Deprecated and has no effect.<br>api_open: If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.<br>max_size: The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited."}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "concurrency_count", "annotation": "int", "doc": "Number of worker threads that will be processing requests from the queue concurrently. Increasing this number will increase the rate at which requests are processed, but will also increase the memory usage of the queue.", "default": "1"}, {"name": "status_update_rate", "annotation": "float | Literal['auto']", "doc": "If \"auto\", Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.", "default": "\"auto\""}, {"name": "client_position_to_load_data", "annotation": "int | None", "doc": "DEPRECATED. This parameter is deprecated and has no effect.", "default": "None"}, {"name": "default_enabled", "annotation": "bool | None", "doc": "Deprecated and has no effect.", "default": "None"}, {"name": "api_open", "annotation": "bool", "doc": "If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.", "default": "True"}, {"name": "max_size", "annotation": "int | None", "doc": "The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited.", "default": "None"}], "returns": {"annotation": null}, "example": "demo = gr.Interface(gr.Textbox(), gr.Image(), image_generator)\ndemo.queue(concurrency_count=3)\ndemo.launch()", "fns": [], "parent": "gradio"}, "blocks": {"class": null, "name": "Blocks", "description": "Blocks is Gradio's low-level API that allows you to create more custom web applications and demos than Interfaces (yet still entirely in Python). <br> <br> Compared to the Interface class, Blocks offers more flexibility and control over: (1) the layout of components (2) the events that trigger the execution of functions (3) data flows (e.g. inputs can trigger outputs, which can trigger the next level of outputs). Blocks also offers ways to group together related demos such as with tabs. <br> <br> The basic usage of Blocks is as follows: create a Blocks object, then use it as a context (with the \"with\" statement), and then define layouts, components, or events within the Blocks context. Finally, call the launch() method to launch the demo. <br>", "tags": {"demos": "blocks_hello, blocks_flipper, blocks_speech_text_sentiment, generate_english_german, sound_alert", "guides": "blocks-and-event-listeners, controlling-layout, state-in-blocks, custom-CSS-and-JS, custom-interpretations-with-blocks, using-blocks-like-functions"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "theme", "annotation": "str", "doc": "which theme to use - right now, only \"default\" is supported.", "default": "\"default\""}, {"name": "analytics_enabled", "annotation": "bool | None", "doc": "whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable or default to True.", "default": "None"}, {"name": "mode", "annotation": "str", "doc": "a human-friendly name for the kind of Blocks or Interface being created.", "default": "\"blocks\""}, {"name": "title", "annotation": "str", "doc": "The tab title to display when this is opened in a browser window.", "default": "\"Gradio\""}, {"name": "css", "annotation": "str | None", "doc": "custom css or path to custom css file to apply to entire Blocks", "default": "None"}], "returns": {"annotation": null}, "example": "import gradio as gr\ndef update(name):\n    return f\"Welcome to Gradio, {name}!\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Start typing below and then click **Run** to see the output.\")\n    with gr.Row():\n        inp = gr.Textbox(placeholder=\"What is your name?\")\n        out = gr.Textbox()\n    btn = gr.Button(\"Run\")\n    btn.click(fn=update, inputs=inp, outputs=out)\n\ndemo.launch()", "fns": [{"fn": null, "name": "load", "description": "For reverse compatibility reasons, this is both a class method and an instance method, the two of which, confusingly, do two completely different things. <br> <br> Class method: loads a demo from a Hugging Face Spaces repo and creates it locally and returns a block instance. Equivalent to gradio.Interface.load() <br> <br> Instance method: adds event that runs as soon as the demo loads in the browser. Example usage below.", "tags": {}, "parameters": [{"name": "fn", "annotation": "Callable | None", "doc": "Instance Method - the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.", "default": "None"}, {"name": "inputs", "annotation": "List[Component] | None", "doc": "Instance Method - List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "List[Component] | None", "doc": "Instance Method - List of gradio.components to use as inputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Instance Method - Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "Instance Method - If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "Instance Method - If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "<class 'inspect._empty'>", "doc": "Instance Method - If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "Instance Method - If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Instance Method - Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "Instance Method - If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "Instance Method - If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "every", "annotation": "float | None", "doc": "Instance Method - Run this event 'every' number of seconds. Interpreted in seconds. Queue must be enabled.", "default": "None"}, {"name": "name", "annotation": "str | None", "doc": "Class Method - the name of the model (e.g. \"gpt2\" or \"facebook/bart-base\") or space (e.g. \"flax-community/spanish-gpt2\"), can include the `src` as prefix (e.g. \"models/facebook/bart-base\")", "default": "None"}, {"name": "src", "annotation": "str | None", "doc": "Class Method - the source of the model: `models` or `spaces` (or leave empty if source is provided as a prefix in `name`)", "default": "None"}, {"name": "api_key", "annotation": "str | None", "doc": "Class Method - optional access token for loading private Hugging Face Hub models or spaces. Find your token here: https://huggingface.co/settings/tokens", "default": "None"}, {"name": "alias", "annotation": "str | None", "doc": "Class Method - optional string used as the name of the loaded model instead of the default name (only applies if loading a Space running Gradio 2.x)", "default": "None"}], "returns": {}, "example": "import gradio as gr\nimport datetime\nwith gr.Blocks() as demo:\n    def get_time():\n        return datetime.datetime.now().time()\n    dt = gr.Textbox(label=\"Current time\")\n    demo.load(get_time, inputs=None, outputs=dt)\ndemo.launch()", "parent": "gradio.Blocks"}], "demos": [["blocks_hello", "import gradio as gr\n\ndef welcome(name):\n    return f\"Welcome to Gradio, {name}!\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n    inp.change(welcome, inp, out)\n\nif __name__ == \"__main__\":\n    demo.launch()"], ["blocks_flipper", "import numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n    \nif __name__ == \"__main__\":\n    demo.launch()"], ["blocks_speech_text_sentiment", "from transformers import pipeline\n\nimport gradio as gr\n\nasr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\nclassifier = pipeline(\"text-classification\")\n\n\ndef speech_to_text(speech):\n    text = asr(speech)[\"text\"]\n    return text\n\n\ndef text_to_sentiment(text):\n    return classifier(text)[0][\"label\"]\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    audio_file = gr.Audio(type=\"filepath\")\n    text = gr.Textbox()\n    label = gr.Label()\n\n    b1 = gr.Button(\"Recognize Speech\")\n    b2 = gr.Button(\"Classify Sentiment\")\n\n    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n    b2.click(text_to_sentiment, inputs=text, outputs=label)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["generate_english_german", "import gradio as gr\n\nfrom transformers import pipeline\n\nenglish_translator = gr.Blocks.load(name=\"spaces/gradio/english_translator\")\nenglish_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n\n\ndef generate_text(text):\n    english_text = english_generator(text)[0][\"generated_text\"]\n    german_text = english_translator(english_text)\n    return english_text, german_text\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            seed = gr.Text(label=\"Input Phrase\")\n        with gr.Column():\n            english = gr.Text(label=\"Generated English Text\")\n            german = gr.Text(label=\"Generated German Text\")\n    btn = gr.Button(\"Generate\")\n    btn.click(generate_text, inputs=[seed], outputs=[english, german])\n    gr.Examples([\"My name is Clara and I am\"], inputs=[seed])\n\nif __name__ == \"__main__\":\n    demo.launch()"], ["sound_alert", "import time\nimport gradio as gr\n\n\njs_function = \"() => {new Audio('file=beep.mp3').play();}\"\n\ndef task(x):\n    time.sleep(2)\n    return \"Hello, \" + x \n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"name\")\n    greeting = gr.Textbox(label=\"greeting\")\n    name.blur(task, name, greeting)\n    greeting.change(None, [], [], _js=js_function)  # Note that _js is a special arugment whose usage may change in the future\n    \ndemo.launch()"]], "guides": [{"name": "blocks-and-event-listeners", "category": "building-with-blocks", "pretty_category": "Building With Blocks", "guide_index": 1, "absolute_index": 8, "pretty_name": "Blocks And Event Listeners", "content": "# Blocks and Event Listeners\n\nWe took a quick look at Blocks in the [Quickstart](https://gradio.app/quickstart/#blocks-more-flexibility-and-control). Let's dive deeper. This guide will cover the how Blocks are structured, event listeners and their types, running events continuously, updating configurations, and using dictionaries vs lists. \n\n## Blocks Structure\n\nTake a look at the demo below.\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n    greet_btn.click(fn=greet, inputs=name, outputs=output)\n\ndemo.launch()\n```\n<gradio-app space='gradio/hello_blocks'></gradio-app>\n\n- First, note the `with gr.Blocks() as demo:` clause. The Blocks app code will be contained within this clause.\n- Next come the Components. These are the same Components used in `Interface`. However, instead of being passed to some constructor, Components are automatically added to the Blocks as they are created within the `with` clause.\n- Finally, the `click()` event listener. Event listeners define the data flow within the app. In the example above, the listener ties the two Textboxes together. The Textbox `name` acts as the input and Textbox `output` acts as the output to the `greet` method. This dataflow is triggered when the Button `greet_btn` is clicked. Like an Interface, an event listener can take multiple inputs or outputs.\n\n## Event Listeners and Interactivity\n\nIn the example above, you'll notice that you are able to edit Textbox `name`, but not Textbox `output`. This is because any Component that acts as an input to an event listener is made interactive. However, since Textbox `output` acts only as an output, it is not interactive. You can directly configure the interactivity of a Component with the `interactive=` keyword argument. \n\n```python\noutput = gr.Textbox(label=\"Output\", interactive=True)\n```\n\n## Types of Event Listeners\n\nTake a look at the demo below:\n\n```python\nimport gradio as gr\n\ndef welcome(name):\n    return f\"Welcome to Gradio, {name}!\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n    inp.change(welcome, inp, out)\n\ndemo.launch()\n```\n<gradio-app space='gradio/blocks_hello'></gradio-app>\n\nInstead of being triggered by a click, the `welcome` function is triggered by typing in the Textbox `inp`. This is due to the `change()` event listener. Different Components support different event listeners. For example, the `Video` Component supports a `play()` event listener, triggered when a user presses play. See the [Docs](http://gradio.app/docs#components) for the event listeners for each Component.\n\n## Running Events Continuously\n\nYou can run events on a fixed schedule using the `every` parameter of the event listener. This will run the event\n`every` number of seconds while the client connection is open. If the connection is closed, the event will stop running after the following iteration.\nNote that this does not take into account the runtime of the event itself. So a function\nwith a 1 second runtime running with `every=5`, would actually run every 6 seconds.\n\nHere is an example of a sine curve that updates every second!\n\n```python\nimport math\nimport gradio as gr\nimport plotly.express as px\nimport numpy as np\n\n\nplot_end = 2 * math.pi\n\n\ndef get_plot(period=1):\n    global plot_end\n    x = np.arange(plot_end - 2 * math.pi, plot_end, 0.02)\n    y = np.sin(2*math.pi*period * x)\n    fig = px.line(x=x, y=y)\n    plot_end += 2 * math.pi\n    if plot_end > 1000:\n        plot_end = 2 * math.pi\n    return fig\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\"Change the value of the slider to automatically update the plot\")\n            period = gr.Slider(label=\"Period of plot\", value=1, minimum=0, maximum=10, step=1)\n            plot = gr.Plot(label=\"Plot (updates every half second)\")\n\n    dep = demo.load(get_plot, None, plot, every=1)\n    period.change(get_plot, period, plot, every=1, cancels=[dep])\n\n\nif __name__ == \"__main__\":\n    demo.queue().launch()\n\n```\n<gradio-app space='gradio/sine_curve'></gradio-app>\n\n## Multiple Data Flows\n\nA Blocks app is not limited to a single data flow the way Interfaces are. Take a look at the demo below:\n\n```python\nimport gradio as gr\n\ndef increase(num):\n    return num + 1\n\nwith gr.Blocks() as demo:\n    a = gr.Number(label=\"a\")\n    b = gr.Number(label=\"b\")\n    btoa = gr.Button(\"a > b\")\n    atob = gr.Button(\"b > a\")\n    atob.click(increase, a, b)\n    btoa.click(increase, b, a)\n\ndemo.launch()\n```\n<gradio-app space='gradio/reversible_flow'></gradio-app>\n\nNote that `num1` can act as input to `num2`, and also vice-versa! As your apps get more complex, you will have many data flows connecting various Components. \n\nHere's an example of a \"multi-step\" demo, where the output of one model (a speech-to-text model) gets fed into the next model (a sentiment classifier).\n\n```python\nfrom transformers import pipeline\n\nimport gradio as gr\n\nasr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\nclassifier = pipeline(\"text-classification\")\n\n\ndef speech_to_text(speech):\n    text = asr(speech)[\"text\"]\n    return text\n\n\ndef text_to_sentiment(text):\n    return classifier(text)[0][\"label\"]\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    audio_file = gr.Audio(type=\"filepath\")\n    text = gr.Textbox()\n    label = gr.Label()\n\n    b1 = gr.Button(\"Recognize Speech\")\n    b2 = gr.Button(\"Classify Sentiment\")\n\n    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n    b2.click(text_to_sentiment, inputs=text, outputs=label)\n\ndemo.launch()\n\n```\n<gradio-app space='gradio/blocks_speech_text_sentiment'></gradio-app>\n\n## Function Input List vs Dict\n\nThe event listeners you've seen so far have a single input component. If you'd like to have multiple input components pass data to the function, you have two options on how the function can accept input component values:\n\n1. as a list of arguments, or\n2. as a single dictionary of values, keyed by the component\n\nLet's see an example of each:\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    a = gr.Number(label=\"a\")\n    b = gr.Number(label=\"b\")\n    with gr.Row():\n        add_btn = gr.Button(\"Add\")\n        sub_btn = gr.Button(\"Subtract\")\n    c = gr.Number(label=\"sum\")\n\n    def add(num1, num2):\n        return num1 + num2\n    add_btn.click(add, inputs=[a, b], outputs=c)\n\n    def sub(data):\n        return data[a] - data[b]\n    sub_btn.click(sub, inputs={a, b}, outputs=c)\n\n\ndemo.launch()\n```\n\nBoth `add()` and `sub()` take `a` and `b` as inputs. However, the syntax is different between these listeners. \n\n1. To the `add_btn` listener, we pass the inputs as a list. The function `add()` takes each of these inputs as arguments. The value of `a` maps to the argument `num1`, and the value of `b` maps to the argument `num2`.\n2. To the `sub_btn` listener, we pass the inputs as a set (note the curly brackets!). The function `sub()` takes a single dictionary argument `data`, where the keys are the input components, and the values are the values of those components.\n\nIt is a matter of preference which syntax you prefer! For functions with many input components, option 2 may be easier to manage.\n\n<gradio-app space='gradio/calculator_list_and_dict'></gradio-app>\n\n## Function Return List vs Dict\n\nSimilarly, you may return values for multiple output components either as:\n\n1. a list of values, or\n2. a dictionary keyed by the component\n\nLet's first see an example of (1), where we set the values of two output components by returning two values:\n\n```python\nwith gr.Blocks() as demo:\n    food_box = gr.Number(value=10, label=\"Food Count\")\n    status_box = gr.Textbox()\n    def eat(food):\n        if food > 0:\n            return food - 1, \"full\"\n        else:\n            return 0, \"hungry\"\n    gr.Button(\"EAT\").click(\n        fn=eat, \n        inputs=food_box,\n        outputs=[food_box, status_box]\n    )\n```\n\nAbove, each return statement returns two values corresponding to `food_box` and `status_box`, respectively.\n\nInstead of returning a list of values corresponding to each output component in order, you can also return a dictionary, with the key corresponding to the output component and the value as the new value. This also allows you to skip updating some output components. \n\n```python\nwith gr.Blocks() as demo:\n    food_box = gr.Number(value=10, label=\"Food Count\")\n    status_box = gr.Textbox()\n    def eat(food):\n        if food > 0:\n            return {food_box: food - 1, status_box: \"full\"}\n        else:\n            return {status_box: \"hungry\"}\n    gr.Button(\"EAT\").click(\n        fn=eat, \n        inputs=food_box,\n        outputs=[food_box, status_box]\n    )\n```\n\nNotice how when there is no food, we only update the `status_box` element. We skipped updating the `food_box` component.\n\nDictionary returns are helpful when an event listener affects many components on return, or conditionally affects outputs and not others.\n\nKeep in mind that with dictionary returns, we still need to specify the possible outputs in the event listener.\n\n## Updating Component Configurations\n\nThe return value of an event listener function is usually the updated value of the corresponding output Component. Sometimes we want to update the configuration of the Component as well, such as the visibility. In this case, we return a `gr.update()` object instead of just the update Component value.\n\n```python\nimport gradio as gr\n\ndef change_textbox(choice):\n    if choice == \"short\":\n        return gr.update(lines=2, visible=True, value=\"Short story: \")\n    elif choice == \"long\":\n        return gr.update(lines=8, visible=True, value=\"Long story...\")\n    else:\n        return gr.update(visible=False)\n\nwith gr.Blocks() as demo:\n    radio = gr.Radio(\n        [\"short\", \"long\", \"none\"], label=\"Essay Length to Write?\"\n    )\n    text = gr.Textbox(lines=2, interactive=True)\n    radio.change(fn=change_textbox, inputs=radio, outputs=text)\n\ndemo.launch()\n```\n<gradio-app space='gradio/blocks_essay_update'></gradio-app>\n\nSee how we can configure the Textbox itself through the `gr.update()` method. The `value=` argument can still be used to update the value along with Component configuration.\n\n", "html": "<h1 id=\"blocks-and-event-listeners\">Blocks and Event Listeners</h1>\n\n<p>We took a quick look at Blocks in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/quickstart/#blocks-more-flexibility-and-control\">Quickstart</a>. Let's dive deeper. This guide will cover the how Blocks are structured, event listeners and their types, running events continuously, updating configurations, and using dictionaries vs lists. </p>\n\n<h2 id=\"blocks-structure\">Blocks Structure</h2>\n\n<p>Take a look at the demo below.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n    greet_btn.click(fn=greet, inputs=name, outputs=output)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/hello_blocks'></gradio-app></p>\n\n<ul>\n<li>First, note the <code>with gr.Blocks() as demo:</code> clause. The Blocks app code will be contained within this clause.</li>\n<li>Next come the Components. These are the same Components used in <code>Interface</code>. However, instead of being passed to some constructor, Components are automatically added to the Blocks as they are created within the <code>with</code> clause.</li>\n<li>Finally, the <code>click()</code> event listener. Event listeners define the data flow within the app. In the example above, the listener ties the two Textboxes together. The Textbox <code>name</code> acts as the input and Textbox <code>output</code> acts as the output to the <code>greet</code> method. This dataflow is triggered when the Button <code>greet_btn</code> is clicked. Like an Interface, an event listener can take multiple inputs or outputs.</li>\n</ul>\n\n<h2 id=\"event-listeners-and-interactivity\">Event Listeners and Interactivity</h2>\n\n<p>In the example above, you'll notice that you are able to edit Textbox <code>name</code>, but not Textbox <code>output</code>. This is because any Component that acts as an input to an event listener is made interactive. However, since Textbox <code>output</code> acts only as an output, it is not interactive. You can directly configure the interactivity of a Component with the <code>interactive=</code> keyword argument. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>output = gr.Textbox(label=\"Output\", interactive=True)\n</code></pre></div>\n\n<h2 id=\"types-of-event-listeners\">Types of Event Listeners</h2>\n\n<p>Take a look at the demo below:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef welcome(name):\n    return f\"Welcome to Gradio, {name}!\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n    inp.change(welcome, inp, out)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_hello'></gradio-app></p>\n\n<p>Instead of being triggered by a click, the <code>welcome</code> function is triggered by typing in the Textbox <code>inp</code>. This is due to the <code>change()</code> event listener. Different Components support different event listeners. For example, the <code>Video</code> Component supports a <code>play()</code> event listener, triggered when a user presses play. See the <a rel=\"noopener\" target=\"_blank\" href=\"http://gradio.app/docs#components\">Docs</a> for the event listeners for each Component.</p>\n\n<h2 id=\"running-events-continuously\">Running Events Continuously</h2>\n\n<p>You can run events on a fixed schedule using the <code>every</code> parameter of the event listener. This will run the event\n<code>every</code> number of seconds while the client connection is open. If the connection is closed, the event will stop running after the following iteration.\nNote that this does not take into account the runtime of the event itself. So a function\nwith a 1 second runtime running with <code>every=5</code>, would actually run every 6 seconds.</p>\n\n<p>Here is an example of a sine curve that updates every second!</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import math\nimport gradio as gr\nimport plotly.express as px\nimport numpy as np\n\n\nplot_end = 2 * math.pi\n\n\ndef get_plot(period=1):\n    global plot_end\n    x = np.arange(plot_end - 2 * math.pi, plot_end, 0.02)\n    y = np.sin(2*math.pi*period * x)\n    fig = px.line(x=x, y=y)\n    plot_end += 2 * math.pi\n    if plot_end > 1000:\n        plot_end = 2 * math.pi\n    return fig\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\"Change the value of the slider to automatically update the plot\")\n            period = gr.Slider(label=\"Period of plot\", value=1, minimum=0, maximum=10, step=1)\n            plot = gr.Plot(label=\"Plot (updates every half second)\")\n\n    dep = demo.load(get_plot, None, plot, every=1)\n    period.change(get_plot, period, plot, every=1, cancels=[dep])\n\n\nif __name__ == \"__main__\":\n    demo.queue().launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/sine_curve'></gradio-app></p>\n\n<h2 id=\"multiple-data-flows\">Multiple Data Flows</h2>\n\n<p>A Blocks app is not limited to a single data flow the way Interfaces are. Take a look at the demo below:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef increase(num):\n    return num + 1\n\nwith gr.Blocks() as demo:\n    a = gr.Number(label=\"a\")\n    b = gr.Number(label=\"b\")\n    btoa = gr.Button(\"a > b\")\n    atob = gr.Button(\"b > a\")\n    atob.click(increase, a, b)\n    btoa.click(increase, b, a)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/reversible_flow'></gradio-app></p>\n\n<p>Note that <code>num1</code> can act as input to <code>num2</code>, and also vice-versa! As your apps get more complex, you will have many data flows connecting various Components. </p>\n\n<p>Here's an example of a \"multi-step\" demo, where the output of one model (a speech-to-text model) gets fed into the next model (a sentiment classifier).</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import pipeline\n\nimport gradio as gr\n\nasr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\nclassifier = pipeline(\"text-classification\")\n\n\ndef speech_to_text(speech):\n    text = asr(speech)[\"text\"]\n    return text\n\n\ndef text_to_sentiment(text):\n    return classifier(text)[0][\"label\"]\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    audio_file = gr.Audio(type=\"filepath\")\n    text = gr.Textbox()\n    label = gr.Label()\n\n    b1 = gr.Button(\"Recognize Speech\")\n    b2 = gr.Button(\"Classify Sentiment\")\n\n    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n    b2.click(text_to_sentiment, inputs=text, outputs=label)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks<em>speech</em>text_sentiment'></gradio-app></p>\n\n<h2 id=\"function-input-list-vs-dict\">Function Input List vs Dict</h2>\n\n<p>The event listeners you've seen so far have a single input component. If you'd like to have multiple input components pass data to the function, you have two options on how the function can accept input component values:</p>\n\n<ol>\n<li>as a list of arguments, or</li>\n<li>as a single dictionary of values, keyed by the component</li>\n</ol>\n\n<p>Let's see an example of each:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    a = gr.Number(label=\"a\")\n    b = gr.Number(label=\"b\")\n    with gr.Row():\n        add_btn = gr.Button(\"Add\")\n        sub_btn = gr.Button(\"Subtract\")\n    c = gr.Number(label=\"sum\")\n\n    def add(num1, num2):\n        return num1 + num2\n    add_btn.click(add, inputs=[a, b], outputs=c)\n\n    def sub(data):\n        return data[a] - data[b]\n    sub_btn.click(sub, inputs={a, b}, outputs=c)\n\n\ndemo.launch()\n</code></pre></div>\n\n<p>Both <code>add()</code> and <code>sub()</code> take <code>a</code> and <code>b</code> as inputs. However, the syntax is different between these listeners. </p>\n\n<ol>\n<li>To the <code>add_btn</code> listener, we pass the inputs as a list. The function <code>add()</code> takes each of these inputs as arguments. The value of <code>a</code> maps to the argument <code>num1</code>, and the value of <code>b</code> maps to the argument <code>num2</code>.</li>\n<li>To the <code>sub_btn</code> listener, we pass the inputs as a set (note the curly brackets!). The function <code>sub()</code> takes a single dictionary argument <code>data</code>, where the keys are the input components, and the values are the values of those components.</li>\n</ol>\n\n<p>It is a matter of preference which syntax you prefer! For functions with many input components, option 2 may be easier to manage.</p>\n\n<p><gradio-app space='gradio/calculator<em>list</em>and_dict'></gradio-app></p>\n\n<h2 id=\"function-return-list-vs-dict\">Function Return List vs Dict</h2>\n\n<p>Similarly, you may return values for multiple output components either as:</p>\n\n<ol>\n<li>a list of values, or</li>\n<li>a dictionary keyed by the component</li>\n</ol>\n\n<p>Let's first see an example of (1), where we set the values of two output components by returning two values:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    food_box = gr.Number(value=10, label=\"Food Count\")\n    status_box = gr.Textbox()\n    def eat(food):\n        if food > 0:\n            return food - 1, \"full\"\n        else:\n            return 0, \"hungry\"\n    gr.Button(\"EAT\").click(\n        fn=eat, \n        inputs=food_box,\n        outputs=[food_box, status_box]\n    )\n</code></pre></div>\n\n<p>Above, each return statement returns two values corresponding to <code>food_box</code> and <code>status_box</code>, respectively.</p>\n\n<p>Instead of returning a list of values corresponding to each output component in order, you can also return a dictionary, with the key corresponding to the output component and the value as the new value. This also allows you to skip updating some output components. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    food_box = gr.Number(value=10, label=\"Food Count\")\n    status_box = gr.Textbox()\n    def eat(food):\n        if food > 0:\n            return {food_box: food - 1, status_box: \"full\"}\n        else:\n            return {status_box: \"hungry\"}\n    gr.Button(\"EAT\").click(\n        fn=eat, \n        inputs=food_box,\n        outputs=[food_box, status_box]\n    )\n</code></pre></div>\n\n<p>Notice how when there is no food, we only update the <code>status_box</code> element. We skipped updating the <code>food_box</code> component.</p>\n\n<p>Dictionary returns are helpful when an event listener affects many components on return, or conditionally affects outputs and not others.</p>\n\n<p>Keep in mind that with dictionary returns, we still need to specify the possible outputs in the event listener.</p>\n\n<h2 id=\"updating-component-configurations\">Updating Component Configurations</h2>\n\n<p>The return value of an event listener function is usually the updated value of the corresponding output Component. Sometimes we want to update the configuration of the Component as well, such as the visibility. In this case, we return a <code>gr.update()</code> object instead of just the update Component value.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef change_textbox(choice):\n    if choice == \"short\":\n        return gr.update(lines=2, visible=True, value=\"Short story: \")\n    elif choice == \"long\":\n        return gr.update(lines=8, visible=True, value=\"Long story...\")\n    else:\n        return gr.update(visible=False)\n\nwith gr.Blocks() as demo:\n    radio = gr.Radio(\n        [\"short\", \"long\", \"none\"], label=\"Essay Length to Write?\"\n    )\n    text = gr.Textbox(lines=2, interactive=True)\n    radio.change(fn=change_textbox, inputs=radio, outputs=text)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks<em>essay</em>update'></gradio-app></p>\n\n<p>See how we can configure the Textbox itself through the <code>gr.update()</code> method. The <code>value=</code> argument can still be used to update the value along with Component configuration.</p>\n", "tags": [], "spaces": [], "url": "/guides/blocks-and-event-listeners/", "contributor": null}, {"name": "controlling-layout", "category": "building-with-blocks", "pretty_category": "Building With Blocks", "guide_index": 2, "absolute_index": 9, "pretty_name": "Controlling Layout", "content": "# Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n\n## Rows\n\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n```\n\nTo make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row().style(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nLearn more about Rows in the [docs](https://gradio.app/docs/#row). \n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually  nested within Rows. For example:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\").style(full_width=True)\n\ndemo.launch()\n```\n<gradio-app space='gradio/rows_and_columns'></gradio-app>\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. \n\nColumns have a `min_width` parameter as well (320 pixels by default). This prevents adjacent columns from becoming too narrow on mobile screens.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column). \n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gradio.Tab('tab_name'):` clause. Any component created inside of a `with gradio.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n    \ndemo.launch()\n```\n<gradio-app space='gradio/blocks_flipper'></gradio-app>\n\nAlso note the `gradio.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gradio.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/#tab) and [Accordions](https://gradio.app/docs/#accordion) in the docs.\n\n## Visibility\n\nBoth Components and Layout elements have a `visible` argument that can set initially and also updated using `gr.update()`. Setting `gr.update(visible=...)` on a Column can be used to show or hide a set of Components.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\")\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.update(value=\"Enter name\", visible=True)}\n        if age < 0 or age > 200:\n            return {error_box: gr.update(value=\"Enter valid age\", visible=True)}\n        return {\n            output_col: gr.update(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\"\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n```\n<gradio-app space='gradio/blocks_form'></gradio-app>\n\n## Variable Number of Outputs\n\nBy adjusting the visibility of components in a dynamic way, it is possible to create \ndemos with Gradio that support a *variable numbers of outputs*. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:\n\n```python\nimport gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox.update(visible=True)]*k + [gr.Textbox.update(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n```\n<gradio-app space='gradio/variable_outputs'></gradio-app>\n\n\n## Defining and Rendering Components Separately\n\nIn some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using `gr.Examples` above the corresponding `gr.Textbox` input. Since `gr.Examples` requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the `gr.Examples` object.\n\nThe solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.\n\nHere's a full code example:\n\n```python\ninput_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n```\n\n", "html": "<h1 id=\"controlling-layout\">Controlling Layout</h1>\n\n<p>By default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the <a rel=\"noopener\" target=\"_blank\" href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox\">flexbox model of web development</a>.</p>\n\n<h2 id=\"rows\">Rows</h2>\n\n<p>Elements within a <code>with gr.Row</code> clause will all be displayed horizontally. For example, to display two Buttons side by side:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n</code></pre></div>\n\n<p>To make every element in a Row have the same height, use the <code>equal_height</code> argument of the <code>style</code> method.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row().style(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n</code></pre></div>\n\n<p>Learn more about Rows in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#row\">docs</a>. </p>\n\n<h2 id=\"columns-and-nesting\">Columns and Nesting</h2>\n\n<p>Components within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually  nested within Rows. For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\").style(full_width=True)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/rows<em>and</em>columns'></gradio-app></p>\n\n<p>See how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the <code>scale</code> parameter. The column with twice the <code>scale</code> value takes up twice the width. </p>\n\n<p>Columns have a <code>min_width</code> parameter as well (320 pixels by default). This prevents adjacent columns from becoming too narrow on mobile screens.</p>\n\n<p>Learn more about Columns in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#column\">docs</a>. </p>\n\n<h2 id=\"tabs-and-accordions\">Tabs and Accordions</h2>\n\n<p>You can also create Tabs using the <code>with gradio.Tab('tab_name'):</code> clause. Any component created inside of a <code>with gradio.Tab('tab_name'):</code> context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.</p>\n\n<p>For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_flipper'></gradio-app></p>\n\n<p>Also note the <code>gradio.Accordion('label')</code> in this example. The Accordion is a layout that can be toggled open or closed. Like <code>Tabs</code>, it is a layout element that can selectively hide or show content. Any components that are defined inside of a <code>with gradio.Accordion('label'):</code> will be hidden or shown when the accordion's toggle icon is clicked.</p>\n\n<p>Learn more about <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#tab\">Tabs</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#accordion\">Accordions</a> in the docs.</p>\n\n<h2 id=\"visibility\">Visibility</h2>\n\n<p>Both Components and Layout elements have a <code>visible</code> argument that can set initially and also updated using <code>gr.update()</code>. Setting <code>gr.update(visible=...)</code> on a Column can be used to show or hide a set of Components.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\")\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.update(value=\"Enter name\", visible=True)}\n        if age < 0 or age > 200:\n            return {error_box: gr.update(value=\"Enter valid age\", visible=True)}\n        return {\n            output_col: gr.update(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\"\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_form'></gradio-app></p>\n\n<h2 id=\"variable-number-of-outputs\">Variable Number of Outputs</h2>\n\n<p>By adjusting the visibility of components in a dynamic way, it is possible to create \ndemos with Gradio that support a <em>variable numbers of outputs</em>. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox.update(visible=True)]*k + [gr.Textbox.update(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/variable_outputs'></gradio-app></p>\n\n<h2 id=\"defining-and-rendering-components-separately\">Defining and Rendering Components Separately</h2>\n\n<p>In some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using <code>gr.Examples</code> above the corresponding <code>gr.Textbox</code> input. Since <code>gr.Examples</code> requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the <code>gr.Examples</code> object.</p>\n\n<p>The solution to this is to define the <code>gr.Textbox</code> outside of the <code>gr.Blocks()</code> scope and use the component's <code>.render()</code> method wherever you'd like it placed in the UI.</p>\n\n<p>Here's a full code example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>input_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n</code></pre></div>\n", "tags": [], "spaces": [], "url": "/guides/controlling-layout/", "contributor": null}, {"name": "state-in-blocks", "category": "building-with-blocks", "pretty_category": "Building With Blocks", "guide_index": 3, "absolute_index": 10, "pretty_name": "State In Blocks", "content": "# State in Blocks\n\nWe covered [State in Interfaces](https://gradio.app/interface-state), this guide takes a look at state in Blocks, which works mostly the same. \n\n## Global State\n\nGlobal state in Blocks works the same as in Interface. Any variable created outside a function call is a reference shared between all users.\n\n## Session State\n\nGradio supports session **state**, where data persists across multiple submits within a page session, in Blocks apps as well. To reiterate, session data is *not* shared between different users of your model. To store data in a session state, you need to do three things:\n\n1. Create a `gr.State()` object. If there is a default value to this stateful object, pass that into the constructor.\n2. In the event listener, put the `State` object as an input and output.\n3. In the event listener function, add the variable to the input parameters and the return value.\n\nLet's take a look at a game of hangman. \n\n```python\nimport gradio as gr\nimport random\n\nsecret_word = \"gradio\"\n\nwith gr.Blocks() as demo:    \n    used_letters_var = gr.State([])\n    with gr.Row() as row:\n        with gr.Column():\n            input_letter = gr.Textbox(label=\"Enter letter\")\n            btn = gr.Button(\"Guess Letter\")\n        with gr.Column():\n            hangman = gr.Textbox(\n                label=\"Hangman\",\n                value=\"_\"*len(secret_word)\n            )\n            used_letters_box = gr.Textbox(label=\"Used Letters\")\n\n    def guess_letter(letter, used_letters):\n        used_letters.append(letter)\n        answer = \"\".join([\n            (letter if letter in used_letters else \"_\")\n            for letter in secret_word\n        ])\n        return {\n            used_letters_var: used_letters,\n            used_letters_box: \", \".join(used_letters),\n            hangman: answer\n        }\n    btn.click(\n        guess_letter, \n        [input_letter, used_letters_var],\n        [used_letters_var, used_letters_box, hangman]\n        )\ndemo.launch()\n```\n<gradio-app space='gradio/hangman'></gradio-app>\n\nLet's see how we do each of the 3 steps listed above in this game:\n\n1. We store the used letters in `used_letters_var`. In the constructor of `State`, we set the initial value of this to `[]`, an empty list. \n2. In `btn.click()`, we have a reference to `used_letters_var` in both the inputs and outputs.\n3. In `guess_letter`, we pass the value of this `State` to `used_letters`, and then return an updated value of this `State` in the return statement.\n\nWith more complex apps, you will likely have many State variables storing session state in a single Blocks app.\n\nLearn more about `State` in the [docs](https://gradio.app/docs#state).\n\n\n\n", "html": "<h1 id=\"state-in-blocks\">State in Blocks</h1>\n\n<p>We covered <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/interface-state\">State in Interfaces</a>, this guide takes a look at state in Blocks, which works mostly the same. </p>\n\n<h2 id=\"global-state\">Global State</h2>\n\n<p>Global state in Blocks works the same as in Interface. Any variable created outside a function call is a reference shared between all users.</p>\n\n<h2 id=\"session-state\">Session State</h2>\n\n<p>Gradio supports session <strong>state</strong>, where data persists across multiple submits within a page session, in Blocks apps as well. To reiterate, session data is <em>not</em> shared between different users of your model. To store data in a session state, you need to do three things:</p>\n\n<ol>\n<li>Create a <code>gr.State()</code> object. If there is a default value to this stateful object, pass that into the constructor.</li>\n<li>In the event listener, put the <code>State</code> object as an input and output.</li>\n<li>In the event listener function, add the variable to the input parameters and the return value.</li>\n</ol>\n\n<p>Let's take a look at a game of hangman. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nimport random\n\nsecret_word = \"gradio\"\n\nwith gr.Blocks() as demo:    \n    used_letters_var = gr.State([])\n    with gr.Row() as row:\n        with gr.Column():\n            input_letter = gr.Textbox(label=\"Enter letter\")\n            btn = gr.Button(\"Guess Letter\")\n        with gr.Column():\n            hangman = gr.Textbox(\n                label=\"Hangman\",\n                value=\"_\"*len(secret_word)\n            )\n            used_letters_box = gr.Textbox(label=\"Used Letters\")\n\n    def guess_letter(letter, used_letters):\n        used_letters.append(letter)\n        answer = \"\".join([\n            (letter if letter in used_letters else \"_\")\n            for letter in secret_word\n        ])\n        return {\n            used_letters_var: used_letters,\n            used_letters_box: \", \".join(used_letters),\n            hangman: answer\n        }\n    btn.click(\n        guess_letter, \n        [input_letter, used_letters_var],\n        [used_letters_var, used_letters_box, hangman]\n        )\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/hangman'></gradio-app></p>\n\n<p>Let's see how we do each of the 3 steps listed above in this game:</p>\n\n<ol>\n<li>We store the used letters in <code>used_letters_var</code>. In the constructor of <code>State</code>, we set the initial value of this to <code>[]</code>, an empty list. </li>\n<li>In <code>btn.click()</code>, we have a reference to <code>used_letters_var</code> in both the inputs and outputs.</li>\n<li>In <code>guess_letter</code>, we pass the value of this <code>State</code> to <code>used_letters</code>, and then return an updated value of this <code>State</code> in the return statement.</li>\n</ol>\n\n<p>With more complex apps, you will likely have many State variables storing session state in a single Blocks app.</p>\n\n<p>Learn more about <code>State</code> in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#state\">docs</a>.</p>\n", "tags": [], "spaces": [], "url": "/guides/state-in-blocks/", "contributor": null}, {"name": "custom-CSS-and-JS", "category": "building-with-blocks", "pretty_category": "Building With Blocks", "guide_index": 4, "absolute_index": 11, "pretty_name": "Custom CSS And JS", "content": "# Custom JS and CSS\n\nThis guide covers how to style Blocks with more flexibility, as well as adding Javascript code to event listeners. \n\n## Custom CSS\n\nFor additional styling ability, you can pass any CSS to your app using the `css=` kwarg.\n\nThe base class for the Gradio app is `gradio-container`, so here's an example that changes the background color of the Gradio app:\n```python\nwith gr.Blocks(css=\".gradio-container {background-color: red}\") as demo:\n    ...\n```\n\nIf you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with `\"file=\"`, for example:\n\n```python\nwith gr.Blocks(css=\".gradio-container {background-image: url('file=clouds.jpg')}\") as demo:\n    ...\n```\n\nYou can also pass the filepath to a CSS file to the `css` argument.\n\n## The `elem_id` Argument\n\nYou can `elem_id` to add an HTML element `id` to any component. This will allow you to select elements more easily with CSS.\n\n```python\nwith gr.Blocks(css=\"#warning {color: red}\") as demo:\n    box1 = gr.Textbox(value=\"Good Job\")\n    box2 = gr.Textbox(value=\"Failure\", elem_id=\"warning\")\n```\n\nThe CSS ruleset will only target the second Textbox here.\n\n## Custom JS\n\nEvent listeners have a `_js` argument that can take a Javascript function as a string and treat it just like a Python event listener function. You can pass both a Javascript function and a Python function (in which case the Javascript function is run first) or only Javascript (and set the Python `fn` to `None`). Take a look at the code below:\n\n```python\nimport gradio as gr\n\nblocks = gr.Blocks()\n\nwith blocks as demo:\n    subject = gr.Textbox(placeholder=\"subject\")\n    verb = gr.Radio([\"ate\", \"loved\", \"hated\"])\n    object = gr.Textbox(placeholder=\"object\")\n\n    with gr.Row():\n        btn = gr.Button(\"Create sentence.\")\n        reverse_btn = gr.Button(\"Reverse sentence.\")\n        foo_bar_btn = gr.Button(\"Foo bar.\")\n    \n    def sentence_maker(w1, w2, w3):\n        return f\"{w1} {w2} {w3}\"\n\n    output1 = gr.Textbox(label=\"output 1\")\n    output2 = gr.Textbox(label=\"verb\")\n    output3 = gr.Textbox(label=\"verb reversed\")\n\n    btn.click(sentence_maker, [subject, verb, object], output1)\n    reverse_btn.click(None, [subject, verb, object], output2, _js=\"(s, v, o) => o + ' ' + v + ' ' + s\")\n    verb.change(lambda x: x, verb, output3, _js=\"(x) => [...x].reverse().join('')\")\n    foo_bar_btn.click(None, [], subject, _js=\"(x) => x + ' foo'\")\n\ndemo.launch()\n```\n<gradio-app space='gradio/blocks_js_methods'></gradio-app>", "html": "<h1 id=\"custom-js-and-css\">Custom JS and CSS</h1>\n\n<p>This guide covers how to style Blocks with more flexibility, as well as adding Javascript code to event listeners. </p>\n\n<h2 id=\"custom-css\">Custom CSS</h2>\n\n<p>For additional styling ability, you can pass any CSS to your app using the <code>css=</code> kwarg.</p>\n\n<p>The base class for the Gradio app is <code>gradio-container</code>, so here's an example that changes the background color of the Gradio app:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks(css=\".gradio-container {background-color: red}\") as demo:\n    ...\n</code></pre></div>\n\n<p>If you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with <code>\"file=\"</code>, for example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks(css=\".gradio-container {background-image: url('file=clouds.jpg')}\") as demo:\n    ...\n</code></pre></div>\n\n<p>You can also pass the filepath to a CSS file to the <code>css</code> argument.</p>\n\n<h2 id=\"the-elem_id-argument\">The <code>elem_id</code> Argument</h2>\n\n<p>You can <code>elem_id</code> to add an HTML element <code>id</code> to any component. This will allow you to select elements more easily with CSS.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks(css=\"#warning {color: red}\") as demo:\n    box1 = gr.Textbox(value=\"Good Job\")\n    box2 = gr.Textbox(value=\"Failure\", elem_id=\"warning\")\n</code></pre></div>\n\n<p>The CSS ruleset will only target the second Textbox here.</p>\n\n<h2 id=\"custom-js\">Custom JS</h2>\n\n<p>Event listeners have a <code>_js</code> argument that can take a Javascript function as a string and treat it just like a Python event listener function. You can pass both a Javascript function and a Python function (in which case the Javascript function is run first) or only Javascript (and set the Python <code>fn</code> to <code>None</code>). Take a look at the code below:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nblocks = gr.Blocks()\n\nwith blocks as demo:\n    subject = gr.Textbox(placeholder=\"subject\")\n    verb = gr.Radio([\"ate\", \"loved\", \"hated\"])\n    object = gr.Textbox(placeholder=\"object\")\n\n    with gr.Row():\n        btn = gr.Button(\"Create sentence.\")\n        reverse_btn = gr.Button(\"Reverse sentence.\")\n        foo_bar_btn = gr.Button(\"Foo bar.\")\n\n    def sentence_maker(w1, w2, w3):\n        return f\"{w1} {w2} {w3}\"\n\n    output1 = gr.Textbox(label=\"output 1\")\n    output2 = gr.Textbox(label=\"verb\")\n    output3 = gr.Textbox(label=\"verb reversed\")\n\n    btn.click(sentence_maker, [subject, verb, object], output1)\n    reverse_btn.click(None, [subject, verb, object], output2, _js=\"(s, v, o) => o + ' ' + v + ' ' + s\")\n    verb.change(lambda x: x, verb, output3, _js=\"(x) => [...x].reverse().join('')\")\n    foo_bar_btn.click(None, [], subject, _js=\"(x) => x + ' foo'\")\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks<em>js</em>methods'></gradio-app></p>\n", "tags": [], "spaces": [], "url": "/guides/custom-CSS-and-JS/", "contributor": null}, {"name": "custom-interpretations-with-blocks", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 29, "pretty_name": "Custom Interpretations With Blocks", "content": "# Custom Machine Learning Interpretations with Blocks\nTags: INTERPRETATION, SENTIMENT ANALYSIS\n\n**Prerequisite**: This Guide requires you to know about Blocks and the interpretation feature of Interfaces.\nMake sure to [read the Guide to Blocks first](https://gradio.app/quickstart/#blocks-more-flexibility-and-control) as well as the\ninterpretation section of the [Advanced Interface Features Guide](/advanced-interface-features#interpreting-your-predictions).\n\n## Introduction\n\nIf you have experience working with the Interface class, then you know that interpreting the prediction of your machine learning model\nis as easy as setting the `interpretation` parameter to either \"default\" or \"shap\".\n\nYou may be wondering if it is possible to add the same interpretation functionality to an app built with the Blocks API.\nNot only is it possible, but the flexibility of Blocks lets you display the interpretation output in ways that are\nimpossible to do with Interfaces!\n\nThis guide will show how to:\n\n1. Recreate the behavior of Interfaces's interpretation feature in a Blocks app.\n2. Customize how interpretations are displayed in a Blocks app.\n\nLet's get started!\n\n## Setting up the Blocks app\n\nLet's build a sentiment classification app with the Blocks API.\nThis app will take text as input and output the probability that this text expresses either negative or positive sentiment.\nWe'll have a single input `Textbox` and a single output `Label` component.\nBelow is the code for the app as well as the app itself.\n\n```python\nimport gradio as gr \nfrom transformers import pipeline\n\nsentiment_classifier = pipeline(\"text-classification\", return_all_scores=True)\n\ndef classifier(text):\n    pred = sentiment_classifier(text)\n    return {p[\"label\"]: p[\"score\"] for p in pred[0]}\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n\n    classify.click(classifier, input_text, label)\ndemo.launch()\n```\n\n<gradio-app space=\"freddyaboulton/sentiment-classification\"> </gradio-app>\n\n## Adding interpretations to the app\n\nOur goal is to present to our users how the words in the input contribute to the model's prediction.\nThis will help our users understand how the model works and also evaluate its effectiveness.\nFor example, we should expect our model to identify the words \"happy\" and \"love\" with positive sentiment - if not it's a sign we made a mistake in training it!\n\nFor each word in the input, we will compute a score of how much the model's prediction of positive sentiment is changed by that word.\nOnce we have those `(word, score)` pairs we can use gradio to visualize them for the user.\n\nThe [shap](https://shap.readthedocs.io/en/stable/index.html) library will help us compute the `(word, score)` pairs and\ngradio will take care of displaying the output to the user.\n\nThe following code computes the `(word, score)` pairs:\n\n```python\ndef interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n    \n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n    # Scores contains (word, score) pairs\n    \n    \n    # Format expected by gr.components.Interpretation\n    return {\"original\": text, \"interpretation\": scores}\n```\n\nNow, all we have to do is add a button that runs this function when clicked.\nTo display the interpretation, we will use `gr.components.Interpretation`.\nThis will color each word in the input either red or blue.\nRed if it contributes to positive sentiment and blue if it contributes to negative sentiment.\nThis is how `Interface` displays the interpretation output for text.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            interpretation = gr.components.Interpretation(input_text)\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, interpretation)\n\ndemo.launch()\n```\n\n<gradio-app space=\"freddyaboulton/sentiment-classification-interpretation\"> </gradio-app>\n\n\n## Customizing how the interpretation is displayed\n\nThe `gr.components.Interpretation` component does a good job of showing how individual words contribute to the sentiment prediction,\nbut what if we also wanted to display the score themselves along with the words?\n\nOne way to do this would be to generate a bar plot where the words are on the horizontal axis and the bar height corresponds\nto the shap score.\n\nWe can do this by modifying our `interpretation_function` to additionally return a matplotlib bar plot.\nWe will display it with the `gr.Plot` component in a separate tab.\n\nThis is how the interpretation function will look:\n```python\ndef interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n\n    scores_desc = sorted(scores, key=lambda t: t[1])[::-1]\n\n    # Filter out empty string added by shap\n    scores_desc = [t for t in scores_desc if t[0] != \"\"]\n\n    fig_m = plt.figure()\n    \n    # Select top 5 words that contribute to positive sentiment\n    plt.bar(x=[s[0] for s in scores_desc[:5]],\n            height=[s[1] for s in scores_desc[:5]])\n    plt.title(\"Top words contributing to positive sentiment\")\n    plt.ylabel(\"Shap Value\")\n    plt.xlabel(\"Word\")\n    return {\"original\": text, \"interpretation\": scores}, fig_m\n```\n\nAnd this is how the app code will look:\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            with gr.Tabs():\n                with gr.TabItem(\"Display interpretation with built-in component\"):\n                    interpretation = gr.components.Interpretation(input_text)\n                with gr.TabItem(\"Display interpretation with plot\"):\n                    interpretation_plot = gr.Plot()\n\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, [interpretation, interpretation_plot])\n\ndemo.launch()\n```\n\nYou can see the demo below!\n\n<gradio-app space=\"freddyaboulton/sentiment-classification-interpretation-tabs\"> </gradio-app>\n\n## Beyond Sentiment Classification\nAlthough we have focused on sentiment classification so far, you can add interpretations to almost any machine learning model.\nThe output must be an `gr.Image` or `gr.Label` but the input can be almost anything (`gr.Number`, `gr.Slider`, `gr.Radio`, `gr.Image`).\n\nHere is a demo built with blocks of interpretations for an image classification model:\n\n<gradio-app space=\"freddyaboulton/image-classification-interpretation-blocks\"> </gradio-app>\n\n\n## Closing remarks\n\nWe did a deep dive \ud83e\udd3f into how interpretations work and how you can add them to your Blocks app.\n\nWe also showed how the Blocks API gives you the power to control how the interpretation is visualized in your app.\n\nAdding interpretations is a helpful way to make your users understand and gain trust in your model.\nNow you have all the tools you need to add them to all of your apps!\n", "html": "<h1 id=\"custom-machine-learning-interpretations-with-blocks\">Custom Machine Learning Interpretations with Blocks</h1>\n\n<p><strong>Prerequisite</strong>: This Guide requires you to know about Blocks and the interpretation feature of Interfaces.\nMake sure to <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/quickstart/#blocks-more-flexibility-and-control\">read the Guide to Blocks first</a> as well as the\ninterpretation section of the <a rel=\"noopener\" target=\"_blank\" href=\"/advanced-interface-features#interpreting-your-predictions\">Advanced Interface Features Guide</a>.</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>If you have experience working with the Interface class, then you know that interpreting the prediction of your machine learning model\nis as easy as setting the <code>interpretation</code> parameter to either \"default\" or \"shap\".</p>\n\n<p>You may be wondering if it is possible to add the same interpretation functionality to an app built with the Blocks API.\nNot only is it possible, but the flexibility of Blocks lets you display the interpretation output in ways that are\nimpossible to do with Interfaces!</p>\n\n<p>This guide will show how to:</p>\n\n<ol>\n<li>Recreate the behavior of Interfaces's interpretation feature in a Blocks app.</li>\n<li>Customize how interpretations are displayed in a Blocks app.</li>\n</ol>\n\n<p>Let's get started!</p>\n\n<h2 id=\"setting-up-the-blocks-app\">Setting up the Blocks app</h2>\n\n<p>Let's build a sentiment classification app with the Blocks API.\nThis app will take text as input and output the probability that this text expresses either negative or positive sentiment.\nWe'll have a single input <code>Textbox</code> and a single output <code>Label</code> component.\nBelow is the code for the app as well as the app itself.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr \nfrom transformers import pipeline\n\nsentiment_classifier = pipeline(\"text-classification\", return_all_scores=True)\n\ndef classifier(text):\n    pred = sentiment_classifier(text)\n    return {p[\"label\"]: p[\"score\"] for p in pred[0]}\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n\n    classify.click(classifier, input_text, label)\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space=\"freddyaboulton/sentiment-classification\"> </gradio-app></p>\n\n<h2 id=\"adding-interpretations-to-the-app\">Adding interpretations to the app</h2>\n\n<p>Our goal is to present to our users how the words in the input contribute to the model's prediction.\nThis will help our users understand how the model works and also evaluate its effectiveness.\nFor example, we should expect our model to identify the words \"happy\" and \"love\" with positive sentiment - if not it's a sign we made a mistake in training it!</p>\n\n<p>For each word in the input, we will compute a score of how much the model's prediction of positive sentiment is changed by that word.\nOnce we have those <code>(word, score)</code> pairs we can use gradio to visualize them for the user.</p>\n\n<p>The <a rel=\"noopener\" target=\"_blank\" href=\"https://shap.readthedocs.io/en/stable/index.html\">shap</a> library will help us compute the <code>(word, score)</code> pairs and\ngradio will take care of displaying the output to the user.</p>\n\n<p>The following code computes the <code>(word, score)</code> pairs:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n\n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n    # Scores contains (word, score) pairs\n\n\n    # Format expected by gr.components.Interpretation\n    return {\"original\": text, \"interpretation\": scores}\n</code></pre></div>\n\n<p>Now, all we have to do is add a button that runs this function when clicked.\nTo display the interpretation, we will use <code>gr.components.Interpretation</code>.\nThis will color each word in the input either red or blue.\nRed if it contributes to positive sentiment and blue if it contributes to negative sentiment.\nThis is how <code>Interface</code> displays the interpretation output for text.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            interpretation = gr.components.Interpretation(input_text)\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, interpretation)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space=\"freddyaboulton/sentiment-classification-interpretation\"> </gradio-app></p>\n\n<h2 id=\"customizing-how-the-interpretation-is-displayed\">Customizing how the interpretation is displayed</h2>\n\n<p>The <code>gr.components.Interpretation</code> component does a good job of showing how individual words contribute to the sentiment prediction,\nbut what if we also wanted to display the score themselves along with the words?</p>\n\n<p>One way to do this would be to generate a bar plot where the words are on the horizontal axis and the bar height corresponds\nto the shap score.</p>\n\n<p>We can do this by modifying our <code>interpretation_function</code> to additionally return a matplotlib bar plot.\nWe will display it with the <code>gr.Plot</code> component in a separate tab.</p>\n\n<p>This is how the interpretation function will look:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n\n    scores_desc = sorted(scores, key=lambda t: t[1])[::-1]\n\n    # Filter out empty string added by shap\n    scores_desc = [t for t in scores_desc if t[0] != \"\"]\n\n    fig_m = plt.figure()\n\n    # Select top 5 words that contribute to positive sentiment\n    plt.bar(x=[s[0] for s in scores_desc[:5]],\n            height=[s[1] for s in scores_desc[:5]])\n    plt.title(\"Top words contributing to positive sentiment\")\n    plt.ylabel(\"Shap Value\")\n    plt.xlabel(\"Word\")\n    return {\"original\": text, \"interpretation\": scores}, fig_m\n</code></pre></div>\n\n<p>And this is how the app code will look:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            with gr.Tabs():\n                with gr.TabItem(\"Display interpretation with built-in component\"):\n                    interpretation = gr.components.Interpretation(input_text)\n                with gr.TabItem(\"Display interpretation with plot\"):\n                    interpretation_plot = gr.Plot()\n\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, [interpretation, interpretation_plot])\n\ndemo.launch()\n</code></pre></div>\n\n<p>You can see the demo below!</p>\n\n<p><gradio-app space=\"freddyaboulton/sentiment-classification-interpretation-tabs\"> </gradio-app></p>\n\n<h2 id=\"beyond-sentiment-classification\">Beyond Sentiment Classification</h2>\n\n<p>Although we have focused on sentiment classification so far, you can add interpretations to almost any machine learning model.\nThe output must be an <code>gr.Image</code> or <code>gr.Label</code> but the input can be almost anything (<code>gr.Number</code>, <code>gr.Slider</code>, <code>gr.Radio</code>, <code>gr.Image</code>).</p>\n\n<p>Here is a demo built with blocks of interpretations for an image classification model:</p>\n\n<p><gradio-app space=\"freddyaboulton/image-classification-interpretation-blocks\"> </gradio-app></p>\n\n<h2 id=\"closing-remarks\">Closing remarks</h2>\n\n<p>We did a deep dive \ud83e\udd3f into how interpretations work and how you can add them to your Blocks app.</p>\n\n<p>We also showed how the Blocks API gives you the power to control how the interpretation is visualized in your app.</p>\n\n<p>Adding interpretations is a helpful way to make your users understand and gain trust in your model.\nNow you have all the tools you need to add them to all of your apps!</p>\n", "tags": ["INTERPRETATION", "SENTIMENT ANALYSIS"], "spaces": [], "url": "/guides/custom-interpretations-with-blocks/", "contributor": null}, {"name": "using-blocks-like-functions", "category": "building-with-blocks", "pretty_category": "Building With Blocks", "guide_index": 5, "absolute_index": 12, "pretty_name": "Using Blocks Like Functions", "content": "# Using Gradio Blocks Like Functions\nTags: TRANSLATION, HUB, SPACES\n\n\n**Prerequisite**: This Guide builds on the Blocks Introduction. Make sure to [read that guide first](https://gradio.app/quickstart/#blocks-more-flexibility-and-control).\n\n## Introduction\n\nDid you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old python function!?\n\nThis means that if you have a gradio Blocks (or Interface) app called `demo`, you can use `demo` like you would any python function.\n\nSo doing something like `output = demo(\"Hello\", \"friend\")` will run the first event defined in `demo` on the inputs \"Hello\" and \"friend\" and store it\nin the variable `output`.\n\nIf I put you to sleep \ud83e\udd71, please bear with me! By using apps like functions, you can seamlessly compose Gradio apps.\nThe following section will show how.\n\n## Treating spaces like functions\n\nLet's say we have the following demo that translates english text to german text. \n\n```python\nimport gradio as gr\n\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"t5-base\")\n\n\ndef translate(text):\n    return pipe(text)[0][\"translation_text\"]\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            english = gr.Textbox(label=\"English text\")\n            translate_btn = gr.Button(value=\"Translate\")\n        with gr.Column():\n            german = gr.Textbox(label=\"German Text\")\n\n    translate_btn.click(translate, inputs=english, outputs=german, api_name=\"translate-to-german\")\n    examples = gr.Examples(examples=[\"I went to the supermarket yesterday.\", \"Helen is a good swimmer.\"],\n                           inputs=[english])\n\ndemo.launch()\n```\n\nI already went ahead and hosted it in Hugging Face spaces at [gradio/english_translator](https://huggingface.co/spaces/gradio/english_translator).\n\nYou can see the demo below as well:\n\n<gradio-app space='gradio/english_translator'></gradio-app>\n\nNow, let's say you have an app that generates english text, but you wanted to additionally generate german text.\n\nYou could either:\n\n1. Copy the source code of my english-to-german translation and paste it in your app.\n\n2. Load my english-to-german translation in your app and treat it like a normal python function.\n\nOption 1 technically always works, but it often introduces unwanted complexity.\n\nOption 2 lets you borrow the functionality you want without tightly coupling our apps.\n\nAll you have to do is call the `Blocks.load` class method in your source file.\nAfter that, you can use my translation app like a regular python function!\n\nThe following code snippet and demo shows how to use `Blocks.load`.\n\nNote that the variable `english_translator` is my english to german app, but its used in `generate_text` like a regular function.\n\n```python\nimport gradio as gr\n\nfrom transformers import pipeline\n\nenglish_translator = gr.Blocks.load(name=\"spaces/gradio/english_translator\")\nenglish_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n\n\ndef generate_text(text):\n    english_text = english_generator(text)[0][\"generated_text\"]\n    german_text = english_translator(english_text)\n    return english_text, german_text\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            seed = gr.Text(label=\"Input Phrase\")\n        with gr.Column():\n            english = gr.Text(label=\"Generated English Text\")\n            german = gr.Text(label=\"Generated German Text\")\n    btn = gr.Button(\"Generate\")\n    btn.click(generate_text, inputs=[seed], outputs=[english, german])\n    gr.Examples([\"My name is Clara and I am\"], inputs=[seed])\n\ndemo.launch()\n```\n\n<gradio-app space='gradio/generate_english_german'></gradio-app>\n\n## How to control which function in the app to use\n\nIf the app you are loading defines more than one function, you can specify which function to use\nwith the `fn_index` and `api_name` parameters.\n\nIn the code for our english to german demo, you'll see the following line:\n\n```python\ntranslate_btn.click(translate, inputs=english, outputs=german, api_name=\"translate-to-german\")\n```\n\nThe `api_name` gives this function a unique name in our app. You can use this name to tell gradio which\nfunction in the upstream space you want to use:\n\n```python\nenglish_generator(text, api_name=\"translate-to-german\")[0][\"generated_text\"]\n```\n\nYou can also use the `fn_index` parameter.\nImagine my app also defined an english to spanish translation function.\nIn order to use it in our text generation app, we would use the following code:\n\n```python\nenglish_generator(text, fn_index=1)[0][\"generated_text\"]\n```\n\nFunctions in gradio spaces are zero-indexed, so since the spanish translator would be the second function in my space,\nyou would use index 1. \n\n## Parting Remarks\n\nWe showed how treating a Blocks app like a regular python helps you compose functionality across different apps.\nAny Blocks app can be treated like a function, but a powerful pattern is to `load` an app hosted on \n[Hugging Face Spaces](https://huggingface.co/spaces) prior to treating it like a function in your own app.\nYou can also load models hosted on the [Hugging Face Model Hub](https://huggingface.co/models) - see the [Using Hugging Face Integrations](/using_hugging_face_integrations) guide for an example.\n\n### Happy building! \u2692\ufe0f\n", "html": "<h1 id=\"using-gradio-blocks-like-functions\">Using Gradio Blocks Like Functions</h1>\n\n<p><strong>Prerequisite</strong>: This Guide builds on the Blocks Introduction. Make sure to <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/quickstart/#blocks-more-flexibility-and-control\">read that guide first</a>.</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Did you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old python function!?</p>\n\n<p>This means that if you have a gradio Blocks (or Interface) app called <code>demo</code>, you can use <code>demo</code> like you would any python function.</p>\n\n<p>So doing something like <code>output = demo(\"Hello\", \"friend\")</code> will run the first event defined in <code>demo</code> on the inputs \"Hello\" and \"friend\" and store it\nin the variable <code>output</code>.</p>\n\n<p>If I put you to sleep \ud83e\udd71, please bear with me! By using apps like functions, you can seamlessly compose Gradio apps.\nThe following section will show how.</p>\n\n<h2 id=\"treating-spaces-like-functions\">Treating spaces like functions</h2>\n\n<p>Let's say we have the following demo that translates english text to german text. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"t5-base\")\n\n\ndef translate(text):\n    return pipe(text)[0][\"translation_text\"]\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            english = gr.Textbox(label=\"English text\")\n            translate_btn = gr.Button(value=\"Translate\")\n        with gr.Column():\n            german = gr.Textbox(label=\"German Text\")\n\n    translate_btn.click(translate, inputs=english, outputs=german, api_name=\"translate-to-german\")\n    examples = gr.Examples(examples=[\"I went to the supermarket yesterday.\", \"Helen is a good swimmer.\"],\n                           inputs=[english])\n\ndemo.launch()\n</code></pre></div>\n\n<p>I already went ahead and hosted it in Hugging Face spaces at <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/gradio/english_translator\">gradio/english_translator</a>.</p>\n\n<p>You can see the demo below as well:</p>\n\n<p><gradio-app space='gradio/english_translator'></gradio-app></p>\n\n<p>Now, let's say you have an app that generates english text, but you wanted to additionally generate german text.</p>\n\n<p>You could either:</p>\n\n<ol>\n<li><p>Copy the source code of my english-to-german translation and paste it in your app.</p></li>\n<li><p>Load my english-to-german translation in your app and treat it like a normal python function.</p></li>\n</ol>\n\n<p>Option 1 technically always works, but it often introduces unwanted complexity.</p>\n\n<p>Option 2 lets you borrow the functionality you want without tightly coupling our apps.</p>\n\n<p>All you have to do is call the <code>Blocks.load</code> class method in your source file.\nAfter that, you can use my translation app like a regular python function!</p>\n\n<p>The following code snippet and demo shows how to use <code>Blocks.load</code>.</p>\n\n<p>Note that the variable <code>english_translator</code> is my english to german app, but its used in <code>generate_text</code> like a regular function.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nfrom transformers import pipeline\n\nenglish_translator = gr.Blocks.load(name=\"spaces/gradio/english_translator\")\nenglish_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n\n\ndef generate_text(text):\n    english_text = english_generator(text)[0][\"generated_text\"]\n    german_text = english_translator(english_text)\n    return english_text, german_text\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            seed = gr.Text(label=\"Input Phrase\")\n        with gr.Column():\n            english = gr.Text(label=\"Generated English Text\")\n            german = gr.Text(label=\"Generated German Text\")\n    btn = gr.Button(\"Generate\")\n    btn.click(generate_text, inputs=[seed], outputs=[english, german])\n    gr.Examples([\"My name is Clara and I am\"], inputs=[seed])\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/generate<em>english</em>german'></gradio-app></p>\n\n<h2 id=\"how-to-control-which-function-in-the-app-to-use\">How to control which function in the app to use</h2>\n\n<p>If the app you are loading defines more than one function, you can specify which function to use\nwith the <code>fn_index</code> and <code>api_name</code> parameters.</p>\n\n<p>In the code for our english to german demo, you'll see the following line:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>translate_btn.click(translate, inputs=english, outputs=german, api_name=\"translate-to-german\")\n</code></pre></div>\n\n<p>The <code>api_name</code> gives this function a unique name in our app. You can use this name to tell gradio which\nfunction in the upstream space you want to use:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>english_generator(text, api_name=\"translate-to-german\")[0][\"generated_text\"]\n</code></pre></div>\n\n<p>You can also use the <code>fn_index</code> parameter.\nImagine my app also defined an english to spanish translation function.\nIn order to use it in our text generation app, we would use the following code:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>english_generator(text, fn_index=1)[0][\"generated_text\"]\n</code></pre></div>\n\n<p>Functions in gradio spaces are zero-indexed, so since the spanish translator would be the second function in my space,\nyou would use index 1. </p>\n\n<h2 id=\"parting-remarks\">Parting Remarks</h2>\n\n<p>We showed how treating a Blocks app like a regular python helps you compose functionality across different apps.\nAny Blocks app can be treated like a function, but a powerful pattern is to <code>load</code> an app hosted on \n<a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces\">Hugging Face Spaces</a> prior to treating it like a function in your own app.\nYou can also load models hosted on the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/models\">Hugging Face Model Hub</a> - see the <a rel=\"noopener\" target=\"_blank\" href=\"/using_hugging_face_integrations\">Using Hugging Face Integrations</a> guide for an example.</p>\n\n<h3 id=\"happy-building\">Happy building! \u2692\ufe0f</h3>\n", "tags": ["TRANSLATION", "HUB", "SPACES"], "spaces": [], "url": "/guides/using-blocks-like-functions/", "contributor": null}], "override_signature": "with gradio.Blocks():", "parent": "gradio"}, "row": {"class": null, "name": "Row", "description": "Row is a layout element within Blocks that renders all children horizontally.", "tags": {"guides": "controlling-layout"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "variant", "annotation": "str", "doc": "row type, 'default' (no background), 'panel' (gray background color and rounded corners), or 'compact' (rounded corners and no internal gap).", "default": "\"default\""}, {"name": "visible", "annotation": "bool", "doc": "If False, row will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": "with gradio.Blocks() as demo:\n    with gradio.Row():\n        gr.Image(\"lion.jpg\")\n        gr.Image(\"tiger.jpg\")\ndemo.launch()", "fns": [], "guides": [{"name": "controlling-layout", "category": "building-with-blocks", "pretty_category": "Building With Blocks", "guide_index": 2, "absolute_index": 9, "pretty_name": "Controlling Layout", "content": "# Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n\n## Rows\n\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n```\n\nTo make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row().style(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nLearn more about Rows in the [docs](https://gradio.app/docs/#row). \n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually  nested within Rows. For example:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\").style(full_width=True)\n\ndemo.launch()\n```\n<gradio-app space='gradio/rows_and_columns'></gradio-app>\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. \n\nColumns have a `min_width` parameter as well (320 pixels by default). This prevents adjacent columns from becoming too narrow on mobile screens.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column). \n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gradio.Tab('tab_name'):` clause. Any component created inside of a `with gradio.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n    \ndemo.launch()\n```\n<gradio-app space='gradio/blocks_flipper'></gradio-app>\n\nAlso note the `gradio.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gradio.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/#tab) and [Accordions](https://gradio.app/docs/#accordion) in the docs.\n\n## Visibility\n\nBoth Components and Layout elements have a `visible` argument that can set initially and also updated using `gr.update()`. Setting `gr.update(visible=...)` on a Column can be used to show or hide a set of Components.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\")\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.update(value=\"Enter name\", visible=True)}\n        if age < 0 or age > 200:\n            return {error_box: gr.update(value=\"Enter valid age\", visible=True)}\n        return {\n            output_col: gr.update(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\"\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n```\n<gradio-app space='gradio/blocks_form'></gradio-app>\n\n## Variable Number of Outputs\n\nBy adjusting the visibility of components in a dynamic way, it is possible to create \ndemos with Gradio that support a *variable numbers of outputs*. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:\n\n```python\nimport gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox.update(visible=True)]*k + [gr.Textbox.update(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n```\n<gradio-app space='gradio/variable_outputs'></gradio-app>\n\n\n## Defining and Rendering Components Separately\n\nIn some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using `gr.Examples` above the corresponding `gr.Textbox` input. Since `gr.Examples` requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the `gr.Examples` object.\n\nThe solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.\n\nHere's a full code example:\n\n```python\ninput_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n```\n\n", "html": "<h1 id=\"controlling-layout\">Controlling Layout</h1>\n\n<p>By default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the <a rel=\"noopener\" target=\"_blank\" href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox\">flexbox model of web development</a>.</p>\n\n<h2 id=\"rows\">Rows</h2>\n\n<p>Elements within a <code>with gr.Row</code> clause will all be displayed horizontally. For example, to display two Buttons side by side:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n</code></pre></div>\n\n<p>To make every element in a Row have the same height, use the <code>equal_height</code> argument of the <code>style</code> method.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row().style(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n</code></pre></div>\n\n<p>Learn more about Rows in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#row\">docs</a>. </p>\n\n<h2 id=\"columns-and-nesting\">Columns and Nesting</h2>\n\n<p>Components within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually  nested within Rows. For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\").style(full_width=True)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/rows<em>and</em>columns'></gradio-app></p>\n\n<p>See how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the <code>scale</code> parameter. The column with twice the <code>scale</code> value takes up twice the width. </p>\n\n<p>Columns have a <code>min_width</code> parameter as well (320 pixels by default). This prevents adjacent columns from becoming too narrow on mobile screens.</p>\n\n<p>Learn more about Columns in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#column\">docs</a>. </p>\n\n<h2 id=\"tabs-and-accordions\">Tabs and Accordions</h2>\n\n<p>You can also create Tabs using the <code>with gradio.Tab('tab_name'):</code> clause. Any component created inside of a <code>with gradio.Tab('tab_name'):</code> context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.</p>\n\n<p>For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_flipper'></gradio-app></p>\n\n<p>Also note the <code>gradio.Accordion('label')</code> in this example. The Accordion is a layout that can be toggled open or closed. Like <code>Tabs</code>, it is a layout element that can selectively hide or show content. Any components that are defined inside of a <code>with gradio.Accordion('label'):</code> will be hidden or shown when the accordion's toggle icon is clicked.</p>\n\n<p>Learn more about <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#tab\">Tabs</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#accordion\">Accordions</a> in the docs.</p>\n\n<h2 id=\"visibility\">Visibility</h2>\n\n<p>Both Components and Layout elements have a <code>visible</code> argument that can set initially and also updated using <code>gr.update()</code>. Setting <code>gr.update(visible=...)</code> on a Column can be used to show or hide a set of Components.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\")\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.update(value=\"Enter name\", visible=True)}\n        if age < 0 or age > 200:\n            return {error_box: gr.update(value=\"Enter valid age\", visible=True)}\n        return {\n            output_col: gr.update(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\"\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_form'></gradio-app></p>\n\n<h2 id=\"variable-number-of-outputs\">Variable Number of Outputs</h2>\n\n<p>By adjusting the visibility of components in a dynamic way, it is possible to create \ndemos with Gradio that support a <em>variable numbers of outputs</em>. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox.update(visible=True)]*k + [gr.Textbox.update(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/variable_outputs'></gradio-app></p>\n\n<h2 id=\"defining-and-rendering-components-separately\">Defining and Rendering Components Separately</h2>\n\n<p>In some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using <code>gr.Examples</code> above the corresponding <code>gr.Textbox</code> input. Since <code>gr.Examples</code> requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the <code>gr.Examples</code> object.</p>\n\n<p>The solution to this is to define the <code>gr.Textbox</code> outside of the <code>gr.Blocks()</code> scope and use the component's <code>.render()</code> method wherever you'd like it placed in the UI.</p>\n\n<p>Here's a full code example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>input_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n</code></pre></div>\n", "tags": [], "spaces": [], "url": "/guides/controlling-layout/", "contributor": null}], "override_signature": "with gradio.Row():", "parent": "gradio"}, "column": {"class": null, "name": "Column", "description": "Column is a layout element within Blocks that renders all children vertically. The widths of columns can be set through the `scale` and `min_width` parameters. If a certain scale results in a column narrower than min_width, the min_width parameter will win.", "tags": {"guides": "controlling-layout"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "scale", "annotation": "int", "doc": "relative width compared to adjacent Columns. For example, if Column A has scale=2, and Column B has scale=1, A will be twice as wide as B.", "default": "1"}, {"name": "min_width", "annotation": "int", "doc": "minimum pixel width of Column, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in a column narrower than min_width, the min_width parameter will be respected first.", "default": "320"}, {"name": "variant", "annotation": "str", "doc": "column type, 'default' (no background), 'panel' (gray background color and rounded corners), or 'compact' (rounded corners and no internal gap).", "default": "\"default\""}, {"name": "visible", "annotation": "bool", "doc": "If False, column will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": "with gradio.Blocks() as demo:\n    with gradio.Row():\n        with gradio.Column(scale=1):\n            text1 = gr.Textbox()\n            text2 = gr.Textbox()\n        with gradio.Column(scale=4):\n            btn1 = gr.Button(\"Button 1\")\n            btn2 = gr.Button(\"Button 2\")", "fns": [], "guides": [{"name": "controlling-layout", "category": "building-with-blocks", "pretty_category": "Building With Blocks", "guide_index": 2, "absolute_index": 9, "pretty_name": "Controlling Layout", "content": "# Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n\n## Rows\n\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n```\n\nTo make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row().style(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nLearn more about Rows in the [docs](https://gradio.app/docs/#row). \n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually  nested within Rows. For example:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\").style(full_width=True)\n\ndemo.launch()\n```\n<gradio-app space='gradio/rows_and_columns'></gradio-app>\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. \n\nColumns have a `min_width` parameter as well (320 pixels by default). This prevents adjacent columns from becoming too narrow on mobile screens.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column). \n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gradio.Tab('tab_name'):` clause. Any component created inside of a `with gradio.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n    \ndemo.launch()\n```\n<gradio-app space='gradio/blocks_flipper'></gradio-app>\n\nAlso note the `gradio.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gradio.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/#tab) and [Accordions](https://gradio.app/docs/#accordion) in the docs.\n\n## Visibility\n\nBoth Components and Layout elements have a `visible` argument that can set initially and also updated using `gr.update()`. Setting `gr.update(visible=...)` on a Column can be used to show or hide a set of Components.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\")\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.update(value=\"Enter name\", visible=True)}\n        if age < 0 or age > 200:\n            return {error_box: gr.update(value=\"Enter valid age\", visible=True)}\n        return {\n            output_col: gr.update(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\"\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n```\n<gradio-app space='gradio/blocks_form'></gradio-app>\n\n## Variable Number of Outputs\n\nBy adjusting the visibility of components in a dynamic way, it is possible to create \ndemos with Gradio that support a *variable numbers of outputs*. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:\n\n```python\nimport gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox.update(visible=True)]*k + [gr.Textbox.update(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n```\n<gradio-app space='gradio/variable_outputs'></gradio-app>\n\n\n## Defining and Rendering Components Separately\n\nIn some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using `gr.Examples` above the corresponding `gr.Textbox` input. Since `gr.Examples` requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the `gr.Examples` object.\n\nThe solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.\n\nHere's a full code example:\n\n```python\ninput_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n```\n\n", "html": "<h1 id=\"controlling-layout\">Controlling Layout</h1>\n\n<p>By default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the <a rel=\"noopener\" target=\"_blank\" href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox\">flexbox model of web development</a>.</p>\n\n<h2 id=\"rows\">Rows</h2>\n\n<p>Elements within a <code>with gr.Row</code> clause will all be displayed horizontally. For example, to display two Buttons side by side:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n</code></pre></div>\n\n<p>To make every element in a Row have the same height, use the <code>equal_height</code> argument of the <code>style</code> method.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row().style(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n</code></pre></div>\n\n<p>Learn more about Rows in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#row\">docs</a>. </p>\n\n<h2 id=\"columns-and-nesting\">Columns and Nesting</h2>\n\n<p>Components within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually  nested within Rows. For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\").style(full_width=True)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/rows<em>and</em>columns'></gradio-app></p>\n\n<p>See how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the <code>scale</code> parameter. The column with twice the <code>scale</code> value takes up twice the width. </p>\n\n<p>Columns have a <code>min_width</code> parameter as well (320 pixels by default). This prevents adjacent columns from becoming too narrow on mobile screens.</p>\n\n<p>Learn more about Columns in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#column\">docs</a>. </p>\n\n<h2 id=\"tabs-and-accordions\">Tabs and Accordions</h2>\n\n<p>You can also create Tabs using the <code>with gradio.Tab('tab_name'):</code> clause. Any component created inside of a <code>with gradio.Tab('tab_name'):</code> context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.</p>\n\n<p>For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_flipper'></gradio-app></p>\n\n<p>Also note the <code>gradio.Accordion('label')</code> in this example. The Accordion is a layout that can be toggled open or closed. Like <code>Tabs</code>, it is a layout element that can selectively hide or show content. Any components that are defined inside of a <code>with gradio.Accordion('label'):</code> will be hidden or shown when the accordion's toggle icon is clicked.</p>\n\n<p>Learn more about <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#tab\">Tabs</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#accordion\">Accordions</a> in the docs.</p>\n\n<h2 id=\"visibility\">Visibility</h2>\n\n<p>Both Components and Layout elements have a <code>visible</code> argument that can set initially and also updated using <code>gr.update()</code>. Setting <code>gr.update(visible=...)</code> on a Column can be used to show or hide a set of Components.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\")\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.update(value=\"Enter name\", visible=True)}\n        if age < 0 or age > 200:\n            return {error_box: gr.update(value=\"Enter valid age\", visible=True)}\n        return {\n            output_col: gr.update(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\"\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_form'></gradio-app></p>\n\n<h2 id=\"variable-number-of-outputs\">Variable Number of Outputs</h2>\n\n<p>By adjusting the visibility of components in a dynamic way, it is possible to create \ndemos with Gradio that support a <em>variable numbers of outputs</em>. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox.update(visible=True)]*k + [gr.Textbox.update(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/variable_outputs'></gradio-app></p>\n\n<h2 id=\"defining-and-rendering-components-separately\">Defining and Rendering Components Separately</h2>\n\n<p>In some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using <code>gr.Examples</code> above the corresponding <code>gr.Textbox</code> input. Since <code>gr.Examples</code> requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the <code>gr.Examples</code> object.</p>\n\n<p>The solution to this is to define the <code>gr.Textbox</code> outside of the <code>gr.Blocks()</code> scope and use the component's <code>.render()</code> method wherever you'd like it placed in the UI.</p>\n\n<p>Here's a full code example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>input_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n</code></pre></div>\n", "tags": [], "spaces": [], "url": "/guides/controlling-layout/", "contributor": null}], "override_signature": "with gradio.Column():", "parent": "gradio"}, "tab": {"class": null, "name": "Tab", "description": "Tab (or its alias TabItem) is a layout element. Components defined within the Tab will be visible when this tab is selected tab.", "tags": {"guides": "controlling-layout"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "label", "annotation": "str", "doc": "The visual label for the tab"}, {"name": "id", "annotation": "int | str | None", "doc": "An optional identifier for the tab, required if you wish to control the selected tab from a predict function.", "default": "None"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": "with gradio.Blocks() as demo:\n    with gradio.Tab(\"Lion\"):\n        gr.Image(\"lion.jpg\")\n        gr.Button(\"New Lion\")\n    with gradio.Tab(\"Tiger\"):\n        gr.Image(\"tiger.jpg\")\n        gr.Button(\"New Tiger\")", "fns": [], "guides": [{"name": "controlling-layout", "category": "building-with-blocks", "pretty_category": "Building With Blocks", "guide_index": 2, "absolute_index": 9, "pretty_name": "Controlling Layout", "content": "# Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n\n## Rows\n\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n```\n\nTo make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row().style(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nLearn more about Rows in the [docs](https://gradio.app/docs/#row). \n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually  nested within Rows. For example:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\").style(full_width=True)\n\ndemo.launch()\n```\n<gradio-app space='gradio/rows_and_columns'></gradio-app>\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. \n\nColumns have a `min_width` parameter as well (320 pixels by default). This prevents adjacent columns from becoming too narrow on mobile screens.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column). \n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gradio.Tab('tab_name'):` clause. Any component created inside of a `with gradio.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n    \ndemo.launch()\n```\n<gradio-app space='gradio/blocks_flipper'></gradio-app>\n\nAlso note the `gradio.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gradio.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/#tab) and [Accordions](https://gradio.app/docs/#accordion) in the docs.\n\n## Visibility\n\nBoth Components and Layout elements have a `visible` argument that can set initially and also updated using `gr.update()`. Setting `gr.update(visible=...)` on a Column can be used to show or hide a set of Components.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\")\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.update(value=\"Enter name\", visible=True)}\n        if age < 0 or age > 200:\n            return {error_box: gr.update(value=\"Enter valid age\", visible=True)}\n        return {\n            output_col: gr.update(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\"\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n```\n<gradio-app space='gradio/blocks_form'></gradio-app>\n\n## Variable Number of Outputs\n\nBy adjusting the visibility of components in a dynamic way, it is possible to create \ndemos with Gradio that support a *variable numbers of outputs*. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:\n\n```python\nimport gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox.update(visible=True)]*k + [gr.Textbox.update(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n```\n<gradio-app space='gradio/variable_outputs'></gradio-app>\n\n\n## Defining and Rendering Components Separately\n\nIn some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using `gr.Examples` above the corresponding `gr.Textbox` input. Since `gr.Examples` requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the `gr.Examples` object.\n\nThe solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.\n\nHere's a full code example:\n\n```python\ninput_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n```\n\n", "html": "<h1 id=\"controlling-layout\">Controlling Layout</h1>\n\n<p>By default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the <a rel=\"noopener\" target=\"_blank\" href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox\">flexbox model of web development</a>.</p>\n\n<h2 id=\"rows\">Rows</h2>\n\n<p>Elements within a <code>with gr.Row</code> clause will all be displayed horizontally. For example, to display two Buttons side by side:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n</code></pre></div>\n\n<p>To make every element in a Row have the same height, use the <code>equal_height</code> argument of the <code>style</code> method.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row().style(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n</code></pre></div>\n\n<p>Learn more about Rows in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#row\">docs</a>. </p>\n\n<h2 id=\"columns-and-nesting\">Columns and Nesting</h2>\n\n<p>Components within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually  nested within Rows. For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\").style(full_width=True)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/rows<em>and</em>columns'></gradio-app></p>\n\n<p>See how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the <code>scale</code> parameter. The column with twice the <code>scale</code> value takes up twice the width. </p>\n\n<p>Columns have a <code>min_width</code> parameter as well (320 pixels by default). This prevents adjacent columns from becoming too narrow on mobile screens.</p>\n\n<p>Learn more about Columns in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#column\">docs</a>. </p>\n\n<h2 id=\"tabs-and-accordions\">Tabs and Accordions</h2>\n\n<p>You can also create Tabs using the <code>with gradio.Tab('tab_name'):</code> clause. Any component created inside of a <code>with gradio.Tab('tab_name'):</code> context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.</p>\n\n<p>For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_flipper'></gradio-app></p>\n\n<p>Also note the <code>gradio.Accordion('label')</code> in this example. The Accordion is a layout that can be toggled open or closed. Like <code>Tabs</code>, it is a layout element that can selectively hide or show content. Any components that are defined inside of a <code>with gradio.Accordion('label'):</code> will be hidden or shown when the accordion's toggle icon is clicked.</p>\n\n<p>Learn more about <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#tab\">Tabs</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs/#accordion\">Accordions</a> in the docs.</p>\n\n<h2 id=\"visibility\">Visibility</h2>\n\n<p>Both Components and Layout elements have a <code>visible</code> argument that can set initially and also updated using <code>gr.update()</code>. Setting <code>gr.update(visible=...)</code> on a Column can be used to show or hide a set of Components.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\")\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.update(value=\"Enter name\", visible=True)}\n        if age < 0 or age > 200:\n            return {error_box: gr.update(value=\"Enter valid age\", visible=True)}\n        return {\n            output_col: gr.update(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\"\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_form'></gradio-app></p>\n\n<h2 id=\"variable-number-of-outputs\">Variable Number of Outputs</h2>\n\n<p>By adjusting the visibility of components in a dynamic way, it is possible to create \ndemos with Gradio that support a <em>variable numbers of outputs</em>. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox.update(visible=True)]*k + [gr.Textbox.update(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/variable_outputs'></gradio-app></p>\n\n<h2 id=\"defining-and-rendering-components-separately\">Defining and Rendering Components Separately</h2>\n\n<p>In some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using <code>gr.Examples</code> above the corresponding <code>gr.Textbox</code> input. Since <code>gr.Examples</code> requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the <code>gr.Examples</code> object.</p>\n\n<p>The solution to this is to define the <code>gr.Textbox</code> outside of the <code>gr.Blocks()</code> scope and use the component's <code>.render()</code> method wherever you'd like it placed in the UI.</p>\n\n<p>Here's a full code example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>input_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n</code></pre></div>\n", "tags": [], "spaces": [], "url": "/guides/controlling-layout/", "contributor": null}], "override_signature": "with gradio.Tab():", "parent": "gradio"}, "box": {"class": null, "name": "Box", "description": "Box is a a layout element which places children in a box with rounded corners and some padding around them.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "visible", "annotation": "bool", "doc": "If False, box will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": "with gradio.Box():\n    gr.Textbox(label=\"First\")\n    gr.Textbox(label=\"Last\")", "fns": [], "override_signature": "with gradio.Box():", "parent": "gradio"}, "accordion": {"class": null, "name": "Accordion", "description": "Accordion is a layout element which can be toggled to show/hide the contained content.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "label", "annotation": "<class 'inspect._empty'>", "doc": "name of accordion section."}, {"name": "open", "annotation": "bool", "doc": "if True, accordion is open by default.", "default": "True"}, {"name": "visible", "annotation": "bool", "doc": null, "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": "with gradio.Accordion(\"See Details\"):\n    gr.Markdown(\"lorem ipsum\")", "fns": [], "parent": "gradio"}, "interface": {"class": null, "name": "Interface", "description": "Interface is Gradio's main high-level class, and allows you to create a web-based GUI / demo around a machine learning model (or any Python function) in a few lines of code. You must specify three parameters: (1) the function to create a GUI for (2) the desired input components and (3) the desired output components. Additional parameters can be used to control the appearance and behavior of the demo. <br>", "tags": {"demos": "hello_world, hello_world_3, gpt_j", "guides": "quickstart, key-features, sharing-your-app, interface-state, reactive-interfaces, advanced-interface-features, setting-up-a-gradio-demo-for-maximum-performance"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "str | IOComponent | List[str | IOComponent] | None", "doc": "a single Gradio component, or list of Gradio components. Components can either be passed as instantiated objects, or referred to by their string shortcuts. The number of input components should match the number of parameters in fn. If set to None, then only the output components will be displayed."}, {"name": "outputs", "annotation": "str | IOComponent | List[str | IOComponent] | None", "doc": "a single Gradio component, or list of Gradio components. Components can either be passed as instantiated objects, or referred to by their string shortcuts. The number of output components should match the number of values returned by fn. If set to None, then only the input components will be displayed."}, {"name": "examples", "annotation": "List[Any] | List[List[Any]] | str | None", "doc": "sample inputs for the function; if provided, appear below the UI components and can be clicked to populate the interface. Should be nested list, in which the outer list consists of samples and each inner list consists of an input corresponding to each input component. A string path to a directory of examples can also be provided, but it should be within the directory with the python file running the gradio app. If there are multiple input components and a directory is provided, a log.csv file must be present in the directory to link corresponding inputs.", "default": "None"}, {"name": "cache_examples", "annotation": "bool | None", "doc": "If True, caches examples in the server for fast runtime in examples. The default option in HuggingFace Spaces is True. The default option elsewhere is False.", "default": "None"}, {"name": "examples_per_page", "annotation": "int", "doc": "If examples are provided, how many to display per page.", "default": "10"}, {"name": "live", "annotation": "bool", "doc": "whether the interface should automatically rerun if any of the inputs change.", "default": "False"}, {"name": "interpretation", "annotation": "Callable | str | None", "doc": "function that provides interpretation explaining prediction output. Pass \"default\" to use simple built-in interpreter, \"shap\" to use a built-in shapley-based interpreter, or your own custom interpretation function. For more information on the different interpretation methods, see the Advanced Interface Features guide.", "default": "None"}, {"name": "num_shap", "annotation": "float", "doc": "a multiplier that determines how many examples are computed for shap-based interpretation. Increasing this value will increase shap runtime, but improve results. Only applies if interpretation is \"shap\".", "default": "2.0"}, {"name": "title", "annotation": "str | None", "doc": "a title for the interface; if provided, appears above the input and output components in large font. Also used as the tab title when opened in a browser window.", "default": "None"}, {"name": "description", "annotation": "str | None", "doc": "a description for the interface; if provided, appears above the input and output components and beneath the title in regular font. Accepts Markdown and HTML content.", "default": "None"}, {"name": "article", "annotation": "str | None", "doc": "an expanded article explaining the interface; if provided, appears below the input and output components in regular font. Accepts Markdown and HTML content.", "default": "None"}, {"name": "thumbnail", "annotation": "str | None", "doc": "path or url to image to use as display image when the web demo is shared on social media.", "default": "None"}, {"name": "theme", "annotation": "str", "doc": "Theme to use - right now, only \"default\" is supported. Can be set with the GRADIO_THEME environment variable.", "default": "\"default\""}, {"name": "css", "annotation": "str | None", "doc": "custom css or path to custom css file to use with interface.", "default": "None"}, {"name": "allow_flagging", "annotation": "str | None", "doc": "one of \"never\", \"auto\", or \"manual\". If \"never\" or \"auto\", users will not see a button to flag an input and output. If \"manual\", users will see a button to flag. If \"auto\", every input the user submits will be automatically flagged (outputs are not flagged). If \"manual\", both the input and outputs are flagged when the user clicks flag button. This parameter can be set with environmental variable GRADIO_ALLOW_FLAGGING; otherwise defaults to \"manual\".", "default": "None"}, {"name": "flagging_options", "annotation": "List[str] | None", "doc": "if provided, allows user to select from the list of options when flagging. Only applies if allow_flagging is \"manual\".", "default": "None"}, {"name": "flagging_dir", "annotation": "str", "doc": "what to name the directory where flagged data is stored.", "default": "\"flagged\""}, {"name": "flagging_callback", "annotation": "FlaggingCallback", "doc": "An instance of a subclass of FlaggingCallback which will be called when a sample is flagged. By default logs to a local CSV file.", "default": "CSVLogger()"}, {"name": "analytics_enabled", "annotation": "bool | None", "doc": "Whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable if defined, or default to True.", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}], "returns": {"annotation": null}, "example": "import gradio as gr\n\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\n\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\")\ndemo.launch()", "fns": [{"fn": null, "name": "launch", "description": "Launches a simple web server that serves the demo. Can also be used to create a public link used by anyone to access the demo from their browser by setting share=True. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "inline", "annotation": "bool | None", "doc": "whether to display in the interface inline in an iframe. Defaults to True in python notebooks; False otherwise.", "default": "None"}, {"name": "inbrowser", "annotation": "bool", "doc": "whether to automatically launch the interface in a new tab on the default browser.", "default": "False"}, {"name": "share", "annotation": "bool | None", "doc": "whether to create a publicly shareable link for the interface. Creates an SSH tunnel to make your UI accessible from anywhere. If not provided, it is set to False by default every time, except when running in Google Colab. When localhost is not accessible (e.g. Google Colab), setting share=False is not supported.", "default": "None"}, {"name": "debug", "annotation": "bool", "doc": "if True, blocks the main thread from running. If running in Google Colab, this is needed to print the errors in the cell output.", "default": "False"}, {"name": "enable_queue", "annotation": "bool | None", "doc": "DEPRECATED (use .queue() method instead.) if True, inference requests will be served through a queue instead of with parallel threads. Required for longer inference times (> 1min) to prevent timeout. The default option in HuggingFace Spaces is True. The default option elsewhere is False.", "default": "None"}, {"name": "max_threads", "annotation": "int", "doc": "the maximum number of total threads that the Gradio app can generate in parallel. The default is inherited from the starlette library (currently 40). Applies whether the queue is enabled or not. But if queuing is enabled, this parameter is increaseed to be at least the concurrency_count of the queue.", "default": "40"}, {"name": "auth", "annotation": "Callable | Tuple[str, str] | List[Tuple[str, str]] | None", "doc": "If provided, username and password (or list of username-password tuples) required to access interface. Can also provide function that takes username and password and returns True if valid login.", "default": "None"}, {"name": "auth_message", "annotation": "str | None", "doc": "If provided, HTML message provided on login page.", "default": "None"}, {"name": "prevent_thread_lock", "annotation": "bool", "doc": "If True, the interface will block the main thread while the server is running.", "default": "False"}, {"name": "show_error", "annotation": "bool", "doc": "If True, any errors in the interface will be displayed in an alert modal and printed in the browser console log", "default": "False"}, {"name": "server_name", "annotation": "str | None", "doc": "to make app accessible on local network, set this to \"0.0.0.0\". Can be set by environment variable GRADIO_SERVER_NAME. If None, will use \"127.0.0.1\".", "default": "None"}, {"name": "server_port", "annotation": "int | None", "doc": "will start gradio app on this port (if available). Can be set by environment variable GRADIO_SERVER_PORT. If None, will search for an available port starting at 7860.", "default": "None"}, {"name": "show_tips", "annotation": "bool", "doc": "if True, will occasionally show tips about new Gradio features", "default": "False"}, {"name": "height", "annotation": "int", "doc": "The height in pixels of the iframe element containing the interface (used if inline=True)", "default": "500"}, {"name": "width", "annotation": "int | str", "doc": "The width in pixels of the iframe element containing the interface (used if inline=True)", "default": "\"100%\""}, {"name": "encrypt", "annotation": "bool", "doc": "If True, flagged data will be encrypted by key provided by creator at launch", "default": "False"}, {"name": "favicon_path", "annotation": "str | None", "doc": "If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for the web page.", "default": "None"}, {"name": "ssl_keyfile", "annotation": "str | None", "doc": "If a path to a file is provided, will use this as the private key file to create a local server running on https.", "default": "None"}, {"name": "ssl_certfile", "annotation": "str | None", "doc": "If a path to a file is provided, will use this as the signed certificate for https. Needs to be provided if ssl_keyfile is provided.", "default": "None"}, {"name": "ssl_keyfile_password", "annotation": "str | None", "doc": "If a password is provided, will use this with the ssl certificate for https.", "default": "None"}, {"name": "quiet", "annotation": "bool", "doc": "If True, suppresses most print statements.", "default": "False"}, {"name": "show_api", "annotation": "bool", "doc": "If True, shows the api docs in the footer of the app. Default True. If the queue is enabled, then api_open parameter of .queue() will determine if the api docs are shown, independent of the value of show_api.", "default": "True"}, {"name": "file_directories", "annotation": "List[str] | None", "doc": "List of directories that gradio is allowed to serve files from (in addition to the directory containing the gradio python file). Must be absolute paths. Warning: any files in these directories or its children are potentially accessible to all users of your app.", "default": "None"}], "returns": {}, "example": "import gradio as gr\ndef reverse(text):\n    return text[::-1]\ndemo = gr.Interface(reverse, \"text\", \"text\")\ndemo.launch(share=True, auth=(\"username\", \"password\"))", "parent": "gradio.Interface"}, {"fn": null, "name": "load", "description": "Class method that constructs an Interface from a Hugging Face repo. Can accept model repos (if src is \"models\") or Space repos (if src is \"spaces\"). The input and output components are automatically loaded from the repo.", "tags": {}, "parameters": [{"name": "name", "annotation": "str", "doc": "the name of the model (e.g. \"gpt2\" or \"facebook/bart-base\") or space (e.g. \"flax-community/spanish-gpt2\"), can include the `src` as prefix (e.g. \"models/facebook/bart-base\")"}, {"name": "src", "annotation": "str | None", "doc": "the source of the model: `models` or `spaces` (or leave empty if source is provided as a prefix in `name`)", "default": "None"}, {"name": "api_key", "annotation": "str | None", "doc": "optional access token for loading private Hugging Face Hub models or spaces. Find your token here: https://huggingface.co/settings/tokens", "default": "None"}, {"name": "alias", "annotation": "str | None", "doc": "optional string used as the name of the loaded model instead of the default name (only applies if loading a Space running Gradio 2.x)", "default": "None"}], "returns": {"annotation": "Interface", "doc": "a Gradio Interface object for the given model"}, "example": "import gradio as gr\ndescription = \"Story generation with GPT\"\nexamples = [[\"An adventurer is approached by a mysterious stranger in the tavern for a new quest.\"]]\ndemo = gr.Interface.load(\"models/EleutherAI/gpt-neo-1.3B\", description=description, examples=examples)\ndemo.launch()", "parent": "gradio.Interface"}, {"fn": null, "name": "from_pipeline", "description": "Class method that constructs an Interface from a Hugging Face transformers.Pipeline object. The input and output components are automatically determined from the pipeline.", "tags": {}, "parameters": [{"name": "pipeline", "annotation": "Pipeline", "doc": "the pipeline object to use."}], "returns": {"annotation": "Interface", "doc": "a Gradio Interface object from the given Pipeline"}, "example": "import gradio as gr\nfrom transformers import pipeline\npipe = pipeline(\"image-classification\")\ngr.Interface.from_pipeline(pipe).launch()", "parent": "gradio.Interface"}, {"fn": null, "name": "integrate", "description": "A catch-all method for integrating with other libraries. This method should be run after launch()", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "comet_ml", "annotation": "comet_ml.Experiment | None", "doc": "If a comet_ml Experiment object is provided, will integrate with the experiment and appear on Comet dashboard", "default": "None"}, {"name": "wandb", "annotation": "ModuleType | None", "doc": "If the wandb module is provided, will integrate with it and appear on WandB dashboard", "default": "None"}, {"name": "mlflow", "annotation": "ModuleType | None", "doc": "If the mlflow module  is provided, will integrate with the experiment and appear on ML Flow dashboard", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Interface"}, {"fn": null, "name": "queue", "description": "You can control the rate of processed requests by creating a queue. This will allow you to set the number of requests to be processed at one time, and will let users know their position in the queue.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "concurrency_count", "annotation": "int", "doc": "Number of worker threads that will be processing requests from the queue concurrently. Increasing this number will increase the rate at which requests are processed, but will also increase the memory usage of the queue.", "default": "1"}, {"name": "status_update_rate", "annotation": "float | Literal['auto']", "doc": "If \"auto\", Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.", "default": "\"auto\""}, {"name": "client_position_to_load_data", "annotation": "int | None", "doc": "DEPRECATED. This parameter is deprecated and has no effect.", "default": "None"}, {"name": "default_enabled", "annotation": "bool | None", "doc": "Deprecated and has no effect.", "default": "None"}, {"name": "api_open", "annotation": "bool", "doc": "If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.", "default": "True"}, {"name": "max_size", "annotation": "int | None", "doc": "The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited.", "default": "None"}], "returns": {}, "example": "demo = gr.Interface(gr.Textbox(), gr.Image(), image_generator)\ndemo.queue(concurrency_count=3)\ndemo.launch()", "parent": "gradio.Interface"}], "demos": [["hello_world", "import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n    \nif __name__ == \"__main__\":\n    demo.launch()   "], ["hello_world_3", "import gradio as gr\n\ndef greet(name, is_morning, temperature):\n    salutation = \"Good morning\" if is_morning else \"Good evening\"\n    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n    celsius = (temperature - 32) * 5 / 9\n    return greeting, round(celsius, 2)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n    outputs=[\"text\", \"number\"],\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["gpt_j", "import gradio as gr\n\ntitle = \"GPT-J-6B\"\n\nexamples = [\n    [\"The tower is 324 metres (1,063 ft) tall,\"],\n    [\"The Moon's orbit around Earth has\"],\n    [\"The smooth Borealis basin in the Northern Hemisphere covers 40%\"],\n]\n\ndemo = gr.Interface.load(\n    \"huggingface/EleutherAI/gpt-j-6B\",\n    inputs=gr.Textbox(lines=5, max_lines=6, label=\"Input Text\"),\n    title=title,\n    examples=examples,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "guides": [{"name": "quickstart", "category": "getting-started", "pretty_category": "Getting Started", "guide_index": 1, "absolute_index": 0, "pretty_name": "Quickstart", "content": "# Quickstart\n\n**Prerequisite**: Gradio requires Python 3.7 or higher, that's all!\n\n## What Does Gradio Do?\n\nOne of the *best ways to share* your machine learning model, API, or data science workflow with others is to create an **interactive app** that allows your users or colleagues to try out the demo in their browsers.\n\nGradio allows you to **build demos and share them, all in Python.** And usually in just a few lines of code! So let's get started.\n\n## Hello, World\n\nTo get Gradio running with a simple \"Hello, World\" example, follow these three steps:\n\n1\\. Install Gradio using pip:\n\n```bash\npip install gradio\n```\n\n2\\. Run the code below as a Python script or in a Jupyter Notebook (or [Google Colab](https://colab.research.google.com/drive/18ODkJvyxHutTN0P5APWyGFO_xwNcgHDZ?usp=sharing)):\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n    \ndemo.launch()   \n```\n\n3\\. The demo below will appear automatically within the Jupyter Notebook, or pop in a browser on [http://localhost:7860](http://localhost:7860) if running from a script:\n\n<gradio-app space='gradio/hello_world'></gradio-app>\n\nWhen developing locally, if you want to run the code as a Python script, you can use the Gradio CLI to launch the application **in reload mode**, which will provide seamless and fast development. Learn more about reloading in the [Auto-Reloading Guide](https://gradio.app/developing-faster-with-reload-mode/).\n\n```bash\ngradio app.py\n```\n\nNote: you can also do `python app.py`, but it won't provide the automatic reload mechanism.\n\n## The `Interface` Class\n\nYou'll notice that in order to make the demo, we created a `gradio.Interface`. This `Interface` class can wrap any Python function with a user interface. In the example above, we saw a simple text-based function, but the function could be anything from music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n\nThe core `Interface` class is initialized with three required parameters:\n\n- `fn`: the function to wrap a UI around\n- `inputs`: which component(s) to use for the input (e.g. `\"text\"`, `\"image\"` or `\"audio\"`)\n- `outputs`: which component(s) to use for the output (e.g. `\"text\"`, `\"image\"` or `\"label\"`)\n\nLet's take a closer look at these components used to provide input and output.\n\n## Components Attributes\n\nWe saw some simple `Textbox` components in the previous examples, but what if you want to change how the UI components look or behave?\n\nLet's say you want to customize the input text field \u2014 for example, you wanted it to be larger and have a text placeholder. If we use the actual class for `Textbox` instead of using the string shortcut, you have access to much more customizability through component attributes.\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=gr.Textbox(lines=2, placeholder=\"Name Here...\"),\n    outputs=\"text\",\n)\ndemo.launch()\n\n```\n<gradio-app space='gradio/hello_world_2'></gradio-app>\n\n## Multiple Input and Output Components\n\nSuppose you had a more complex function, with multiple inputs and outputs. In the example below, we define a function that takes a string, boolean, and number, and returns a string and number. Take a look how you pass a list of input and output components.\n\n```python\nimport gradio as gr\n\ndef greet(name, is_morning, temperature):\n    salutation = \"Good morning\" if is_morning else \"Good evening\"\n    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n    celsius = (temperature - 32) * 5 / 9\n    return greeting, round(celsius, 2)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n    outputs=[\"text\", \"number\"],\n)\ndemo.launch()\n\n```\n<gradio-app space='gradio/hello_world_3'></gradio-app>\n\nYou simply wrap the components in a list. Each component in the `inputs` list corresponds to one of the parameters of the function, in order. Each component in the `outputs` list corresponds to one of the values returned by the function, again in order.\n\n## An Image Example\n\nGradio supports many types of components, such as `Image`, `DataFrame`, `Video`, or `Label`. Let's try an image-to-image function to get a feel for these!\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img):\n    sepia_filter = np.array([\n        [0.393, 0.769, 0.189], \n        [0.349, 0.686, 0.168], \n        [0.272, 0.534, 0.131]\n    ])\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ndemo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \"image\")\ndemo.launch()\n\n```\n<gradio-app space='gradio/sepia_filter'></gradio-app>\n\nWhen using the `Image` component as input, your function will receive a NumPy array with the shape `(width, height, 3)`, where the last dimension represents the RGB values. We'll return an image as well in the form of a NumPy array.\n\nYou can also set the datatype used by the component with the `type=` keyword argument. For example, if you wanted your function to take a file path to an image instead of a NumPy array, the input `Image` component could be written as:\n\n```python\ngr.Image(type=\"filepath\", shape=...)\n```\n\nAlso note that our input `Image` component comes with an edit button \ud83d\udd89, which allows for cropping and zooming into images. Manipulating images in this way can help reveal biases or hidden flaws in a machine learning model!\n\nYou can read more about the many components and how to use them in the [Gradio docs](https://gradio.app/docs).\n\n## Blocks: More Flexibility and Control\n\nGradio offers two classes to build apps:\n\n1\\. **Interface**, that provides a high-level abstraction for creating demos that we've been discussing so far.\n\n2\\. **Blocks**, a low-level API for designing web apps with more flexible layouts and data flows. Blocks allows you to do things like feature multiple data flows and demos, control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. If this customizability is what you need, try `Blocks` instead!\n\n## Hello, Blocks\n\nLet's take a look at a simple example. Note how the API here differs from `Interface`.\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n    greet_btn.click(fn=greet, inputs=name, outputs=output)\n\ndemo.launch()\n```\n<gradio-app space='gradio/hello_blocks'></gradio-app>\n\nThings to note:\n\n- `Blocks` are made with a `with` clause, and any component created inside this clause is automatically added to the app.\n- Components appear vertically in the app in the order they are created. (Later we will cover customizing layouts!)\n- A `Button` was created, and then a `click` event-listener was added to this button. The API for this should look familiar! Like an `Interface`, the `click` method takes a Python function, input components, and output components.\n\n## More Complexity\n\nHere's an app to give you a taste of what's possible with `Blocks`:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n    \ndemo.launch()\n```\n<gradio-app space='gradio/blocks_flipper'></gradio-app>\n\nA lot more going on here! We'll cover how to create complex `Blocks` apps like this in the [building with blocks](https://gradio.app/building_with_blocks) section for you.\n\nCongrats, you're now familiar with the basics of Gradio! \ud83e\udd73 Go to our [next guide](https://gradio.app/key_features) to learn more about the key features of Gradio.\n", "html": "<h1 id=\"quickstart\">Quickstart</h1>\n\n<p><strong>Prerequisite</strong>: Gradio requires Python 3.7 or higher, that's all!</p>\n\n<h2 id=\"what-does-gradio-do\">What Does Gradio Do?</h2>\n\n<p>One of the <em>best ways to share</em> your machine learning model, API, or data science workflow with others is to create an <strong>interactive app</strong> that allows your users or colleagues to try out the demo in their browsers.</p>\n\n<p>Gradio allows you to <strong>build demos and share them, all in Python.</strong> And usually in just a few lines of code! So let's get started.</p>\n\n<h2 id=\"hello-world\">Hello, World</h2>\n\n<p>To get Gradio running with a simple \"Hello, World\" example, follow these three steps:</p>\n\n<p>1. Install Gradio using pip:</p>\n\n<div class='codeblock'><pre><code class='lang-bash'>pip install gradio\n</code></pre></div>\n\n<p>2. Run the code below as a Python script or in a Jupyter Notebook (or <a rel=\"noopener\" target=\"_blank\" href=\"https://colab.research.google.com/drive/18ODkJvyxHutTN0P5APWyGFO_xwNcgHDZ?usp=sharing\">Google Colab</a>):</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n\ndemo.launch()   \n</code></pre></div>\n\n<p>3. The demo below will appear automatically within the Jupyter Notebook, or pop in a browser on <a rel=\"noopener\" target=\"_blank\" href=\"http://localhost:7860\">http://localhost:7860</a> if running from a script:</p>\n\n<p><gradio-app space='gradio/hello_world'></gradio-app></p>\n\n<p>When developing locally, if you want to run the code as a Python script, you can use the Gradio CLI to launch the application <strong>in reload mode</strong>, which will provide seamless and fast development. Learn more about reloading in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/developing-faster-with-reload-mode/\">Auto-Reloading Guide</a>.</p>\n\n<div class='codeblock'><pre><code class='lang-bash'>gradio app.py\n</code></pre></div>\n\n<p>Note: you can also do <code>python app.py</code>, but it won't provide the automatic reload mechanism.</p>\n\n<h2 id=\"the-interface-class\">The <code>Interface</code> Class</h2>\n\n<p>You'll notice that in order to make the demo, we created a <code>gradio.Interface</code>. This <code>Interface</code> class can wrap any Python function with a user interface. In the example above, we saw a simple text-based function, but the function could be anything from music generator to a tax calculator to the prediction function of a pretrained machine learning model.</p>\n\n<p>The core <code>Interface</code> class is initialized with three required parameters:</p>\n\n<ul>\n<li><code>fn</code>: the function to wrap a UI around</li>\n<li><code>inputs</code>: which component(s) to use for the input (e.g. <code>\"text\"</code>, <code>\"image\"</code> or <code>\"audio\"</code>)</li>\n<li><code>outputs</code>: which component(s) to use for the output (e.g. <code>\"text\"</code>, <code>\"image\"</code> or <code>\"label\"</code>)</li>\n</ul>\n\n<p>Let's take a closer look at these components used to provide input and output.</p>\n\n<h2 id=\"components-attributes\">Components Attributes</h2>\n\n<p>We saw some simple <code>Textbox</code> components in the previous examples, but what if you want to change how the UI components look or behave?</p>\n\n<p>Let's say you want to customize the input text field \u2014 for example, you wanted it to be larger and have a text placeholder. If we use the actual class for <code>Textbox</code> instead of using the string shortcut, you have access to much more customizability through component attributes.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=gr.Textbox(lines=2, placeholder=\"Name Here...\"),\n    outputs=\"text\",\n)\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/hello<em>world</em>2'></gradio-app></p>\n\n<h2 id=\"multiple-input-and-output-components\">Multiple Input and Output Components</h2>\n\n<p>Suppose you had a more complex function, with multiple inputs and outputs. In the example below, we define a function that takes a string, boolean, and number, and returns a string and number. Take a look how you pass a list of input and output components.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef greet(name, is_morning, temperature):\n    salutation = \"Good morning\" if is_morning else \"Good evening\"\n    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n    celsius = (temperature - 32) * 5 / 9\n    return greeting, round(celsius, 2)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n    outputs=[\"text\", \"number\"],\n)\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/hello<em>world</em>3'></gradio-app></p>\n\n<p>You simply wrap the components in a list. Each component in the <code>inputs</code> list corresponds to one of the parameters of the function, in order. Each component in the <code>outputs</code> list corresponds to one of the values returned by the function, again in order.</p>\n\n<h2 id=\"an-image-example\">An Image Example</h2>\n\n<p>Gradio supports many types of components, such as <code>Image</code>, <code>DataFrame</code>, <code>Video</code>, or <code>Label</code>. Let's try an image-to-image function to get a feel for these!</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import numpy as np\nimport gradio as gr\n\ndef sepia(input_img):\n    sepia_filter = np.array([\n        [0.393, 0.769, 0.189], \n        [0.349, 0.686, 0.168], \n        [0.272, 0.534, 0.131]\n    ])\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ndemo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \"image\")\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/sepia_filter'></gradio-app></p>\n\n<p>When using the <code>Image</code> component as input, your function will receive a NumPy array with the shape <code>(width, height, 3)</code>, where the last dimension represents the RGB values. We'll return an image as well in the form of a NumPy array.</p>\n\n<p>You can also set the datatype used by the component with the <code>type=</code> keyword argument. For example, if you wanted your function to take a file path to an image instead of a NumPy array, the input <code>Image</code> component could be written as:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Image(type=\"filepath\", shape=...)\n</code></pre></div>\n\n<p>Also note that our input <code>Image</code> component comes with an edit button \ud83d\udd89, which allows for cropping and zooming into images. Manipulating images in this way can help reveal biases or hidden flaws in a machine learning model!</p>\n\n<p>You can read more about the many components and how to use them in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs\">Gradio docs</a>.</p>\n\n<h2 id=\"blocks-more-flexibility-and-control\">Blocks: More Flexibility and Control</h2>\n\n<p>Gradio offers two classes to build apps:</p>\n\n<p>1. <strong>Interface</strong>, that provides a high-level abstraction for creating demos that we've been discussing so far.</p>\n\n<p>2. <strong>Blocks</strong>, a low-level API for designing web apps with more flexible layouts and data flows. Blocks allows you to do things like feature multiple data flows and demos, control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. If this customizability is what you need, try <code>Blocks</code> instead!</p>\n\n<h2 id=\"hello-blocks\">Hello, Blocks</h2>\n\n<p>Let's take a look at a simple example. Note how the API here differs from <code>Interface</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n    greet_btn.click(fn=greet, inputs=name, outputs=output)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/hello_blocks'></gradio-app></p>\n\n<p>Things to note:</p>\n\n<ul>\n<li><code>Blocks</code> are made with a <code>with</code> clause, and any component created inside this clause is automatically added to the app.</li>\n<li>Components appear vertically in the app in the order they are created. (Later we will cover customizing layouts!)</li>\n<li>A <code>Button</code> was created, and then a <code>click</code> event-listener was added to this button. The API for this should look familiar! Like an <code>Interface</code>, the <code>click</code> method takes a Python function, input components, and output components.</li>\n</ul>\n\n<h2 id=\"more-complexity\">More Complexity</h2>\n\n<p>Here's an app to give you a taste of what's possible with <code>Blocks</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\"):\n        gr.Markdown(\"Look at me...\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space='gradio/blocks_flipper'></gradio-app></p>\n\n<p>A lot more going on here! We'll cover how to create complex <code>Blocks</code> apps like this in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/building_with_blocks\">building with blocks</a> section for you.</p>\n\n<p>Congrats, you're now familiar with the basics of Gradio! \ud83e\udd73 Go to our <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/key_features\">next guide</a> to learn more about the key features of Gradio.</p>\n", "tags": [], "spaces": [], "url": "/guides/quickstart/", "contributor": null}, {"name": "key-features", "category": "getting-started", "pretty_category": "Getting Started", "guide_index": 2, "absolute_index": 1, "pretty_name": "Key Features", "content": "# Key Features\n\nLet's go through some of the most popular features of Gradio! Here are Gradio's key features: \n\n1. [Adding example inputs](#example-inputs)\n2. [Passing custom error messages](#errors)\n3. [Adding descriptive content](#descriptive-content)\n4. [Setting up flagging](#flagging)\n5. [Preprocessing and postprocessing](#preprocessing-and-postprocessing)\n6. [Styling demos](#styling)\n7. [Queuing users](#queuing)\n8. [Iterative outputs](#iterative-outputs)\n9. [Progress bars](#progress-bars)\n10. [Batch functions](#batch-functions)\n\n## Example Inputs\n\nYou can provide example data that a user can easily load into `Interface`. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you can provide a **nested list** to the `examples=`  keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the [Docs](https://gradio.app/docs#components).\n\n```python\nimport gradio as gr\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        if num2 == 0:\n            raise gr.Error(\"Cannot divide by zero!\")\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\", \n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    examples=[\n        [5, \"add\", 3],\n        [4, \"divide\", 2],\n        [-4, \"multiply\", 2.5],\n        [0, \"subtract\", 1.2],\n    ],\n    title=\"Toy Calculator\",\n    description=\"Here's a sample toy calculator. Allows you to calculate things like $2+2=4$\",\n)\ndemo.launch()\n\n```\n<gradio-app space='gradio/calculator'></gradio-app>\n\nYou can load a large dataset into the examples to browse and interact with the dataset through Gradio. The examples will be automatically paginated (you can configure this through the `examples_per_page` argument of `Interface`). \n\nContinue learning about examples in the [More On Examples](https://gradio.app/more-on-examples) guide.\n\n## Errors\n\nYou wish to pass custom error messages to the user. To do so, raise a `gr.Error(\"custom message\")` to display an error message. If you try to divide by zero in the calculator demo above, a popup modal will display the custom error message. Learn more about Error in the [docs](https://gradio.app/docs#errors).\n\n## Descriptive Content\n\nIn the previous example, you may have noticed the `title=` and `description=` keyword arguments in the `Interface` constructor that helps users understand your app.\n\nThere are three arguments in the `Interface` constructor to specify where this content should go:\n\n* `title`: which accepts text and can display it at the very top of interface, and also becomes the page title.\n* `description`: which accepts text, markdown or HTML and places it right under the title.\n* `article`: which also accepts text, markdown or HTML and places it below the interface.\n\n![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\n\nIf you're using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor.\n\nAnother useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`.\n\n```python\ngr.Number(label='Age')\n```\n\n## Flagging\n\nBy default, an `Interface` will have \"Flag\" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the  `flagging_dir=`  argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.\n\nFor example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- calculator.py\n+-- flagged/\n|   +-- logs.csv\n```\n\n*flagged/logs.csv*\n```csv\nnum1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n```\n\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- sepia.py\n+-- flagged/\n|   +-- logs.csv\n|   +-- im/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n*flagged/logs.csv*\n```csv\nim,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.\n\n## Preprocessing and Postprocessing\n\n![](https://github.com/gradio-app/gradio/blob/main/ui/packages/_website/src/assets/img/dataflow.svg?raw=true)\n\nAs you've seen, Gradio includes components that can handle a variety of different data types, such as images, audio, and video. Most components can be used both as inputs or outputs.\n\nWhen a component is used as an input, Gradio automatically handles the *preprocessing* needed to convert the data from a type sent by the user's browser (such as a base64 representation of a webcam snapshot) to a form that can be accepted by your function (such as a `numpy` array). \n\nSimilarly, when a component is used as an output, Gradio automatically handles the *postprocessing* needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (such as a `Gallery` of images in base64 format).\n\nYou can control the *preprocessing* using the parameters when constructing the image component. For example, here if you instantiate the `Image` component with the following parameters, it will convert the image to the `PIL` type and reshape it to be `(100, 100)` no matter the original size that it was submitted as:\n\n```py\nimg = gradio.Image(shape=(100, 100), type=\"pil\")\n```\n\nIn contrast, here we keep the original size of the image, but invert the colors before converting it to a numpy array:\n\n```py\nimg = gradio.Image(invert_colors=True, type=\"numpy\")\n```\n\nPostprocessing is a lot easier! Gradio automatically recognizes the format of the returned data (e.g. is the `Image` a `numpy` array or a `str` filepath?) and postprocesses it into a format that can be displayed by the browser.\n\nTake a look at the [Docs](https://gradio.app/docs) to see all the preprocessing-related parameters for each Component.\n\n\n## Styling\n\nMany components can be styled through the `style()` method. For example:\n\n```python\nimg = gr.Image(\"lion.jpg\").style(height='24', rounded=False)\n```\n\nTake a look at the [Docs](https://gradio.app/docs) to see all the styling options for each Component.\n\nFor additional styling ability, you can pass any CSS to your app using the `css=` kwarg.\nThe base class for the Gradio app is `gradio-container`, so here's an example that changes the background color of the Gradio app:\n\n```python\nwith gr.Interface(css=\".gradio-container {background-color: red}\") as demo:\n    ...\n```\n\n## Queuing\n\nIf your app expects heavy traffic, use the `queue()` method to control processing rate. This will queue up calls so only a certain number of requests are processed at a single time. Queueing uses websockets, which also prevent network timeouts, so you should use queueing if the inference time of your function is long (> 1min). \n\nWith `Interface`:\n```python\ndemo = gr.Interface(...).queue()\ndemo.launch()\n```\n\nWith `Blocks`:\n```python\nwith gr.Blocks() as demo:\n    #...\ndemo.queue()\ndemo.launch()\n```\n\nYou can control the number of requests processsed at a single time as such:\n\n```python\ndemo.queue(concurrency_count=3)\n```\n\nSee the [Docs on queueing](/docs/#queue) on configuring other queuing parameters.\n\nTo specify only certain functions for queueing in Blocks:\n```python\nwith gr.Blocks() as demo2:\n    num1 = gr.Number()\n    num2 = gr.Number()\n    output = gr.Number()\n    gr.Button(\"Add\").click(\n        lambda a, b: a + b, [num1, num2], output)\n    gr.Button(\"Multiply\").click(\n        lambda a, b: a * b, [num1, num2], output, queue=True)\ndemo2.launch()\n```\n\n## Iterative Outputs\n\nIn some cases, you may want to show a sequence of outputs rather than a single output. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image.\n\nIn such cases, you can supply a **generator** function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single `return` value, a function should `yield` a series of values instead. Usually the `yield` statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:\n\n```python\ndef my_generator(x):\n    for i in range(x):\n        yield i\n```\n\nYou supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image:\n\n```python\nimport gradio as gr\nimport numpy as np\nimport time\n\n# define core fn, which returns a generator {steps} times before returning the image\ndef fake_diffusion(steps):\n    for _ in range(steps):\n        time.sleep(1)\n        image = np.random.random((600, 600, 3))\n        yield image\n    image = \"https://gradio-builds.s3.amazonaws.com/diffusion_image/cute_dog.jpg\"\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion, inputs=gr.Slider(1, 10, 3), outputs=\"image\")\n\n# define queue - required for generators\ndemo.queue()\n\ndemo.launch()\n\n```\n<gradio-app space='gradio/fake_diffusion'></gradio-app>\n\nNote that we've added a `time.sleep(1)` in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).\n\nSupplying a generator into Gradio **requires** you to enable queuing in the underlying Interface or Blocks (see the queuing section above).\n\n## Progress Bars\n\nGradio supports the ability to create a custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a `gradio.Progress` instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the `tqdm()` method of the `Progress` instance to track progress over an iterable, as shown below. Queueing must be enabled for progress updates.\n\n```python\nimport gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\nif __name__ == \"__main__\":\n    demo.queue(concurrency_count=10).launch()\n\n```\n<gradio-app space='gradio/progress_simple'></gradio-app>\n\nIf you use the `tqdm` library, you can even report progress updates automatically from any `tqdm.tqdm` that already exists within your function by setting the default argument as  `gr.Progress(track_tqdm=True)`!\n\n## Batch Functions\n\nGradio supports the ability to pass *batch* functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.\n\nFor example, here is a batched function that takes in two lists of inputs (a list of \nwords and a list of ints), and returns a list of trimmed words as output:\n\n```py\nimport time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])        \n    return [trimmed_words]\n```\n\nThe advantage of using batched functions is that if you enable queuing, the Gradio\nserver can automatically *batch* incoming requests and process them in parallel, \npotentially speeding up your demo. Here's what the Gradio code looks like (notice\nthe `batch=True` and `max_batch_size=16` -- both of these parameters can be passed\ninto event triggers or into the `Interface` class) \n\nWith `Interface`:\n```python\ndemo = gr.Interface(trim_words, [\"textbox\", \"number\"], [\"output\"], \n                    batch=True, max_batch_size=16)\ndemo.queue()\ndemo.launch()\n```\n\n\nWith `Blocks`:\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.queue()\ndemo.launch()\n```\n\nIn the example above, 16 requests could be processed in parallel (for a total inference\ntime of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face `transformers` and `diffusers` models\nwork very naturally with Gradio's batch mode: here's [an example demo using diffusers to\ngenerate images in batches](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\n\nNote: using batch functions with Gradio **requires** you to enable queuing in the underlying Interface or Blocks (see the queuing section above).\n", "html": "<h1 id=\"key-features\">Key Features</h1>\n\n<p>Let's go through some of the most popular features of Gradio! Here are Gradio's key features: </p>\n\n<ol>\n<li><a href=\"#example-inputs\">Adding example inputs</a></li>\n<li><a href=\"#errors\">Passing custom error messages</a></li>\n<li><a href=\"#descriptive-content\">Adding descriptive content</a></li>\n<li><a href=\"#flagging\">Setting up flagging</a></li>\n<li><a href=\"#preprocessing-and-postprocessing\">Preprocessing and postprocessing</a></li>\n<li><a href=\"#styling\">Styling demos</a></li>\n<li><a href=\"#queuing\">Queuing users</a></li>\n<li><a href=\"#iterative-outputs\">Iterative outputs</a></li>\n<li><a href=\"#progress-bars\">Progress bars</a></li>\n<li><a href=\"#batch-functions\">Batch functions</a></li>\n</ol>\n\n<h2 id=\"example-inputs\">Example Inputs</h2>\n\n<p>You can provide example data that a user can easily load into <code>Interface</code>. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you can provide a <strong>nested list</strong> to the <code>examples=</code>  keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#components\">Docs</a>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        if num2 == 0:\n            raise gr.Error(\"Cannot divide by zero!\")\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\", \n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    examples=[\n        [5, \"add\", 3],\n        [4, \"divide\", 2],\n        [-4, \"multiply\", 2.5],\n        [0, \"subtract\", 1.2],\n    ],\n    title=\"Toy Calculator\",\n    description=\"Here's a sample toy calculator. Allows you to calculate things like $2+2=4$\",\n)\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/calculator'></gradio-app></p>\n\n<p>You can load a large dataset into the examples to browse and interact with the dataset through Gradio. The examples will be automatically paginated (you can configure this through the <code>examples_per_page</code> argument of <code>Interface</code>). </p>\n\n<p>Continue learning about examples in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/more-on-examples\">More On Examples</a> guide.</p>\n\n<h2 id=\"errors\">Errors</h2>\n\n<p>You wish to pass custom error messages to the user. To do so, raise a <code>gr.Error(\"custom message\")</code> to display an error message. If you try to divide by zero in the calculator demo above, a popup modal will display the custom error message. Learn more about Error in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#errors\">docs</a>.</p>\n\n<h2 id=\"descriptive-content\">Descriptive Content</h2>\n\n<p>In the previous example, you may have noticed the <code>title=</code> and <code>description=</code> keyword arguments in the <code>Interface</code> constructor that helps users understand your app.</p>\n\n<p>There are three arguments in the <code>Interface</code> constructor to specify where this content should go:</p>\n\n<ul>\n<li><code>title</code>: which accepts text and can display it at the very top of interface, and also becomes the page title.</li>\n<li><code>description</code>: which accepts text, markdown or HTML and places it right under the title.</li>\n<li><code>article</code>: which also accepts text, markdown or HTML and places it below the interface.</li>\n</ul>\n\n<p><img src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true\" alt=\"annotated\" /></p>\n\n<p>If you're using the <code>Blocks</code> API instead, you can insert text, markdown, or HTML anywhere using the <code>gr.Markdown(...)</code> or <code>gr.HTML(...)</code> components, with descriptive content inside the <code>Component</code> constructor.</p>\n\n<p>Another useful keyword argument is <code>label=</code>, which is present in every <code>Component</code>. This modifies the label text at the top of each <code>Component</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Number(label='Age')\n</code></pre></div>\n\n<h2 id=\"flagging\">Flagging</h2>\n\n<p>By default, an <code>Interface</code> will have \"Flag\" button. When a user testing your <code>Interface</code> sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the  <code>flagging_dir=</code>  argument to the <code>Interface</code> constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.</p>\n\n<p>For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:</p>\n\n<div class='codeblock'><pre><code class='lang-directory'>+-- calculator.py\n+-- flagged/\n|   +-- logs.csv\n</code></pre></div>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>num1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n</code></pre></div>\n\n<p>With the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:</p>\n\n<div class='codeblock'><pre><code class='lang-directory'>+-- sepia.py\n+-- flagged/\n|   +-- logs.csv\n|   +-- im/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n</code></pre></div>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>im,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n</code></pre></div>\n\n<p>If you wish for the user to provide a reason for flagging, you can pass a list of strings to the <code>flagging_options</code> argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.</p>\n\n<h2 id=\"preprocessing-and-postprocessing\">Preprocessing and Postprocessing</h2>\n\n<p><img src=\"https://github.com/gradio-app/gradio/blob/main/ui/packages/_website/src/assets/img/dataflow.svg?raw=true\" alt=\"\" /></p>\n\n<p>As you've seen, Gradio includes components that can handle a variety of different data types, such as images, audio, and video. Most components can be used both as inputs or outputs.</p>\n\n<p>When a component is used as an input, Gradio automatically handles the <em>preprocessing</em> needed to convert the data from a type sent by the user's browser (such as a base64 representation of a webcam snapshot) to a form that can be accepted by your function (such as a <code>numpy</code> array). </p>\n\n<p>Similarly, when a component is used as an output, Gradio automatically handles the <em>postprocessing</em> needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (such as a <code>Gallery</code> of images in base64 format).</p>\n\n<p>You can control the <em>preprocessing</em> using the parameters when constructing the image component. For example, here if you instantiate the <code>Image</code> component with the following parameters, it will convert the image to the <code>PIL</code> type and reshape it to be <code>(100, 100)</code> no matter the original size that it was submitted as:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>img = gradio.Image(shape=(100, 100), type=\"pil\")\n</code></pre></div>\n\n<p>In contrast, here we keep the original size of the image, but invert the colors before converting it to a numpy array:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>img = gradio.Image(invert_colors=True, type=\"numpy\")\n</code></pre></div>\n\n<p>Postprocessing is a lot easier! Gradio automatically recognizes the format of the returned data (e.g. is the <code>Image</code> a <code>numpy</code> array or a <code>str</code> filepath?) and postprocesses it into a format that can be displayed by the browser.</p>\n\n<p>Take a look at the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs\">Docs</a> to see all the preprocessing-related parameters for each Component.</p>\n\n<h2 id=\"styling\">Styling</h2>\n\n<p>Many components can be styled through the <code>style()</code> method. For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>img = gr.Image(\"lion.jpg\").style(height='24', rounded=False)\n</code></pre></div>\n\n<p>Take a look at the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs\">Docs</a> to see all the styling options for each Component.</p>\n\n<p>For additional styling ability, you can pass any CSS to your app using the <code>css=</code> kwarg.\nThe base class for the Gradio app is <code>gradio-container</code>, so here's an example that changes the background color of the Gradio app:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Interface(css=\".gradio-container {background-color: red}\") as demo:\n    ...\n</code></pre></div>\n\n<h2 id=\"queuing\">Queuing</h2>\n\n<p>If your app expects heavy traffic, use the <code>queue()</code> method to control processing rate. This will queue up calls so only a certain number of requests are processed at a single time. Queueing uses websockets, which also prevent network timeouts, so you should use queueing if the inference time of your function is long (&gt; 1min). </p>\n\n<p>With <code>Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo = gr.Interface(...).queue()\ndemo.launch()\n</code></pre></div>\n\n<p>With <code>Blocks</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    #...\ndemo.queue()\ndemo.launch()\n</code></pre></div>\n\n<p>You can control the number of requests processsed at a single time as such:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo.queue(concurrency_count=3)\n</code></pre></div>\n\n<p>See the <a rel=\"noopener\" target=\"_blank\" href=\"/docs/#queue\">Docs on queueing</a> on configuring other queuing parameters.</p>\n\n<p>To specify only certain functions for queueing in Blocks:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo2:\n    num1 = gr.Number()\n    num2 = gr.Number()\n    output = gr.Number()\n    gr.Button(\"Add\").click(\n        lambda a, b: a + b, [num1, num2], output)\n    gr.Button(\"Multiply\").click(\n        lambda a, b: a * b, [num1, num2], output, queue=True)\ndemo2.launch()\n</code></pre></div>\n\n<h2 id=\"iterative-outputs\">Iterative Outputs</h2>\n\n<p>In some cases, you may want to show a sequence of outputs rather than a single output. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image.</p>\n\n<p>In such cases, you can supply a <strong>generator</strong> function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single <code>return</code> value, a function should <code>yield</code> a series of values instead. Usually the <code>yield</code> statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def my_generator(x):\n    for i in range(x):\n        yield i\n</code></pre></div>\n\n<p>You supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nimport numpy as np\nimport time\n\n# define core fn, which returns a generator {steps} times before returning the image\ndef fake_diffusion(steps):\n    for _ in range(steps):\n        time.sleep(1)\n        image = np.random.random((600, 600, 3))\n        yield image\n    image = \"https://gradio-builds.s3.amazonaws.com/diffusion_image/cute_dog.jpg\"\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion, inputs=gr.Slider(1, 10, 3), outputs=\"image\")\n\n# define queue - required for generators\ndemo.queue()\n\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/fake_diffusion'></gradio-app></p>\n\n<p>Note that we've added a <code>time.sleep(1)</code> in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).</p>\n\n<p>Supplying a generator into Gradio <strong>requires</strong> you to enable queuing in the underlying Interface or Blocks (see the queuing section above).</p>\n\n<h2 id=\"progress-bars\">Progress Bars</h2>\n\n<p>Gradio supports the ability to create a custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a <code>gradio.Progress</code> instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the <code>tqdm()</code> method of the <code>Progress</code> instance to track progress over an iterable, as shown below. Queueing must be enabled for progress updates.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\nif __name__ == \"__main__\":\n    demo.queue(concurrency_count=10).launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/progress_simple'></gradio-app></p>\n\n<p>If you use the <code>tqdm</code> library, you can even report progress updates automatically from any <code>tqdm.tqdm</code> that already exists within your function by setting the default argument as  <code>gr.Progress(track_tqdm=True)</code>!</p>\n\n<h2 id=\"batch-functions\">Batch Functions</h2>\n\n<p>Gradio supports the ability to pass <em>batch</em> functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.</p>\n\n<p>For example, here is a batched function that takes in two lists of inputs (a list of \nwords and a list of ints), and returns a list of trimmed words as output:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])        \n    return [trimmed_words]\n</code></pre></div>\n\n<p>The advantage of using batched functions is that if you enable queuing, the Gradio\nserver can automatically <em>batch</em> incoming requests and process them in parallel, \npotentially speeding up your demo. Here's what the Gradio code looks like (notice\nthe <code>batch=True</code> and <code>max_batch_size=16</code> -- both of these parameters can be passed\ninto event triggers or into the <code>Interface</code> class) </p>\n\n<p>With <code>Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo = gr.Interface(trim_words, [\"textbox\", \"number\"], [\"output\"], \n                    batch=True, max_batch_size=16)\ndemo.queue()\ndemo.launch()\n</code></pre></div>\n\n<p>With <code>Blocks</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.queue()\ndemo.launch()\n</code></pre></div>\n\n<p>In the example above, 16 requests could be processed in parallel (for a total inference\ntime of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face <code>transformers</code> and <code>diffusers</code> models\nwork very naturally with Gradio's batch mode: here's <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py\">an example demo using diffusers to\ngenerate images in batches</a></p>\n\n<p>Note: using batch functions with Gradio <strong>requires</strong> you to enable queuing in the underlying Interface or Blocks (see the queuing section above).</p>\n", "tags": [], "spaces": [], "url": "/guides/key-features/", "contributor": null}, {"name": "sharing-your-app", "category": "getting-started", "pretty_category": "Getting Started", "guide_index": 3, "absolute_index": 2, "pretty_name": "Sharing Your App", "content": "# Sharing Your App\n\nHow to share your Gradio app: \n\n1. [Sharing demos with the share parameter](#sharing-demos)\n2. [Hosting on HF Spaces](#hosting-on-hf-spaces)\n3. [Embedding hosted spaces](#embedding-hosted-spaces)\n4. [Embedding with web components](#embedding-with-web-components)\n5. [Using the API page](#api-page)\n6. [Adding authentication to the page](#authentication)\n7. [Accessing Network Requests](#accessing-the-network-request-directly)\n8. [Mounting within FastAPI](#mounting-within-another-fastapi-app)\n\n## Sharing Demos\n\nGradio demos can be easily shared publicly by setting `share=True` in the `launch()` method. Like this:\n\n```python\ndemo.launch(share=True)\n```\n\nThis generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser. Because the processing happens on your device (as long as your device stays on!), you don't have to worry about any packaging any dependencies. A share link usually looks something like this:  **XXXXX.gradio.app**. Although the link is served through a Gradio URL, we are only a proxy for your local server, and do not store any data sent through your app.\n\nKeep in mind, however, that these links are publicly accessible, meaning that anyone can use your model for prediction! Therefore, make sure not to expose any sensitive information through the functions you write, or allow any critical changes to occur on your device. If you set `share=False` (the default, except in colab notebooks), only a local link is created, which can be shared by  [port-forwarding](https://www.ssh.com/ssh/tunneling/example)  with specific users. \n\n<img style=\"width: 40%\" src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/sharing.svg?raw=true\">\n\nShare links expire after 72 hours.\n\n## Hosting on HF Spaces\n\nIf you'd like to have a permanent link to your Gradio demo on the internet, use Hugging Face Spaces. [Hugging Face Spaces](http://huggingface.co/spaces/) provides the infrastructure to permanently host your machine learning model for free! \n\nYou can either drag and drop a folder containing your Gradio model and all related files, or you can point Spaces to your Git repository and Spaces will pull the Gradio app from there. See [this guide how to host on Hugging Face Spaces](https://huggingface.co/blog/gradio-spaces) for more information. \n\n<video autoplay muted loop>\n  <source src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/hf_demo.mp4?raw=true\" type=\"video/mp4\" />\n</video>\n\n## Embedding Hosted Spaces\n\nOnce you have hosted your app on Hugging Face Spaces, you may want to embed the demo on a different website, such as your blog or your portfolio. Embedding an interactive demo allows people to try out the machine learning model that you have built, without needing to download or install anything \u2014 right in their browser! The best part is that you can embed interative demos even in static websites, such as GitHub pages.\n\nThere are two ways to embed your Gradio demos, hosted on Hugging Face Spaces. You can find quick links to both options directly on the Space page, in the \"Embed this Space\" dropdown option:\n\n![Embed this Space dropdown option](https://github.com/gradio-app/gradio/blob/main/guides/assets/embed_this_space.png?raw=true)\n\n### Embedding with Web Components\n\nUsing web components is faster then iframes, and will automatically adjust to other content on your site. To embed with Web Components:\n\n1. Import the gradio JS library into into your site by adding the script below in your site (replace {GRADIO_VERSION} in the URL with the library version of Gradio you are using). \n\n```html\n<script type=\"module\"\nsrc=\"https://gradio.s3-us-west-2.amazonaws.com/{GRADIO_VERSION}/gradio.js\">\n</script>\n```\n\n2. Add \n```html\n<gradio-app src=\"https://$your_space_host.hf.space\"></gradio-app>\n```\n\nelement where you want to place the app. Set the `src=` attribute to your Space's embed URL, which you can find in the Embed this Space button. For example:\n\n```html\n<gradio-app src=\"https://abidlabs-pytorch-image-classifier.hf.space\"></gradio-app>\n```\n\n<script>\nfetch(\"https://pypi.org/pypi/gradio/json\"\n).then(r => r.json()\n).then(obj => {\n    let v = obj.info.version;\n    content = document.querySelector('.prose');\n    content.innerHTML = content.innerHTML.replaceAll(\"{GRADIO_VERSION}\", v);\n});\n</script>\n\n_Note: While Gradio's CSS will never impact the embedding page, the embedding page can affect the style of the embedded Gradio app. Make sure that any CSS in the parent page isn't so general that it could also apply to the embedded Gradio app and cause the styling to break. Element selectors such as `header { ... }` and `footer { ... }` will be the most likely to cause issues._\n\n### Embedding with IFrames\n\nTo embed with IFrames instead, simply add this element:\n\n```html\n<iframe src=\"https://$your_space_host.hf.space\"></iframe>\n```\n\nFor example: \n\n```html\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\"></iframe>\n```\n\n## API Page\n\n<gradio-app space='gradio/hello_world'></gradio-app>\n\nSee the \"view api\" link in footer of the app above? This is a page that documents the REST API that users can use to query the `Interface` function. `Blocks` apps can also generate an API page, though the API has to be explicitly named for each event listener, such as\n\n```python\nbtn.click(add, [num1, num2], output, api_name=\"addition\")\n```\n\nThis will document the endpoint `/api/addition/` to the automatically generated API page. \n\n## Authentication\n\nYou may wish to put an authentication page in front of your app to limit who can open your app. With the `auth=` keyword argument in the `launch()` method, you can provide a tuple with a username and password, or a  list of acceptable username/password tuples;  Here's an example that provides password-based authentication for a single user named \"admin\":\n\n```python\ndemo.launch(auth=(\"admin\", \"pass1234\"))\n```\n\nFor more complex authentication handling, you can even pass a function that takes a username and password as arguments, and returns True to allow authentication, False otherwise. This can be used for, among other things, making requests to 3rd-party authentication services.\n\nHere's an example of a function that accepts any login where the username and password are the same:\n\n```python\ndef same_auth(username, password):\n    return username == password\ndemo.launch(auth=same_auth)\n```\n\nFor authentication to work properly, third party cookies must be enabled in your browser.\nThis is not the case by default for Safari, Chrome Incognito Mode.\n\n## Accessing the Network Request Directly\n\nWhen a user makes a prediction to your app, you may need the underlying network request, in order to get the request headers (e.g. for advanced authentication), log the client's IP address, or for other reasons. Gradio supports this in a similar manner to FastAPI: simply add a function parameter whose type hint is `gr.Request` and Gradio will pass in the network request as that parameter. Here is an example:\n\n```python\nimport gradio as gr\n\ndef echo(name, request: gr.Request):\n    if request:\n        print(\"Request headers dictionary:\", request.headers)\n        print(\"IP address:\", request.client.host)\n    return name\n\nio = gr.Interface(echo, \"textbox\", \"textbox\").launch()\n```\n\nNote: if your function is called directly instead of through the UI (this happens, for \nexample, when examples are cached), then `request` will be `None`. You should handle\nthis case explicitly to ensure that your app does not throw any errors. That is why\nwe have the explicit check `if request`.\n\n## Mounting Within Another FastAPI App\n\nIn some cases, you might have an existing FastAPI app, and you'd like to add a path for a Gradio demo.\nYou can easily do this with `gradio.mount_gradio_app()`.\n\nHere's a complete example:\n\n```python\nfrom fastapi import FastAPI\nimport gradio as gr\n\nCUSTOM_PATH = \"/gradio\"\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_main():\n    return {\"message\": \"This is your main app\"}\n\n\nio = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")\napp = gr.mount_gradio_app(app, io, path=CUSTOM_PATH)\n\n\n# Run this from the terminal as you would normally start a FastAPI app: `uvicorn run:app`\n# and navigate to http://localhost:8000/gradio in your browser.\n\n```\n\nNote that this approach also allows you run your Gradio apps on custom paths (`http://localhost:8000/gradio` in the example above).\n", "html": "<h1 id=\"sharing-your-app\">Sharing Your App</h1>\n\n<p>How to share your Gradio app: </p>\n\n<ol>\n<li><a href=\"#sharing-demos\">Sharing demos with the share parameter</a></li>\n<li><a href=\"#hosting-on-hf-spaces\">Hosting on HF Spaces</a></li>\n<li><a href=\"#embedding-hosted-spaces\">Embedding hosted spaces</a></li>\n<li><a href=\"#embedding-with-web-components\">Embedding with web components</a></li>\n<li><a href=\"#api-page\">Using the API page</a></li>\n<li><a href=\"#authentication\">Adding authentication to the page</a></li>\n<li><a href=\"#accessing-the-network-request-directly\">Accessing Network Requests</a></li>\n<li><a href=\"#mounting-within-another-fastapi-app\">Mounting within FastAPI</a></li>\n</ol>\n\n<h2 id=\"sharing-demos\">Sharing Demos</h2>\n\n<p>Gradio demos can be easily shared publicly by setting <code>share=True</code> in the <code>launch()</code> method. Like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo.launch(share=True)\n</code></pre></div>\n\n<p>This generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser. Because the processing happens on your device (as long as your device stays on!), you don't have to worry about any packaging any dependencies. A share link usually looks something like this:  <strong>XXXXX.gradio.app</strong>. Although the link is served through a Gradio URL, we are only a proxy for your local server, and do not store any data sent through your app.</p>\n\n<p>Keep in mind, however, that these links are publicly accessible, meaning that anyone can use your model for prediction! Therefore, make sure not to expose any sensitive information through the functions you write, or allow any critical changes to occur on your device. If you set <code>share=False</code> (the default, except in colab notebooks), only a local link is created, which can be shared by  <a rel=\"noopener\" target=\"_blank\" href=\"https://www.ssh.com/ssh/tunneling/example\">port-forwarding</a>  with specific users. </p>\n\n<p><img style=\"width: 40%\" src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/sharing.svg?raw=true\"></p>\n\n<p>Share links expire after 72 hours.</p>\n\n<h2 id=\"hosting-on-hf-spaces\">Hosting on HF Spaces</h2>\n\n<p>If you'd like to have a permanent link to your Gradio demo on the internet, use Hugging Face Spaces. <a rel=\"noopener\" target=\"_blank\" href=\"http://huggingface.co/spaces/\">Hugging Face Spaces</a> provides the infrastructure to permanently host your machine learning model for free! </p>\n\n<p>You can either drag and drop a folder containing your Gradio model and all related files, or you can point Spaces to your Git repository and Spaces will pull the Gradio app from there. See <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/blog/gradio-spaces\">this guide how to host on Hugging Face Spaces</a> for more information. </p>\n\n<p><video autoplay muted loop>\n  <source src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/hf_demo.mp4?raw=true\" type=\"video/mp4\" />\n</video></p>\n\n<h2 id=\"embedding-hosted-spaces\">Embedding Hosted Spaces</h2>\n\n<p>Once you have hosted your app on Hugging Face Spaces, you may want to embed the demo on a different website, such as your blog or your portfolio. Embedding an interactive demo allows people to try out the machine learning model that you have built, without needing to download or install anything \u2014 right in their browser! The best part is that you can embed interative demos even in static websites, such as GitHub pages.</p>\n\n<p>There are two ways to embed your Gradio demos, hosted on Hugging Face Spaces. You can find quick links to both options directly on the Space page, in the \"Embed this Space\" dropdown option:</p>\n\n<p><img src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/embed_this_space.png?raw=true\" alt=\"Embed this Space dropdown option\" /></p>\n\n<h3 id=\"embedding-with-web-components\">Embedding with Web Components</h3>\n\n<p>Using web components is faster then iframes, and will automatically adjust to other content on your site. To embed with Web Components:</p>\n\n<ol>\n<li>Import the gradio JS library into into your site by adding the script below in your site (replace {GRADIO_VERSION} in the URL with the library version of Gradio you are using). </li>\n</ol>\n\n<div class='codeblock'><pre><code class='lang-html'><script type=\"module\"\nsrc=\"https://gradio.s3-us-west-2.amazonaws.com/{GRADIO_VERSION}/gradio.js\">\n</script>\n</code></pre></div>\n\n<ol>\n<li>Add </li>\n</ol>\n\n<div class='codeblock'><pre><code class='lang-html'><gradio-app src=\"https://$your_space_host.hf.space\"></gradio-app>\n</code></pre></div>\n\n<p>element where you want to place the app. Set the <code>src=</code> attribute to your Space's embed URL, which you can find in the Embed this Space button. For example:</p>\n\n<div class='codeblock'><pre><code class='lang-html'><gradio-app src=\"https://abidlabs-pytorch-image-classifier.hf.space\"></gradio-app>\n</code></pre></div>\n\n<script>\nfetch(\"https://pypi.org/pypi/gradio/json\"\n).then(r => r.json()\n).then(obj => {\n    let v = obj.info.version;\n    content = document.querySelector('.prose');\n    content.innerHTML = content.innerHTML.replaceAll(\"{GRADIO_VERSION}\", v);\n});\n</script>\n\n<p><em>Note: While Gradio's CSS will never impact the embedding page, the embedding page can affect the style of the embedded Gradio app. Make sure that any CSS in the parent page isn't so general that it could also apply to the embedded Gradio app and cause the styling to break. Element selectors such as <code>header { ... }</code> and <code>footer { ... }</code> will be the most likely to cause issues.</em></p>\n\n<h3 id=\"embedding-with-iframes\">Embedding with IFrames</h3>\n\n<p>To embed with IFrames instead, simply add this element:</p>\n\n<div class='codeblock'><pre><code class='lang-html'><iframe src=\"https://$your_space_host.hf.space\"></iframe>\n</code></pre></div>\n\n<p>For example: </p>\n\n<div class='codeblock'><pre><code class='lang-html'><iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\"></iframe>\n</code></pre></div>\n\n<h2 id=\"api-page\">API Page</h2>\n\n<p><gradio-app space='gradio/hello_world'></gradio-app></p>\n\n<p>See the \"view api\" link in footer of the app above? This is a page that documents the REST API that users can use to query the <code>Interface</code> function. <code>Blocks</code> apps can also generate an API page, though the API has to be explicitly named for each event listener, such as</p>\n\n<div class='codeblock'><pre><code class='lang-python'>btn.click(add, [num1, num2], output, api_name=\"addition\")\n</code></pre></div>\n\n<p>This will document the endpoint <code>/api/addition/</code> to the automatically generated API page. </p>\n\n<h2 id=\"authentication\">Authentication</h2>\n\n<p>You may wish to put an authentication page in front of your app to limit who can open your app. With the <code>auth=</code> keyword argument in the <code>launch()</code> method, you can provide a tuple with a username and password, or a  list of acceptable username/password tuples;  Here's an example that provides password-based authentication for a single user named \"admin\":</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo.launch(auth=(\"admin\", \"pass1234\"))\n</code></pre></div>\n\n<p>For more complex authentication handling, you can even pass a function that takes a username and password as arguments, and returns True to allow authentication, False otherwise. This can be used for, among other things, making requests to 3rd-party authentication services.</p>\n\n<p>Here's an example of a function that accepts any login where the username and password are the same:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def same_auth(username, password):\n    return username == password\ndemo.launch(auth=same_auth)\n</code></pre></div>\n\n<p>For authentication to work properly, third party cookies must be enabled in your browser.\nThis is not the case by default for Safari, Chrome Incognito Mode.</p>\n\n<h2 id=\"accessing-the-network-request-directly\">Accessing the Network Request Directly</h2>\n\n<p>When a user makes a prediction to your app, you may need the underlying network request, in order to get the request headers (e.g. for advanced authentication), log the client's IP address, or for other reasons. Gradio supports this in a similar manner to FastAPI: simply add a function parameter whose type hint is <code>gr.Request</code> and Gradio will pass in the network request as that parameter. Here is an example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef echo(name, request: gr.Request):\n    if request:\n        print(\"Request headers dictionary:\", request.headers)\n        print(\"IP address:\", request.client.host)\n    return name\n\nio = gr.Interface(echo, \"textbox\", \"textbox\").launch()\n</code></pre></div>\n\n<p>Note: if your function is called directly instead of through the UI (this happens, for \nexample, when examples are cached), then <code>request</code> will be <code>None</code>. You should handle\nthis case explicitly to ensure that your app does not throw any errors. That is why\nwe have the explicit check <code>if request</code>.</p>\n\n<h2 id=\"mounting-within-another-fastapi-app\">Mounting Within Another FastAPI App</h2>\n\n<p>In some cases, you might have an existing FastAPI app, and you'd like to add a path for a Gradio demo.\nYou can easily do this with <code>gradio.mount_gradio_app()</code>.</p>\n\n<p>Here's a complete example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from fastapi import FastAPI\nimport gradio as gr\n\nCUSTOM_PATH = \"/gradio\"\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_main():\n    return {\"message\": \"This is your main app\"}\n\n\nio = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")\napp = gr.mount_gradio_app(app, io, path=CUSTOM_PATH)\n\n\n# Run this from the terminal as you would normally start a FastAPI app: `uvicorn run:app`\n# and navigate to http://localhost:8000/gradio in your browser.\n\n</code></pre></div>\n\n<p>Note that this approach also allows you run your Gradio apps on custom paths (<code>http://localhost:8000/gradio</code> in the example above).</p>\n", "tags": [], "spaces": [], "url": "/guides/sharing-your-app/", "contributor": null}, {"name": "interface-state", "category": "building-interfaces", "pretty_category": "Building Interfaces", "guide_index": 1, "absolute_index": 3, "pretty_name": "Interface State", "content": "# Interface State\n\nThis guide covers how State is handled in Gradio. Learn the difference between Global and Session states, and how to use both.\n\n## Global State\n\nYour function may use data that persists beyond a single function call. If the data is something accessible to all function calls and all users, you can create a variable outside the function call and access it inside the function. For example, you may load a large model outside the function and use it inside the function so that every function call does not need to reload the model. \n\n```python\nimport gradio as gr\n\nscores = []\n\ndef track_score(score):\n    scores.append(score)\n    top_scores = sorted(scores, reverse=True)[:3]\n    return top_scores\n\ndemo = gr.Interface(\n    track_score, \n    gr.Number(label=\"Score\"), \n    gr.JSON(label=\"Top Scores\")\n)\ndemo.launch()\n```\n\nIn the code above, the `scores` array is shared between all users. If multiple users are accessing this demo, their scores will all be added to the same list, and the returned top 3 scores will be collected from this shared reference. \n\n## Session State\n\nAnother type of data persistence Gradio supports is session **state**, where data persists across multiple submits within a page session. However, data is *not* shared between different users of your model. To store data in a session state, you need to do three things:\n\n1. Pass in an extra parameter into your function, which represents the state of the interface.\n2. At the end of the function, return the updated value of the state as an extra return value.\n3. Add the `'state'` input and `'state'` output components when creating your `Interface`\n\nA chatbot is an example where you would need session state - you want access to a users previous submissions, but you cannot store chat history in a global variable, because then chat history would get jumbled between different users. \n\n```python\nimport gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n\ndef predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response \n    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n    return response, history\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    state = gr.State([])\n\n    with gr.Row():\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n\n    txt.submit(predict, [txt, state], [chatbot, state])\n            \ndemo.launch()\n\n```\n<gradio-app space='gradio/chatbot_demo'></gradio-app>\n\nNotice how the state persists across submits within each page, but if you load this demo in another tab (or refresh the page), the demos will not share chat history. \n\nThe default value of `state` is None. If you pass a default value to the state parameter of the function, it is used as the default value of the state instead. The `Interface` class only supports a single input and outputs state variable, though it can be a list with multiple elements. For more complex use cases, you can use Blocks, [which supports multiple `State` variables](/state_in_blocks/).", "html": "<h1 id=\"interface-state\">Interface State</h1>\n\n<p>This guide covers how State is handled in Gradio. Learn the difference between Global and Session states, and how to use both.</p>\n\n<h2 id=\"global-state\">Global State</h2>\n\n<p>Your function may use data that persists beyond a single function call. If the data is something accessible to all function calls and all users, you can create a variable outside the function call and access it inside the function. For example, you may load a large model outside the function and use it inside the function so that every function call does not need to reload the model. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nscores = []\n\ndef track_score(score):\n    scores.append(score)\n    top_scores = sorted(scores, reverse=True)[:3]\n    return top_scores\n\ndemo = gr.Interface(\n    track_score, \n    gr.Number(label=\"Score\"), \n    gr.JSON(label=\"Top Scores\")\n)\ndemo.launch()\n</code></pre></div>\n\n<p>In the code above, the <code>scores</code> array is shared between all users. If multiple users are accessing this demo, their scores will all be added to the same list, and the returned top 3 scores will be collected from this shared reference. </p>\n\n<h2 id=\"session-state\">Session State</h2>\n\n<p>Another type of data persistence Gradio supports is session <strong>state</strong>, where data persists across multiple submits within a page session. However, data is <em>not</em> shared between different users of your model. To store data in a session state, you need to do three things:</p>\n\n<ol>\n<li>Pass in an extra parameter into your function, which represents the state of the interface.</li>\n<li>At the end of the function, return the updated value of the state as an extra return value.</li>\n<li>Add the <code>'state'</code> input and <code>'state'</code> output components when creating your <code>Interface</code></li>\n</ol>\n\n<p>A chatbot is an example where you would need session state - you want access to a users previous submissions, but you cannot store chat history in a global variable, because then chat history would get jumbled between different users. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n\ndef predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response \n    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n    return response, history\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    state = gr.State([])\n\n    with gr.Row():\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n\n    txt.submit(predict, [txt, state], [chatbot, state])\n\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/chatbot_demo'></gradio-app></p>\n\n<p>Notice how the state persists across submits within each page, but if you load this demo in another tab (or refresh the page), the demos will not share chat history. </p>\n\n<p>The default value of <code>state</code> is None. If you pass a default value to the state parameter of the function, it is used as the default value of the state instead. The <code>Interface</code> class only supports a single input and outputs state variable, though it can be a list with multiple elements. For more complex use cases, you can use Blocks, <a rel=\"noopener\" target=\"_blank\" href=\"/state_in_blocks/\">which supports multiple <code>State</code> variables</a>.</p>\n", "tags": [], "spaces": [], "url": "/guides/interface-state/", "contributor": null}, {"name": "reactive-interfaces", "category": "building-interfaces", "pretty_category": "Building Interfaces", "guide_index": 2, "absolute_index": 4, "pretty_name": "Reactive Interfaces", "content": "# Reactive Interfaces\n\nThis guide covers how to get Gradio interfaces to refresh automatically or continuously stream data.\n\n## Live Interfaces\n\nYou can make interfaces automatically refresh by setting `live=True` in the interface. Now the interface will recalculate as soon as the user input changes.\n\n```python\nimport gradio as gr\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\",\n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    live=True,\n)\ndemo.launch()\n\n```\n<gradio-app space='gradio/calculator_live'></gradio-app>\n\nNote there is no submit button, because the interface resubmits automatically on change.\n\n## Streaming Components\n\nSome components have a \"streaming\" mode, such as `Audio` component in microphone mode, or the `Image` component in webcam mode. Streaming means data is sent continuously to the backend and the `Interface` function is continuously being rerun. \n\nThe difference between `gr.Audio(source='microphone')` and `gr.Audio(source='microphone', streaming=True)`, when both are used in `gr.Interface(live=True)`, is that the first  `Component` will automatically submit data and run the `Interface` function when the user stops recording, whereas the second `Component` will continuously send data and run the `Interface` function *during* recording.\n\nHere is example code of streaming images from the webcam.\n\n```python\nimport gradio as gr\nimport numpy as np\n\ndef flip(im):\n    return np.flipud(im)\n\ndemo = gr.Interface(\n    flip, \n    gr.Image(source=\"webcam\", streaming=True), \n    \"image\",\n    live=True\n)\ndemo.launch()\n```", "html": "<h1 id=\"reactive-interfaces\">Reactive Interfaces</h1>\n\n<p>This guide covers how to get Gradio interfaces to refresh automatically or continuously stream data.</p>\n\n<h2 id=\"live-interfaces\">Live Interfaces</h2>\n\n<p>You can make interfaces automatically refresh by setting <code>live=True</code> in the interface. Now the interface will recalculate as soon as the user input changes.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\",\n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    live=True,\n)\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/calculator_live'></gradio-app></p>\n\n<p>Note there is no submit button, because the interface resubmits automatically on change.</p>\n\n<h2 id=\"streaming-components\">Streaming Components</h2>\n\n<p>Some components have a \"streaming\" mode, such as <code>Audio</code> component in microphone mode, or the <code>Image</code> component in webcam mode. Streaming means data is sent continuously to the backend and the <code>Interface</code> function is continuously being rerun. </p>\n\n<p>The difference between <code>gr.Audio(source='microphone')</code> and <code>gr.Audio(source='microphone', streaming=True)</code>, when both are used in <code>gr.Interface(live=True)</code>, is that the first  <code>Component</code> will automatically submit data and run the <code>Interface</code> function when the user stops recording, whereas the second <code>Component</code> will continuously send data and run the <code>Interface</code> function <em>during</em> recording.</p>\n\n<p>Here is example code of streaming images from the webcam.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nimport numpy as np\n\ndef flip(im):\n    return np.flipud(im)\n\ndemo = gr.Interface(\n    flip, \n    gr.Image(source=\"webcam\", streaming=True), \n    \"image\",\n    live=True\n)\ndemo.launch()\n</code></pre></div>\n", "tags": [], "spaces": [], "url": "/guides/reactive-interfaces/", "contributor": null}, {"name": "advanced-interface-features", "category": "building-interfaces", "pretty_category": "Building Interfaces", "guide_index": 4, "absolute_index": 6, "pretty_name": "Advanced Interface Features", "content": "# Advanced Interface Features\n\nThere's more to cover on the [Interface](https://gradio.app/docs#interface) class. This guide covers all the advanced features: Using [Interpretation](https://gradio.app/docs#interpretation), custom styling, loading from the [Hugging Face Hub](https://hf.co), and using [Parallel](https://gradio.app/docs#parallel) and [Series](https://gradio.app/docs#series). \n\n## Interpreting your Predictions\n\nMost models are black boxes such that the internal logic of the function is hidden from the end user. To encourage transparency, we've made it very easy to add interpretation to your model by  simply setting the `interpretation` keyword in the `Interface` class to `default`. This allows your users to understand what parts of the input are responsible for the output. Take a look at the simple interface below which shows an image classifier that also includes interpretation:\n\n```python\nimport requests\nimport tensorflow as tf\n\nimport gradio as gr\n\ninception_net = tf.keras.applications.MobileNetV2()  # load the model\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n\ndef classify_image(inp):\n    inp = inp.reshape((-1, 224, 224, 3))\n    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n    prediction = inception_net.predict(inp).flatten()\n    return {labels[i]: float(prediction[i]) for i in range(1000)}\n\n\nimage = gr.Image(shape=(224, 224))\nlabel = gr.Label(num_top_classes=3)\n\ndemo = gr.Interface(\n    fn=classify_image, inputs=image, outputs=label, interpretation=\"default\"\n)\n\ndemo.launch()\n\n```\n\n\nIn addition to `default`, Gradio also includes [Shapley-based interpretation](https://christophm.github.io/interpretable-ml-book/shap.html), which provides more accurate interpretations, albeit usually with a slower runtime. To use this, simply set the `interpretation` parameter to `\"shap\"` (note: also make sure the python package `shap` is installed). Optionally, you can modify the `num_shap` parameter, which controls the tradeoff between accuracy and runtime (increasing this value generally increases accuracy). Here is an example:\n\n```python\ngr.Interface(fn=classify_image, inputs=image, outputs=label, interpretation=\"shap\", num_shap=5).launch()\n```\n\nThis will work for any function, even if internally, the model is a complex neural network or some other black box. If you use Gradio's `default` or `shap` interpretation, the output component must be a `Label`. All common input components are supported. Here is an example with text input.\n\n```python\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=\"default\",\n)\n\ndemo.launch()\n\n```\n\nSo what is happening under the hood? With these interpretation methods, Gradio runs the prediction multiple times with modified versions of the input. Based on the results, you'll see that the interface automatically highlights the parts of the text (or image, etc.) that contributed increased the likelihood of the class as red. The intensity of color corresponds to the importance of that part of the input. The parts that decrease the class confidence are highlighted blue.\n\nYou can also write your own interpretation function. The demo below adds custom interpretation to the previous demo. This function will take the same inputs as the main wrapped function. The output of this interpretation function will be used to highlight the input of each input component - therefore the function must return a list where the number of elements corresponds to the number of input components. To see the format for interpretation for each input component, check the Docs.\n\n```python\nimport re\n\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\n# Number of arguments to interpretation function must\n# match number of inputs to prediction function\ndef interpret_gender(sentence):\n    result = gender_of_sentence(sentence)\n    is_male = result[\"male\"] > result[\"female\"]\n    interpretation = []\n    for word in re.split(\"( )\", sentence):\n        score = 0\n        token = word.lower()\n        if (is_male and token in male_words) or (not is_male and token in female_words):\n            score = 1\n        elif (is_male and token in female_words) or (\n            not is_male and token in male_words\n        ):\n            score = -1\n        interpretation.append((word, score))\n    # Output must be a list of lists containing the same number of elements as inputs\n    # Each element corresponds to the interpretation scores for the given input\n    return [interpretation]\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=interpret_gender,\n)\n\ndemo.launch()\n\n```\n\nLearn more about Interpretation in the [docs](https://gradio.app/docs#interpretation). \n\n## Custom Styling\n\nIf you'd like to have more fine-grained control over any aspect of your demo, you can also write your own css or pass in a filepath to a css file, with the `css` parameter of the `Interface` class.\n\n```python\ngr.Interface(..., css=\"body {background-color: red}\")\n```\n\nIf you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with `\"file=\"`, for example:\n\n```python\ngr.Interface(..., css=\"body {background-image: url('file=clouds.jpg')}\")\n```\n\n## Loading Hugging Face Models and Spaces\n\nGradio integrates nicely with the [Hugging Face Hub](https://hf.co), allowing you to load models and Spaces with just one line of code. To use this, simply use the `load()` method in the `Interface` class. So:\n\n- To load any model from the Hugging Face Hub and create an interface around it, you pass `\"model/\"` or `\"huggingface/\"` followed by the model name, like these examples:\n\n```python\ngr.Interface.load(\"huggingface/gpt2\").launch();\n```\n\n```python\ngr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\", \n    inputs=gr.Textbox(lines=5, label=\"Input Text\")  # customizes the input component\n).launch()\n```\n\n- To load any Space from the Hugging Face Hub and recreate it locally (so that you can customize the inputs and outputs for example), you pass `\"spaces/\"` followed by the model name:\n\n```python\ngr.Interface.load(\"spaces/eugenesiow/remove-bg\", inputs=\"webcam\", title=\"Remove your webcam background!\").launch()\n```\n\nOne of the great things about loading Hugging Face models or spaces using Gradio is that you can then immediately use the resulting `Interface` object just like function in your Python code (this works for every type of model/space: text, images, audio, video, and even multimodal models):\n\n```python\nio = gr.Interface.load(\"models/EleutherAI/gpt-neo-2.7B\")\nio(\"It was the best of times\")  # outputs model completion\n```\n\n## Putting Interfaces in Parallel and Series\n\nGradio also lets you mix interfaces very easily using the `gradio.Parallel` and `gradio.Series` classes. `Parallel` lets you put two similar models (if they have the same input type) in parallel to compare model predictions:\n\n```python\ngenerator1 = gr.Interface.load(\"huggingface/gpt2\")\ngenerator2 = gr.Interface.load(\"huggingface/EleutherAI/gpt-neo-2.7B\")\ngenerator3 = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\")\n\ngr.Parallel(generator1, generator2, generator3).launch()\n```\n\n`Series` lets you put models and spaces in series, piping the output of one model into the input of the next model. \n\n```python\ngenerator = gr.Interface.load(\"huggingface/gpt2\")\ntranslator = gr.Interface.load(\"huggingface/t5-small\")\n\ngr.Series(generator, translator).launch()  # this demo generates text, then translates it to German, and outputs the final result.\n```\n\nAnd of course, you can also mix `Parallel` and `Series` together whenever that makes sense!\n\nLearn more about Parallel and Series in the [docs](https://gradio.app/docs#parallel). ", "html": "<h1 id=\"advanced-interface-features\">Advanced Interface Features</h1>\n\n<p>There's more to cover on the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#interface\">Interface</a> class. This guide covers all the advanced features: Using <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#interpretation\">Interpretation</a>, custom styling, loading from the <a rel=\"noopener\" target=\"_blank\" href=\"https://hf.co\">Hugging Face Hub</a>, and using <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#parallel\">Parallel</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#series\">Series</a>. </p>\n\n<h2 id=\"interpreting-your-predictions\">Interpreting your Predictions</h2>\n\n<p>Most models are black boxes such that the internal logic of the function is hidden from the end user. To encourage transparency, we've made it very easy to add interpretation to your model by  simply setting the <code>interpretation</code> keyword in the <code>Interface</code> class to <code>default</code>. This allows your users to understand what parts of the input are responsible for the output. Take a look at the simple interface below which shows an image classifier that also includes interpretation:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import requests\nimport tensorflow as tf\n\nimport gradio as gr\n\ninception_net = tf.keras.applications.MobileNetV2()  # load the model\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n\ndef classify_image(inp):\n    inp = inp.reshape((-1, 224, 224, 3))\n    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n    prediction = inception_net.predict(inp).flatten()\n    return {labels[i]: float(prediction[i]) for i in range(1000)}\n\n\nimage = gr.Image(shape=(224, 224))\nlabel = gr.Label(num_top_classes=3)\n\ndemo = gr.Interface(\n    fn=classify_image, inputs=image, outputs=label, interpretation=\"default\"\n)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p>In addition to <code>default</code>, Gradio also includes <a rel=\"noopener\" target=\"_blank\" href=\"https://christophm.github.io/interpretable-ml-book/shap.html\">Shapley-based interpretation</a>, which provides more accurate interpretations, albeit usually with a slower runtime. To use this, simply set the <code>interpretation</code> parameter to <code>\"shap\"</code> (note: also make sure the python package <code>shap</code> is installed). Optionally, you can modify the <code>num_shap</code> parameter, which controls the tradeoff between accuracy and runtime (increasing this value generally increases accuracy). Here is an example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(fn=classify_image, inputs=image, outputs=label, interpretation=\"shap\", num_shap=5).launch()\n</code></pre></div>\n\n<p>This will work for any function, even if internally, the model is a complex neural network or some other black box. If you use Gradio's <code>default</code> or <code>shap</code> interpretation, the output component must be a <code>Label</code>. All common input components are supported. Here is an example with text input.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=\"default\",\n)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p>So what is happening under the hood? With these interpretation methods, Gradio runs the prediction multiple times with modified versions of the input. Based on the results, you'll see that the interface automatically highlights the parts of the text (or image, etc.) that contributed increased the likelihood of the class as red. The intensity of color corresponds to the importance of that part of the input. The parts that decrease the class confidence are highlighted blue.</p>\n\n<p>You can also write your own interpretation function. The demo below adds custom interpretation to the previous demo. This function will take the same inputs as the main wrapped function. The output of this interpretation function will be used to highlight the input of each input component - therefore the function must return a list where the number of elements corresponds to the number of input components. To see the format for interpretation for each input component, check the Docs.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import re\n\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\n# Number of arguments to interpretation function must\n# match number of inputs to prediction function\ndef interpret_gender(sentence):\n    result = gender_of_sentence(sentence)\n    is_male = result[\"male\"] > result[\"female\"]\n    interpretation = []\n    for word in re.split(\"( )\", sentence):\n        score = 0\n        token = word.lower()\n        if (is_male and token in male_words) or (not is_male and token in female_words):\n            score = 1\n        elif (is_male and token in female_words) or (\n            not is_male and token in male_words\n        ):\n            score = -1\n        interpretation.append((word, score))\n    # Output must be a list of lists containing the same number of elements as inputs\n    # Each element corresponds to the interpretation scores for the given input\n    return [interpretation]\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=interpret_gender,\n)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p>Learn more about Interpretation in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#interpretation\">docs</a>. </p>\n\n<h2 id=\"custom-styling\">Custom Styling</h2>\n\n<p>If you'd like to have more fine-grained control over any aspect of your demo, you can also write your own css or pass in a filepath to a css file, with the <code>css</code> parameter of the <code>Interface</code> class.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(..., css=\"body {background-color: red}\")\n</code></pre></div>\n\n<p>If you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with <code>\"file=\"</code>, for example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(..., css=\"body {background-image: url('file=clouds.jpg')}\")\n</code></pre></div>\n\n<h2 id=\"loading-hugging-face-models-and-spaces\">Loading Hugging Face Models and Spaces</h2>\n\n<p>Gradio integrates nicely with the <a rel=\"noopener\" target=\"_blank\" href=\"https://hf.co\">Hugging Face Hub</a>, allowing you to load models and Spaces with just one line of code. To use this, simply use the <code>load()</code> method in the <code>Interface</code> class. So:</p>\n\n<ul>\n<li>To load any model from the Hugging Face Hub and create an interface around it, you pass <code>\"model/\"</code> or <code>\"huggingface/\"</code> followed by the model name, like these examples:</li>\n</ul>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface.load(\"huggingface/gpt2\").launch();\n</code></pre></div>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\", \n    inputs=gr.Textbox(lines=5, label=\"Input Text\")  # customizes the input component\n).launch()\n</code></pre></div>\n\n<ul>\n<li>To load any Space from the Hugging Face Hub and recreate it locally (so that you can customize the inputs and outputs for example), you pass <code>\"spaces/\"</code> followed by the model name:</li>\n</ul>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface.load(\"spaces/eugenesiow/remove-bg\", inputs=\"webcam\", title=\"Remove your webcam background!\").launch()\n</code></pre></div>\n\n<p>One of the great things about loading Hugging Face models or spaces using Gradio is that you can then immediately use the resulting <code>Interface</code> object just like function in your Python code (this works for every type of model/space: text, images, audio, video, and even multimodal models):</p>\n\n<div class='codeblock'><pre><code class='lang-python'>io = gr.Interface.load(\"models/EleutherAI/gpt-neo-2.7B\")\nio(\"It was the best of times\")  # outputs model completion\n</code></pre></div>\n\n<h2 id=\"putting-interfaces-in-parallel-and-series\">Putting Interfaces in Parallel and Series</h2>\n\n<p>Gradio also lets you mix interfaces very easily using the <code>gradio.Parallel</code> and <code>gradio.Series</code> classes. <code>Parallel</code> lets you put two similar models (if they have the same input type) in parallel to compare model predictions:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>generator1 = gr.Interface.load(\"huggingface/gpt2\")\ngenerator2 = gr.Interface.load(\"huggingface/EleutherAI/gpt-neo-2.7B\")\ngenerator3 = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\")\n\ngr.Parallel(generator1, generator2, generator3).launch()\n</code></pre></div>\n\n<p><code>Series</code> lets you put models and spaces in series, piping the output of one model into the input of the next model. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>generator = gr.Interface.load(\"huggingface/gpt2\")\ntranslator = gr.Interface.load(\"huggingface/t5-small\")\n\ngr.Series(generator, translator).launch()  # this demo generates text, then translates it to German, and outputs the final result.\n</code></pre></div>\n\n<p>And of course, you can also mix <code>Parallel</code> and <code>Series</code> together whenever that makes sense!</p>\n\n<p>Learn more about Parallel and Series in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#parallel\">docs</a>. </p>\n", "tags": [], "spaces": [], "url": "/guides/advanced-interface-features/", "contributor": null}], "parent": "gradio"}, "tabbedinterface": {"class": null, "name": "TabbedInterface", "description": "A TabbedInterface is created by providing a list of Interfaces, each of which gets rendered in a separate tab.", "tags": {"demos": "stt_or_tts"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "interface_list", "annotation": "List[Interface]", "doc": "a list of interfaces to be rendered in tabs."}, {"name": "tab_names", "annotation": "List[str] | None", "doc": "a list of tab names. If None, the tab names will be \"Tab 1\", \"Tab 2\", etc.", "default": "None"}, {"name": "title", "annotation": "str | None", "doc": "a title for the interface; if provided, appears above the input and output components in large font. Also used as the tab title when opened in a browser window.", "default": "None"}, {"name": "theme", "annotation": "str", "doc": "which theme to use - right now, only \"default\" is supported.", "default": "\"default\""}, {"name": "analytics_enabled", "annotation": "bool | None", "doc": "whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable or default to True.", "default": "None"}, {"name": "css", "annotation": "str | None", "doc": "custom css or path to custom css file to apply to entire Blocks", "default": "None"}], "returns": {"annotation": null, "doc": "a Gradio Tabbed Interface for the given interfaces"}, "example": null, "fns": [], "demos": [["stt_or_tts", "import gradio as gr\n\ntitle = \"GPT-J-6B\"\n\ntts_examples = [\n    \"I love learning machine learning\",\n    \"How do you do?\",\n]\n\ntts_demo = gr.Interface.load(\n    \"huggingface/facebook/fastspeech2-en-ljspeech\",\n    title=None,\n    examples=tts_examples,\n    description=\"Give me something to say!\",\n)\n\nstt_demo = gr.Interface.load(\n    \"huggingface/facebook/wav2vec2-base-960h\",\n    title=None,\n    inputs=\"mic\",\n    description=\"Let me try to guess what you're saying!\",\n)\n\ndemo = gr.TabbedInterface([tts_demo, stt_demo], [\"Text-to-speech\", \"Speech-to-text\"])\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "parent": "gradio"}, "parallel": {"class": null, "name": "Parallel", "description": "Creates a new Interface consisting of multiple Interfaces in parallel (comparing their outputs). The Interfaces to put in Parallel must share the same input components (but can have different output components). <br>", "tags": {"demos": "interface_parallel, interface_parallel_load", "guides": "advanced-interface-features"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "interfaces", "annotation": "<class 'gradio.interface.Interface'>", "doc": "any number of Interface objects that are to be compared in parallel"}, {"name": "options", "annotation": "<class 'inspect._empty'>", "doc": "additional kwargs that are passed into the new Interface object to customize it", "kwargs": true}], "returns": {"annotation": null, "doc": "an Interface object comparing the given models"}, "example": null, "fns": [], "demos": [["interface_parallel", "import gradio as gr\n\ngreeter_1 = gr.Interface(lambda name: f\"Hello {name}!\", inputs=\"textbox\", outputs=gr.Textbox(label=\"Greeter 1\"))\ngreeter_2 = gr.Interface(lambda name: f\"Greetings {name}!\", inputs=\"textbox\", outputs=gr.Textbox(label=\"Greeter 2\"))\ndemo = gr.Parallel(greeter_1, greeter_2)\n\nif __name__ == \"__main__\":\n    demo.launch()"], ["interface_parallel_load", "import gradio as gr\n\ngenerator1 = gr.Interface.load(\"huggingface/gpt2\")\ngenerator2 = gr.Interface.load(\"huggingface/EleutherAI/gpt-neo-2.7B\")\ngenerator3 = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\")\n\ndemo = gr.Parallel(generator1, generator2, generator3)\n\nif __name__ == \"__main__\":\n    demo.launch()"]], "guides": [{"name": "advanced-interface-features", "category": "building-interfaces", "pretty_category": "Building Interfaces", "guide_index": 4, "absolute_index": 6, "pretty_name": "Advanced Interface Features", "content": "# Advanced Interface Features\n\nThere's more to cover on the [Interface](https://gradio.app/docs#interface) class. This guide covers all the advanced features: Using [Interpretation](https://gradio.app/docs#interpretation), custom styling, loading from the [Hugging Face Hub](https://hf.co), and using [Parallel](https://gradio.app/docs#parallel) and [Series](https://gradio.app/docs#series). \n\n## Interpreting your Predictions\n\nMost models are black boxes such that the internal logic of the function is hidden from the end user. To encourage transparency, we've made it very easy to add interpretation to your model by  simply setting the `interpretation` keyword in the `Interface` class to `default`. This allows your users to understand what parts of the input are responsible for the output. Take a look at the simple interface below which shows an image classifier that also includes interpretation:\n\n```python\nimport requests\nimport tensorflow as tf\n\nimport gradio as gr\n\ninception_net = tf.keras.applications.MobileNetV2()  # load the model\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n\ndef classify_image(inp):\n    inp = inp.reshape((-1, 224, 224, 3))\n    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n    prediction = inception_net.predict(inp).flatten()\n    return {labels[i]: float(prediction[i]) for i in range(1000)}\n\n\nimage = gr.Image(shape=(224, 224))\nlabel = gr.Label(num_top_classes=3)\n\ndemo = gr.Interface(\n    fn=classify_image, inputs=image, outputs=label, interpretation=\"default\"\n)\n\ndemo.launch()\n\n```\n\n\nIn addition to `default`, Gradio also includes [Shapley-based interpretation](https://christophm.github.io/interpretable-ml-book/shap.html), which provides more accurate interpretations, albeit usually with a slower runtime. To use this, simply set the `interpretation` parameter to `\"shap\"` (note: also make sure the python package `shap` is installed). Optionally, you can modify the `num_shap` parameter, which controls the tradeoff between accuracy and runtime (increasing this value generally increases accuracy). Here is an example:\n\n```python\ngr.Interface(fn=classify_image, inputs=image, outputs=label, interpretation=\"shap\", num_shap=5).launch()\n```\n\nThis will work for any function, even if internally, the model is a complex neural network or some other black box. If you use Gradio's `default` or `shap` interpretation, the output component must be a `Label`. All common input components are supported. Here is an example with text input.\n\n```python\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=\"default\",\n)\n\ndemo.launch()\n\n```\n\nSo what is happening under the hood? With these interpretation methods, Gradio runs the prediction multiple times with modified versions of the input. Based on the results, you'll see that the interface automatically highlights the parts of the text (or image, etc.) that contributed increased the likelihood of the class as red. The intensity of color corresponds to the importance of that part of the input. The parts that decrease the class confidence are highlighted blue.\n\nYou can also write your own interpretation function. The demo below adds custom interpretation to the previous demo. This function will take the same inputs as the main wrapped function. The output of this interpretation function will be used to highlight the input of each input component - therefore the function must return a list where the number of elements corresponds to the number of input components. To see the format for interpretation for each input component, check the Docs.\n\n```python\nimport re\n\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\n# Number of arguments to interpretation function must\n# match number of inputs to prediction function\ndef interpret_gender(sentence):\n    result = gender_of_sentence(sentence)\n    is_male = result[\"male\"] > result[\"female\"]\n    interpretation = []\n    for word in re.split(\"( )\", sentence):\n        score = 0\n        token = word.lower()\n        if (is_male and token in male_words) or (not is_male and token in female_words):\n            score = 1\n        elif (is_male and token in female_words) or (\n            not is_male and token in male_words\n        ):\n            score = -1\n        interpretation.append((word, score))\n    # Output must be a list of lists containing the same number of elements as inputs\n    # Each element corresponds to the interpretation scores for the given input\n    return [interpretation]\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=interpret_gender,\n)\n\ndemo.launch()\n\n```\n\nLearn more about Interpretation in the [docs](https://gradio.app/docs#interpretation). \n\n## Custom Styling\n\nIf you'd like to have more fine-grained control over any aspect of your demo, you can also write your own css or pass in a filepath to a css file, with the `css` parameter of the `Interface` class.\n\n```python\ngr.Interface(..., css=\"body {background-color: red}\")\n```\n\nIf you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with `\"file=\"`, for example:\n\n```python\ngr.Interface(..., css=\"body {background-image: url('file=clouds.jpg')}\")\n```\n\n## Loading Hugging Face Models and Spaces\n\nGradio integrates nicely with the [Hugging Face Hub](https://hf.co), allowing you to load models and Spaces with just one line of code. To use this, simply use the `load()` method in the `Interface` class. So:\n\n- To load any model from the Hugging Face Hub and create an interface around it, you pass `\"model/\"` or `\"huggingface/\"` followed by the model name, like these examples:\n\n```python\ngr.Interface.load(\"huggingface/gpt2\").launch();\n```\n\n```python\ngr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\", \n    inputs=gr.Textbox(lines=5, label=\"Input Text\")  # customizes the input component\n).launch()\n```\n\n- To load any Space from the Hugging Face Hub and recreate it locally (so that you can customize the inputs and outputs for example), you pass `\"spaces/\"` followed by the model name:\n\n```python\ngr.Interface.load(\"spaces/eugenesiow/remove-bg\", inputs=\"webcam\", title=\"Remove your webcam background!\").launch()\n```\n\nOne of the great things about loading Hugging Face models or spaces using Gradio is that you can then immediately use the resulting `Interface` object just like function in your Python code (this works for every type of model/space: text, images, audio, video, and even multimodal models):\n\n```python\nio = gr.Interface.load(\"models/EleutherAI/gpt-neo-2.7B\")\nio(\"It was the best of times\")  # outputs model completion\n```\n\n## Putting Interfaces in Parallel and Series\n\nGradio also lets you mix interfaces very easily using the `gradio.Parallel` and `gradio.Series` classes. `Parallel` lets you put two similar models (if they have the same input type) in parallel to compare model predictions:\n\n```python\ngenerator1 = gr.Interface.load(\"huggingface/gpt2\")\ngenerator2 = gr.Interface.load(\"huggingface/EleutherAI/gpt-neo-2.7B\")\ngenerator3 = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\")\n\ngr.Parallel(generator1, generator2, generator3).launch()\n```\n\n`Series` lets you put models and spaces in series, piping the output of one model into the input of the next model. \n\n```python\ngenerator = gr.Interface.load(\"huggingface/gpt2\")\ntranslator = gr.Interface.load(\"huggingface/t5-small\")\n\ngr.Series(generator, translator).launch()  # this demo generates text, then translates it to German, and outputs the final result.\n```\n\nAnd of course, you can also mix `Parallel` and `Series` together whenever that makes sense!\n\nLearn more about Parallel and Series in the [docs](https://gradio.app/docs#parallel). ", "html": "<h1 id=\"advanced-interface-features\">Advanced Interface Features</h1>\n\n<p>There's more to cover on the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#interface\">Interface</a> class. This guide covers all the advanced features: Using <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#interpretation\">Interpretation</a>, custom styling, loading from the <a rel=\"noopener\" target=\"_blank\" href=\"https://hf.co\">Hugging Face Hub</a>, and using <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#parallel\">Parallel</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#series\">Series</a>. </p>\n\n<h2 id=\"interpreting-your-predictions\">Interpreting your Predictions</h2>\n\n<p>Most models are black boxes such that the internal logic of the function is hidden from the end user. To encourage transparency, we've made it very easy to add interpretation to your model by  simply setting the <code>interpretation</code> keyword in the <code>Interface</code> class to <code>default</code>. This allows your users to understand what parts of the input are responsible for the output. Take a look at the simple interface below which shows an image classifier that also includes interpretation:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import requests\nimport tensorflow as tf\n\nimport gradio as gr\n\ninception_net = tf.keras.applications.MobileNetV2()  # load the model\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n\ndef classify_image(inp):\n    inp = inp.reshape((-1, 224, 224, 3))\n    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n    prediction = inception_net.predict(inp).flatten()\n    return {labels[i]: float(prediction[i]) for i in range(1000)}\n\n\nimage = gr.Image(shape=(224, 224))\nlabel = gr.Label(num_top_classes=3)\n\ndemo = gr.Interface(\n    fn=classify_image, inputs=image, outputs=label, interpretation=\"default\"\n)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p>In addition to <code>default</code>, Gradio also includes <a rel=\"noopener\" target=\"_blank\" href=\"https://christophm.github.io/interpretable-ml-book/shap.html\">Shapley-based interpretation</a>, which provides more accurate interpretations, albeit usually with a slower runtime. To use this, simply set the <code>interpretation</code> parameter to <code>\"shap\"</code> (note: also make sure the python package <code>shap</code> is installed). Optionally, you can modify the <code>num_shap</code> parameter, which controls the tradeoff between accuracy and runtime (increasing this value generally increases accuracy). Here is an example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(fn=classify_image, inputs=image, outputs=label, interpretation=\"shap\", num_shap=5).launch()\n</code></pre></div>\n\n<p>This will work for any function, even if internally, the model is a complex neural network or some other black box. If you use Gradio's <code>default</code> or <code>shap</code> interpretation, the output component must be a <code>Label</code>. All common input components are supported. Here is an example with text input.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=\"default\",\n)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p>So what is happening under the hood? With these interpretation methods, Gradio runs the prediction multiple times with modified versions of the input. Based on the results, you'll see that the interface automatically highlights the parts of the text (or image, etc.) that contributed increased the likelihood of the class as red. The intensity of color corresponds to the importance of that part of the input. The parts that decrease the class confidence are highlighted blue.</p>\n\n<p>You can also write your own interpretation function. The demo below adds custom interpretation to the previous demo. This function will take the same inputs as the main wrapped function. The output of this interpretation function will be used to highlight the input of each input component - therefore the function must return a list where the number of elements corresponds to the number of input components. To see the format for interpretation for each input component, check the Docs.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import re\n\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\n# Number of arguments to interpretation function must\n# match number of inputs to prediction function\ndef interpret_gender(sentence):\n    result = gender_of_sentence(sentence)\n    is_male = result[\"male\"] > result[\"female\"]\n    interpretation = []\n    for word in re.split(\"( )\", sentence):\n        score = 0\n        token = word.lower()\n        if (is_male and token in male_words) or (not is_male and token in female_words):\n            score = 1\n        elif (is_male and token in female_words) or (\n            not is_male and token in male_words\n        ):\n            score = -1\n        interpretation.append((word, score))\n    # Output must be a list of lists containing the same number of elements as inputs\n    # Each element corresponds to the interpretation scores for the given input\n    return [interpretation]\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=interpret_gender,\n)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p>Learn more about Interpretation in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#interpretation\">docs</a>. </p>\n\n<h2 id=\"custom-styling\">Custom Styling</h2>\n\n<p>If you'd like to have more fine-grained control over any aspect of your demo, you can also write your own css or pass in a filepath to a css file, with the <code>css</code> parameter of the <code>Interface</code> class.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(..., css=\"body {background-color: red}\")\n</code></pre></div>\n\n<p>If you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with <code>\"file=\"</code>, for example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(..., css=\"body {background-image: url('file=clouds.jpg')}\")\n</code></pre></div>\n\n<h2 id=\"loading-hugging-face-models-and-spaces\">Loading Hugging Face Models and Spaces</h2>\n\n<p>Gradio integrates nicely with the <a rel=\"noopener\" target=\"_blank\" href=\"https://hf.co\">Hugging Face Hub</a>, allowing you to load models and Spaces with just one line of code. To use this, simply use the <code>load()</code> method in the <code>Interface</code> class. So:</p>\n\n<ul>\n<li>To load any model from the Hugging Face Hub and create an interface around it, you pass <code>\"model/\"</code> or <code>\"huggingface/\"</code> followed by the model name, like these examples:</li>\n</ul>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface.load(\"huggingface/gpt2\").launch();\n</code></pre></div>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\", \n    inputs=gr.Textbox(lines=5, label=\"Input Text\")  # customizes the input component\n).launch()\n</code></pre></div>\n\n<ul>\n<li>To load any Space from the Hugging Face Hub and recreate it locally (so that you can customize the inputs and outputs for example), you pass <code>\"spaces/\"</code> followed by the model name:</li>\n</ul>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface.load(\"spaces/eugenesiow/remove-bg\", inputs=\"webcam\", title=\"Remove your webcam background!\").launch()\n</code></pre></div>\n\n<p>One of the great things about loading Hugging Face models or spaces using Gradio is that you can then immediately use the resulting <code>Interface</code> object just like function in your Python code (this works for every type of model/space: text, images, audio, video, and even multimodal models):</p>\n\n<div class='codeblock'><pre><code class='lang-python'>io = gr.Interface.load(\"models/EleutherAI/gpt-neo-2.7B\")\nio(\"It was the best of times\")  # outputs model completion\n</code></pre></div>\n\n<h2 id=\"putting-interfaces-in-parallel-and-series\">Putting Interfaces in Parallel and Series</h2>\n\n<p>Gradio also lets you mix interfaces very easily using the <code>gradio.Parallel</code> and <code>gradio.Series</code> classes. <code>Parallel</code> lets you put two similar models (if they have the same input type) in parallel to compare model predictions:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>generator1 = gr.Interface.load(\"huggingface/gpt2\")\ngenerator2 = gr.Interface.load(\"huggingface/EleutherAI/gpt-neo-2.7B\")\ngenerator3 = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\")\n\ngr.Parallel(generator1, generator2, generator3).launch()\n</code></pre></div>\n\n<p><code>Series</code> lets you put models and spaces in series, piping the output of one model into the input of the next model. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>generator = gr.Interface.load(\"huggingface/gpt2\")\ntranslator = gr.Interface.load(\"huggingface/t5-small\")\n\ngr.Series(generator, translator).launch()  # this demo generates text, then translates it to German, and outputs the final result.\n</code></pre></div>\n\n<p>And of course, you can also mix <code>Parallel</code> and <code>Series</code> together whenever that makes sense!</p>\n\n<p>Learn more about Parallel and Series in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#parallel\">docs</a>. </p>\n", "tags": [], "spaces": [], "url": "/guides/advanced-interface-features/", "contributor": null}], "parent": "gradio"}, "series": {"class": null, "name": "Series", "description": "Creates a new Interface from multiple Interfaces in series (the output of one is fed as the input to the next, and so the input and output components must agree between the interfaces). <br>", "tags": {"demos": "interface_series, interface_series_load", "guides": "advanced-interface-features"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "interfaces", "annotation": "<class 'gradio.interface.Interface'>", "doc": "any number of Interface objects that are to be connected in series"}, {"name": "options", "annotation": "<class 'inspect._empty'>", "doc": "additional kwargs that are passed into the new Interface object to customize it", "kwargs": true}], "returns": {"annotation": null, "doc": "an Interface object connecting the given models"}, "example": null, "fns": [], "demos": [["interface_series", "import gradio as gr\n\nget_name = gr.Interface(lambda name: name, inputs=\"textbox\", outputs=\"textbox\")\nprepend_hello = gr.Interface(lambda name: f\"Hello {name}!\", inputs=\"textbox\", outputs=\"textbox\")\nappend_nice = gr.Interface(lambda greeting: f\"{greeting} Nice to meet you!\",\n                           inputs=\"textbox\", outputs=gr.Textbox(label=\"Greeting\"))\ndemo = gr.Series(get_name, prepend_hello, append_nice)\n\nif __name__ == \"__main__\":\n    demo.launch()"], ["interface_series_load", "import gradio as gr\n\ngenerator = gr.Interface.load(\"huggingface/gpt2\")\ntranslator = gr.Interface.load(\"huggingface/t5-small\")\n\ndemo = gr.Series(generator, translator, description=\"This demo combines two Spaces: a text generator (`huggingface/gpt2`) and a text translator (`huggingface/t5-small`). The first Space takes a prompt as input and generates a text. The second Space takes the generated text as input and translates it into another language.\")\n\nif __name__ == \"__main__\":\n    demo.launch()"]], "guides": [{"name": "advanced-interface-features", "category": "building-interfaces", "pretty_category": "Building Interfaces", "guide_index": 4, "absolute_index": 6, "pretty_name": "Advanced Interface Features", "content": "# Advanced Interface Features\n\nThere's more to cover on the [Interface](https://gradio.app/docs#interface) class. This guide covers all the advanced features: Using [Interpretation](https://gradio.app/docs#interpretation), custom styling, loading from the [Hugging Face Hub](https://hf.co), and using [Parallel](https://gradio.app/docs#parallel) and [Series](https://gradio.app/docs#series). \n\n## Interpreting your Predictions\n\nMost models are black boxes such that the internal logic of the function is hidden from the end user. To encourage transparency, we've made it very easy to add interpretation to your model by  simply setting the `interpretation` keyword in the `Interface` class to `default`. This allows your users to understand what parts of the input are responsible for the output. Take a look at the simple interface below which shows an image classifier that also includes interpretation:\n\n```python\nimport requests\nimport tensorflow as tf\n\nimport gradio as gr\n\ninception_net = tf.keras.applications.MobileNetV2()  # load the model\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n\ndef classify_image(inp):\n    inp = inp.reshape((-1, 224, 224, 3))\n    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n    prediction = inception_net.predict(inp).flatten()\n    return {labels[i]: float(prediction[i]) for i in range(1000)}\n\n\nimage = gr.Image(shape=(224, 224))\nlabel = gr.Label(num_top_classes=3)\n\ndemo = gr.Interface(\n    fn=classify_image, inputs=image, outputs=label, interpretation=\"default\"\n)\n\ndemo.launch()\n\n```\n\n\nIn addition to `default`, Gradio also includes [Shapley-based interpretation](https://christophm.github.io/interpretable-ml-book/shap.html), which provides more accurate interpretations, albeit usually with a slower runtime. To use this, simply set the `interpretation` parameter to `\"shap\"` (note: also make sure the python package `shap` is installed). Optionally, you can modify the `num_shap` parameter, which controls the tradeoff between accuracy and runtime (increasing this value generally increases accuracy). Here is an example:\n\n```python\ngr.Interface(fn=classify_image, inputs=image, outputs=label, interpretation=\"shap\", num_shap=5).launch()\n```\n\nThis will work for any function, even if internally, the model is a complex neural network or some other black box. If you use Gradio's `default` or `shap` interpretation, the output component must be a `Label`. All common input components are supported. Here is an example with text input.\n\n```python\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=\"default\",\n)\n\ndemo.launch()\n\n```\n\nSo what is happening under the hood? With these interpretation methods, Gradio runs the prediction multiple times with modified versions of the input. Based on the results, you'll see that the interface automatically highlights the parts of the text (or image, etc.) that contributed increased the likelihood of the class as red. The intensity of color corresponds to the importance of that part of the input. The parts that decrease the class confidence are highlighted blue.\n\nYou can also write your own interpretation function. The demo below adds custom interpretation to the previous demo. This function will take the same inputs as the main wrapped function. The output of this interpretation function will be used to highlight the input of each input component - therefore the function must return a list where the number of elements corresponds to the number of input components. To see the format for interpretation for each input component, check the Docs.\n\n```python\nimport re\n\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\n# Number of arguments to interpretation function must\n# match number of inputs to prediction function\ndef interpret_gender(sentence):\n    result = gender_of_sentence(sentence)\n    is_male = result[\"male\"] > result[\"female\"]\n    interpretation = []\n    for word in re.split(\"( )\", sentence):\n        score = 0\n        token = word.lower()\n        if (is_male and token in male_words) or (not is_male and token in female_words):\n            score = 1\n        elif (is_male and token in female_words) or (\n            not is_male and token in male_words\n        ):\n            score = -1\n        interpretation.append((word, score))\n    # Output must be a list of lists containing the same number of elements as inputs\n    # Each element corresponds to the interpretation scores for the given input\n    return [interpretation]\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=interpret_gender,\n)\n\ndemo.launch()\n\n```\n\nLearn more about Interpretation in the [docs](https://gradio.app/docs#interpretation). \n\n## Custom Styling\n\nIf you'd like to have more fine-grained control over any aspect of your demo, you can also write your own css or pass in a filepath to a css file, with the `css` parameter of the `Interface` class.\n\n```python\ngr.Interface(..., css=\"body {background-color: red}\")\n```\n\nIf you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with `\"file=\"`, for example:\n\n```python\ngr.Interface(..., css=\"body {background-image: url('file=clouds.jpg')}\")\n```\n\n## Loading Hugging Face Models and Spaces\n\nGradio integrates nicely with the [Hugging Face Hub](https://hf.co), allowing you to load models and Spaces with just one line of code. To use this, simply use the `load()` method in the `Interface` class. So:\n\n- To load any model from the Hugging Face Hub and create an interface around it, you pass `\"model/\"` or `\"huggingface/\"` followed by the model name, like these examples:\n\n```python\ngr.Interface.load(\"huggingface/gpt2\").launch();\n```\n\n```python\ngr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\", \n    inputs=gr.Textbox(lines=5, label=\"Input Text\")  # customizes the input component\n).launch()\n```\n\n- To load any Space from the Hugging Face Hub and recreate it locally (so that you can customize the inputs and outputs for example), you pass `\"spaces/\"` followed by the model name:\n\n```python\ngr.Interface.load(\"spaces/eugenesiow/remove-bg\", inputs=\"webcam\", title=\"Remove your webcam background!\").launch()\n```\n\nOne of the great things about loading Hugging Face models or spaces using Gradio is that you can then immediately use the resulting `Interface` object just like function in your Python code (this works for every type of model/space: text, images, audio, video, and even multimodal models):\n\n```python\nio = gr.Interface.load(\"models/EleutherAI/gpt-neo-2.7B\")\nio(\"It was the best of times\")  # outputs model completion\n```\n\n## Putting Interfaces in Parallel and Series\n\nGradio also lets you mix interfaces very easily using the `gradio.Parallel` and `gradio.Series` classes. `Parallel` lets you put two similar models (if they have the same input type) in parallel to compare model predictions:\n\n```python\ngenerator1 = gr.Interface.load(\"huggingface/gpt2\")\ngenerator2 = gr.Interface.load(\"huggingface/EleutherAI/gpt-neo-2.7B\")\ngenerator3 = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\")\n\ngr.Parallel(generator1, generator2, generator3).launch()\n```\n\n`Series` lets you put models and spaces in series, piping the output of one model into the input of the next model. \n\n```python\ngenerator = gr.Interface.load(\"huggingface/gpt2\")\ntranslator = gr.Interface.load(\"huggingface/t5-small\")\n\ngr.Series(generator, translator).launch()  # this demo generates text, then translates it to German, and outputs the final result.\n```\n\nAnd of course, you can also mix `Parallel` and `Series` together whenever that makes sense!\n\nLearn more about Parallel and Series in the [docs](https://gradio.app/docs#parallel). ", "html": "<h1 id=\"advanced-interface-features\">Advanced Interface Features</h1>\n\n<p>There's more to cover on the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#interface\">Interface</a> class. This guide covers all the advanced features: Using <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#interpretation\">Interpretation</a>, custom styling, loading from the <a rel=\"noopener\" target=\"_blank\" href=\"https://hf.co\">Hugging Face Hub</a>, and using <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#parallel\">Parallel</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#series\">Series</a>. </p>\n\n<h2 id=\"interpreting-your-predictions\">Interpreting your Predictions</h2>\n\n<p>Most models are black boxes such that the internal logic of the function is hidden from the end user. To encourage transparency, we've made it very easy to add interpretation to your model by  simply setting the <code>interpretation</code> keyword in the <code>Interface</code> class to <code>default</code>. This allows your users to understand what parts of the input are responsible for the output. Take a look at the simple interface below which shows an image classifier that also includes interpretation:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import requests\nimport tensorflow as tf\n\nimport gradio as gr\n\ninception_net = tf.keras.applications.MobileNetV2()  # load the model\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n\ndef classify_image(inp):\n    inp = inp.reshape((-1, 224, 224, 3))\n    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n    prediction = inception_net.predict(inp).flatten()\n    return {labels[i]: float(prediction[i]) for i in range(1000)}\n\n\nimage = gr.Image(shape=(224, 224))\nlabel = gr.Label(num_top_classes=3)\n\ndemo = gr.Interface(\n    fn=classify_image, inputs=image, outputs=label, interpretation=\"default\"\n)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p>In addition to <code>default</code>, Gradio also includes <a rel=\"noopener\" target=\"_blank\" href=\"https://christophm.github.io/interpretable-ml-book/shap.html\">Shapley-based interpretation</a>, which provides more accurate interpretations, albeit usually with a slower runtime. To use this, simply set the <code>interpretation</code> parameter to <code>\"shap\"</code> (note: also make sure the python package <code>shap</code> is installed). Optionally, you can modify the <code>num_shap</code> parameter, which controls the tradeoff between accuracy and runtime (increasing this value generally increases accuracy). Here is an example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(fn=classify_image, inputs=image, outputs=label, interpretation=\"shap\", num_shap=5).launch()\n</code></pre></div>\n\n<p>This will work for any function, even if internally, the model is a complex neural network or some other black box. If you use Gradio's <code>default</code> or <code>shap</code> interpretation, the output component must be a <code>Label</code>. All common input components are supported. Here is an example with text input.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=\"default\",\n)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p>So what is happening under the hood? With these interpretation methods, Gradio runs the prediction multiple times with modified versions of the input. Based on the results, you'll see that the interface automatically highlights the parts of the text (or image, etc.) that contributed increased the likelihood of the class as red. The intensity of color corresponds to the importance of that part of the input. The parts that decrease the class confidence are highlighted blue.</p>\n\n<p>You can also write your own interpretation function. The demo below adds custom interpretation to the previous demo. This function will take the same inputs as the main wrapped function. The output of this interpretation function will be used to highlight the input of each input component - therefore the function must return a list where the number of elements corresponds to the number of input components. To see the format for interpretation for each input component, check the Docs.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import re\n\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count / total, \"female\": female_count / total}\n\n\n# Number of arguments to interpretation function must\n# match number of inputs to prediction function\ndef interpret_gender(sentence):\n    result = gender_of_sentence(sentence)\n    is_male = result[\"male\"] > result[\"female\"]\n    interpretation = []\n    for word in re.split(\"( )\", sentence):\n        score = 0\n        token = word.lower()\n        if (is_male and token in male_words) or (not is_male and token in female_words):\n            score = 1\n        elif (is_male and token in female_words) or (\n            not is_male and token in male_words\n        ):\n            score = -1\n        interpretation.append((word, score))\n    # Output must be a list of lists containing the same number of elements as inputs\n    # Each element corresponds to the interpretation scores for the given input\n    return [interpretation]\n\n\ndemo = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.Textbox(value=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=interpret_gender,\n)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p>Learn more about Interpretation in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#interpretation\">docs</a>. </p>\n\n<h2 id=\"custom-styling\">Custom Styling</h2>\n\n<p>If you'd like to have more fine-grained control over any aspect of your demo, you can also write your own css or pass in a filepath to a css file, with the <code>css</code> parameter of the <code>Interface</code> class.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(..., css=\"body {background-color: red}\")\n</code></pre></div>\n\n<p>If you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with <code>\"file=\"</code>, for example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(..., css=\"body {background-image: url('file=clouds.jpg')}\")\n</code></pre></div>\n\n<h2 id=\"loading-hugging-face-models-and-spaces\">Loading Hugging Face Models and Spaces</h2>\n\n<p>Gradio integrates nicely with the <a rel=\"noopener\" target=\"_blank\" href=\"https://hf.co\">Hugging Face Hub</a>, allowing you to load models and Spaces with just one line of code. To use this, simply use the <code>load()</code> method in the <code>Interface</code> class. So:</p>\n\n<ul>\n<li>To load any model from the Hugging Face Hub and create an interface around it, you pass <code>\"model/\"</code> or <code>\"huggingface/\"</code> followed by the model name, like these examples:</li>\n</ul>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface.load(\"huggingface/gpt2\").launch();\n</code></pre></div>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\", \n    inputs=gr.Textbox(lines=5, label=\"Input Text\")  # customizes the input component\n).launch()\n</code></pre></div>\n\n<ul>\n<li>To load any Space from the Hugging Face Hub and recreate it locally (so that you can customize the inputs and outputs for example), you pass <code>\"spaces/\"</code> followed by the model name:</li>\n</ul>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface.load(\"spaces/eugenesiow/remove-bg\", inputs=\"webcam\", title=\"Remove your webcam background!\").launch()\n</code></pre></div>\n\n<p>One of the great things about loading Hugging Face models or spaces using Gradio is that you can then immediately use the resulting <code>Interface</code> object just like function in your Python code (this works for every type of model/space: text, images, audio, video, and even multimodal models):</p>\n\n<div class='codeblock'><pre><code class='lang-python'>io = gr.Interface.load(\"models/EleutherAI/gpt-neo-2.7B\")\nio(\"It was the best of times\")  # outputs model completion\n</code></pre></div>\n\n<h2 id=\"putting-interfaces-in-parallel-and-series\">Putting Interfaces in Parallel and Series</h2>\n\n<p>Gradio also lets you mix interfaces very easily using the <code>gradio.Parallel</code> and <code>gradio.Series</code> classes. <code>Parallel</code> lets you put two similar models (if they have the same input type) in parallel to compare model predictions:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>generator1 = gr.Interface.load(\"huggingface/gpt2\")\ngenerator2 = gr.Interface.load(\"huggingface/EleutherAI/gpt-neo-2.7B\")\ngenerator3 = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\")\n\ngr.Parallel(generator1, generator2, generator3).launch()\n</code></pre></div>\n\n<p><code>Series</code> lets you put models and spaces in series, piping the output of one model into the input of the next model. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>generator = gr.Interface.load(\"huggingface/gpt2\")\ntranslator = gr.Interface.load(\"huggingface/t5-small\")\n\ngr.Series(generator, translator).launch()  # this demo generates text, then translates it to German, and outputs the final result.\n</code></pre></div>\n\n<p>And of course, you can also mix <code>Parallel</code> and <code>Series</code> together whenever that makes sense!</p>\n\n<p>Learn more about Parallel and Series in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#parallel\">docs</a>. </p>\n", "tags": [], "spaces": [], "url": "/guides/advanced-interface-features/", "contributor": null}], "parent": "gradio"}}, "components": {"audio": {"class": null, "name": "Audio", "description": "Creates an audio component that can be used to upload/record audio (as an input) or display audio (as an output).", "tags": {"preprocessing": "passes the uploaded audio as a {Tuple(int, numpy.array)} corresponding to (sample rate, data) or as a {str} filepath, depending on `type`", "postprocessing": "expects a {Tuple(int, numpy.array)} corresponding to (sample rate, data) or as a {str} filepath or URL to an audio file, which gets displayed", "examples-format": "a {str} filepath to a local file that contains audio.", "demos": "main_note, generate_tone, reverse_audio", "guides": "real-time-speech-recognition"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | Tuple[int, np.ndarray] | Callable | None", "doc": "A path, URL, or [sample_rate, numpy array] tuple for the default value that Audio component is going to take. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "source", "annotation": "str", "doc": "Source of audio. \"upload\" creates a box where user can drop an audio file, \"microphone\" creates a microphone input.", "default": "\"upload\""}, {"name": "type", "annotation": "str", "doc": "The format the audio file is converted to before being passed into the prediction function. \"numpy\" converts the audio to a tuple consisting of: (int sample rate, numpy.array for the data), \"filepath\" passes a str path to a temporary file containing the audio.", "default": "\"numpy\""}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, will allow users to upload and edit a audio file; if False, can only be used to play audio. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "streaming", "annotation": "bool", "doc": "If set to True when used in a `live` interface, will automatically stream webcam feed. Only valid is source is 'microphone'.", "default": "False"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Audio"}, {"fn": null, "name": "clear", "description": "This event is triggered when the user clears the component (e.g. image or audio) using the X button for the component. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Audio"}, {"fn": null, "name": "play", "description": "This event is triggered when the user plays the component (e.g. audio or video). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Audio"}, {"fn": null, "name": "pause", "description": "This event is triggered when the user pauses the component (e.g. audio or video). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Audio"}, {"fn": null, "name": "stop", "description": "This event is triggered when the user stops the component (e.g. audio or video). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Audio"}, {"fn": null, "name": "stream", "description": "This event is triggered when the user streams the component (e.g. a live webcam component)", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable", "doc": "Callable function"}, {"name": "inputs", "annotation": "List[Component]", "doc": "List of inputs"}, {"name": "outputs", "annotation": "List[Component]", "doc": "List of outputs"}, {"name": "api_name", "annotation": "str | None", "doc": null, "default": "None"}, {"name": "preprocess", "annotation": "bool", "doc": null, "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": null, "default": "True"}], "returns": {}, "example": null, "parent": "gradio.Audio"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the audio component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}], "returns": {}, "example": null, "parent": "gradio.Audio"}], "string_shortcuts": [["Audio", "audio", "Uses default values"], ["Microphone", "microphone", "Uses source=\"microphone\""]], "demos": [["main_note", "from math import log2, pow\nimport os\n\nimport numpy as np\nfrom scipy.fftpack import fft\n\nimport gradio as gr\n\nA4 = 440\nC0 = A4 * pow(2, -4.75)\nname = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n\n\ndef get_pitch(freq):\n    h = round(12 * log2(freq / C0))\n    n = h % 12\n    return name[n]\n\n\ndef main_note(audio):\n    rate, y = audio\n    if len(y.shape) == 2:\n        y = y.T[0]\n    N = len(y)\n    T = 1.0 / rate\n    x = np.linspace(0.0, N * T, N)\n    yf = fft(y)\n    yf2 = 2.0 / N * np.abs(yf[0 : N // 2])\n    xf = np.linspace(0.0, 1.0 / (2.0 * T), N // 2)\n\n    volume_per_pitch = {}\n    total_volume = np.sum(yf2)\n    for freq, volume in zip(xf, yf2):\n        if freq == 0:\n            continue\n        pitch = get_pitch(freq)\n        if pitch not in volume_per_pitch:\n            volume_per_pitch[pitch] = 0\n        volume_per_pitch[pitch] += 1.0 * volume / total_volume\n    volume_per_pitch = {k: float(v) for k, v in volume_per_pitch.items()}\n    return volume_per_pitch\n\n\ndemo = gr.Interface(\n    main_note,\n    gr.Audio(source=\"microphone\"),\n    gr.Label(num_top_classes=4),\n    examples=[\n        [os.path.join(os.path.dirname(__file__),\"audio/recording1.wav\")],\n        [os.path.join(os.path.dirname(__file__),\"audio/cantina.wav\")],\n    ],\n    interpretation=\"default\",\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["generate_tone", "import numpy as np\nimport gradio as gr\n\nnotes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n\ndef generate_tone(note, octave, duration):\n    sr = 48000\n    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)\n    frequency = a4_freq * 2 ** (tones_from_a4 / 12)\n    duration = int(duration)\n    audio = np.linspace(0, duration, duration * sr)\n    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)\n    return sr, audio\n\ndemo = gr.Interface(\n    generate_tone,\n    [\n        gr.Dropdown(notes, type=\"index\"),\n        gr.Slider(4, 6, step=1),\n        gr.Textbox(value=1, label=\"Duration in seconds\"),\n    ],\n    \"audio\",\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["reverse_audio", "import os\n\nimport numpy as np\n\nimport gradio as gr\n\n\ndef reverse_audio(audio):\n    sr, data = audio\n    return (sr, np.flipud(data))\n\n\ndemo = gr.Interface(fn=reverse_audio, \n                    inputs=\"microphone\", \n                    outputs=\"audio\", \n                    examples=[\n                    \"https://samplelib.com/lib/preview/mp3/sample-3s.mp3\",\n                    os.path.join(os.path.dirname(__file__), \"audio/recording1.wav\")\n        ], cache_examples=True)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()", "clear()", "pause()", "play()", "stop()", "stream()", "upload()"], "events": "change(), clear(), pause(), play(), stop(), stream(), upload()", "guides": [{"name": "real-time-speech-recognition", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 33, "pretty_name": "Real Time Speech Recognition", "content": "# Real Time Speech Recognition \n\nRelated spaces: https://huggingface.co/spaces/abidlabs/streaming-asr-paused, https://huggingface.co/spaces/abidlabs/full-context-asr\nTags: ASR, SPEECH, STREAMING\n\n## Introduction\n\nAutomatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).\n\nUsing `gradio`, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.\n\nThis tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a ***full-context*** model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it ***streaming***, meaning that the audio model will convert speech as you speak. The streaming demo that we create will look something like this (try it below or [in a new tab](https://huggingface.co/spaces/abidlabs/streaming-asr-paused)!):\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nReal-time ASR is inherently *stateful*, meaning that the model's predictions change depending on what words the user previously spoke. So, in this tutorial, we will also cover how to use **state** with Gradio demos. \n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:\n\n* Transformers (for this, `pip install transformers` and `pip install torch`) \n* DeepSpeech (`pip install deepspeech==0.8.2`)\n\nMake sure you have at least one of these installed so that you can follow along the tutorial. You will also need `ffmpeg` [installed on your system](https://www.ffmpeg.org/download.html), if you do not already have it, to process files from the microphone.\n\nHere's how to build a real time speech recognition (ASR) app: \n\n1. [Set up the Transformers ASR Model](#1-set-up-the-transformers-asr-model)\n2. [Create a Full-Context ASR Demo with Transformers](#2-create-a-full-context-asr-demo-with-transformers) \n3. [Create a Streaming ASR Demo  with Transformers](#3-create-a-streaming-asr-demo-with-transformers)\n4. [Create a Streaming ASR Demo with DeepSpeech](#4-create-a-streaming-asr-demo-with-deepspeech)\n\n\n## 1. Set up the Transformers ASR Model\n\nFirst, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the Hugging Face model, `Wav2Vec2`. \n\nHere is the code to load `Wav2Vec2` from Hugging Face `transformers`.\n\n```python\nfrom transformers import pipeline\n\np = pipeline(\"automatic-speech-recognition\")\n```\n\nThat's it! By default, the automatic speech recognition model pipeline loads Facebook's `facebook/wav2vec2-base-960h` model.\n\n## 2. Create a Full-Context ASR Demo with Transformers \n\nWe will start by creating a *full-context* ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the `pipeline` object above.\n\nWe will use `gradio`'s built in `Audio` component, configured to take input from the user's microphone and return a filepath for the recorded audio. The output component will be a plain `Textbox`.\n\n```python\nimport gradio as gr\n\ndef transcribe(audio):\n    text = p(audio)[\"text\"]\n    return text\n\ngr.Interface(\n    fn=transcribe, \n    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n    outputs=\"text\").launch()\n```\n\nSo what's happening here? The `transcribe` function takes a single parameter, `audio`, which is a filepath to the audio file that the user has recorded. The `pipeline` object expects a filepath and converts it to text, which is returned to the frontend and displayed in a textbox. \n\nLet's see it in action! (Record a short audio clip and then click submit, or [open in a new tab](https://huggingface.co/spaces/abidlabs/full-context-asr)):\n\n<iframe src=\"https://abidlabs-full-context-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n## 3. Create a Streaming ASR Demo  with Transformers\n\nOk great! We've built an ASR model that works well for short audio clips. However, if you are recording longer audio clips, you probably want a *streaming* interface, one that transcribes audio as the user speaks instead of just all-at-once at the end.\n\nThe good news is that it's not too difficult to adapt the demo we just made to make it streaming, using the same `Wav2Vec2` model. \n\nThe biggest change is that we must now introduce a `state` parameter, which holds the audio that has been *transcribed so far*. This allows us to only the latest chunk of audio and simply append it to the audio we previously transcribed. \n\nWhen adding state to a Gradio demo, you need to do a total of 3 things:\n\n* Add a `state` parameter to the function\n* Return the updated `state` at the end of the function\n* Add the `\"state\"` components to the `inputs` and `outputs` in `Interface` \n\nHere's what the code looks like:\n\n```python\ndef transcribe(audio, state=\"\"):\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\n# Set the starting state to an empty string\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\" \n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n```\n\nNotice that we've also made one other change, which is that we've set `live=True`. This keeps the Gradio interface running constantly, so it automatically transcribes audio without the user having to repeatedly hit the submit button.\n\nLet's see how it does (try below or [in a new tab](https://huggingface.co/spaces/abidlabs/streaming-asr))!\n\n<iframe src=\"https://abidlabs-streaming-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nOne thing that you may notice is that the transcription quality has dropped since the chunks of audio are so small, they lack the context to properly be transcribed. A \"hacky\" fix to this is to simply increase the runtime of the `transcribe()` function so that longer audio chunks are processed. We can do this by adding a `time.sleep()` inside the function, as shown below (we'll see a proper fix next) \n\n```python\nfrom transformers import pipeline\nimport gradio as gr\nimport time\n\np = pipeline(\"automatic-speech-recognition\")\n\ndef transcribe(audio, state=\"\"):\n    time.sleep(2)\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\"\n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n```\n\nTry the demo below to see the difference (or [open in a new tab](https://huggingface.co/spaces/abidlabs/streaming-asr-paused))!\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\n## 4. Create a Streaming ASR Demo with DeepSpeech\n\nYou're not restricted to ASR models from the `transformers` library -- you can use your own models or models from other libraries. The `DeepSpeech` library contains models that are specifically designed to handle streaming audio data. These models perform really well with  streaming data as they are able to account for previous chunks of audio data when making predictions.\n\nGoing through the DeepSpeech library is beyond the scope of this Guide (check out their [excellent documentation here](https://deepspeech.readthedocs.io/en/r0.9/)), but you can use Gradio very similarly with a DeepSpeech ASR model as with a Transformers ASR model. \n\nHere's a complete example (on Linux):\n\nFirst install the DeepSpeech library and download the pretrained models from the terminal:\n\n```bash\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.pbmm\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.scorer\napt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\npip install deepspeech==0.8.2\n```\n\nThen, create a similar `transcribe()` function as before:\n\n```python\nfrom deepspeech import Model\nimport numpy as np\n\nmodel_file_path = \"deepspeech-0.8.2-models.pbmm\"\nlm_file_path = \"deepspeech-0.8.2-models.scorer\"\nbeam_width = 100\nlm_alpha = 0.93\nlm_beta = 1.18\n\nmodel = Model(model_file_path)\nmodel.enableExternalScorer(lm_file_path)\nmodel.setScorerAlphaBeta(lm_alpha, lm_beta)\nmodel.setBeamWidth(beam_width)\n\n\ndef reformat_freq(sr, y):\n    if sr not in (\n        48000,\n        16000,\n    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)\n        raise ValueError(\"Unsupported rate\", sr)\n    if sr == 48000:\n        y = (\n            ((y / max(np.max(y), 1)) * 32767)\n            .reshape((-1, 3))\n            .mean(axis=1)\n            .astype(\"int16\")\n        )\n        sr = 16000\n    return sr, y\n\n\ndef transcribe(speech, stream):\n    _, y = reformat_freq(*speech)\n    if stream is None:\n        stream = model.createStream()\n    stream.feedAudioContent(y)\n    text = stream.intermediateDecode()\n    return text, stream\n\n```\n\nThen, create a Gradio Interface as before (the only difference being that the return type should be `numpy` instead of a `filepath` to be compatible with the DeepSpeech models)\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"numpy\"), \n        \"state\" \n    ], \n    outputs= [\n        \"text\", \n        \"state\"\n    ], \n    live=True).launch()\n```\n\nRunning all of this should allow you to deploy your realtime ASR model with a nice GUI. Try it out and see how well it works for you.\n\n--------------------------------------------\n\n\nAnd you're done! That's all the code you need to build a web-based GUI for your ASR model. \n\nFun tip: you can share your ASR model instantly with others simply by setting `share=True` in `launch()`. \n\n\n", "html": "<h1 id=\"real-time-speech-recognition\">Real Time Speech Recognition</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Automatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).</p>\n\n<p>Using <code>gradio</code>, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.</p>\n\n<p>This tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a <strong><em>full-context</em></strong> model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it <strong><em>streaming</em></strong>, meaning that the audio model will convert speech as you speak. The streaming demo that we create will look something like this (try it below or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/streaming-asr-paused\">in a new tab</a>!):</p>\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Real-time ASR is inherently <em>stateful</em>, meaning that the model's predictions change depending on what words the user previously spoke. So, in this tutorial, we will also cover how to use <strong>state</strong> with Gradio demos. </p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:</p>\n\n<ul>\n<li>Transformers (for this, <code>pip install transformers</code> and <code>pip install torch</code>) </li>\n<li>DeepSpeech (<code>pip install deepspeech==0.8.2</code>)</li>\n</ul>\n\n<p>Make sure you have at least one of these installed so that you can follow along the tutorial. You will also need <code>ffmpeg</code> <a rel=\"noopener\" target=\"_blank\" href=\"https://www.ffmpeg.org/download.html\">installed on your system</a>, if you do not already have it, to process files from the microphone.</p>\n\n<p>Here's how to build a real time speech recognition (ASR) app: </p>\n\n<ol>\n<li><a href=\"#1-set-up-the-transformers-asr-model\">Set up the Transformers ASR Model</a></li>\n<li><a href=\"#2-create-a-full-context-asr-demo-with-transformers\">Create a Full-Context ASR Demo with Transformers</a> </li>\n<li><a href=\"#3-create-a-streaming-asr-demo-with-transformers\">Create a Streaming ASR Demo  with Transformers</a></li>\n<li><a href=\"#4-create-a-streaming-asr-demo-with-deepspeech\">Create a Streaming ASR Demo with DeepSpeech</a></li>\n</ol>\n\n<h2 id=\"1-set-up-the-transformers-asr-model\">1. Set up the Transformers ASR Model</h2>\n\n<p>First, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the Hugging Face model, <code>Wav2Vec2</code>. </p>\n\n<p>Here is the code to load <code>Wav2Vec2</code> from Hugging Face <code>transformers</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import pipeline\n\np = pipeline(\"automatic-speech-recognition\")\n</code></pre></div>\n\n<p>That's it! By default, the automatic speech recognition model pipeline loads Facebook's <code>facebook/wav2vec2-base-960h</code> model.</p>\n\n<h2 id=\"2-create-a-full-context-asr-demo-with-transformers\">2. Create a Full-Context ASR Demo with Transformers</h2>\n\n<p>We will start by creating a <em>full-context</em> ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the <code>pipeline</code> object above.</p>\n\n<p>We will use <code>gradio</code>'s built in <code>Audio</code> component, configured to take input from the user's microphone and return a filepath for the recorded audio. The output component will be a plain <code>Textbox</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef transcribe(audio):\n    text = p(audio)[\"text\"]\n    return text\n\ngr.Interface(\n    fn=transcribe, \n    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n    outputs=\"text\").launch()\n</code></pre></div>\n\n<p>So what's happening here? The <code>transcribe</code> function takes a single parameter, <code>audio</code>, which is a filepath to the audio file that the user has recorded. The <code>pipeline</code> object expects a filepath and converts it to text, which is returned to the frontend and displayed in a textbox. </p>\n\n<p>Let's see it in action! (Record a short audio clip and then click submit, or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/full-context-asr\">open in a new tab</a>):</p>\n\n<iframe src=\"https://abidlabs-full-context-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"3-create-a-streaming-asr-demo-with-transformers\">3. Create a Streaming ASR Demo  with Transformers</h2>\n\n<p>Ok great! We've built an ASR model that works well for short audio clips. However, if you are recording longer audio clips, you probably want a <em>streaming</em> interface, one that transcribes audio as the user speaks instead of just all-at-once at the end.</p>\n\n<p>The good news is that it's not too difficult to adapt the demo we just made to make it streaming, using the same <code>Wav2Vec2</code> model. </p>\n\n<p>The biggest change is that we must now introduce a <code>state</code> parameter, which holds the audio that has been <em>transcribed so far</em>. This allows us to only the latest chunk of audio and simply append it to the audio we previously transcribed. </p>\n\n<p>When adding state to a Gradio demo, you need to do a total of 3 things:</p>\n\n<ul>\n<li>Add a <code>state</code> parameter to the function</li>\n<li>Return the updated <code>state</code> at the end of the function</li>\n<li>Add the <code>\"state\"</code> components to the <code>inputs</code> and <code>outputs</code> in <code>Interface</code> </li>\n</ul>\n\n<p>Here's what the code looks like:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def transcribe(audio, state=\"\"):\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\n# Set the starting state to an empty string\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\" \n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n</code></pre></div>\n\n<p>Notice that we've also made one other change, which is that we've set <code>live=True</code>. This keeps the Gradio interface running constantly, so it automatically transcribes audio without the user having to repeatedly hit the submit button.</p>\n\n<p>Let's see how it does (try below or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/streaming-asr\">in a new tab</a>)!</p>\n\n<iframe src=\"https://abidlabs-streaming-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>One thing that you may notice is that the transcription quality has dropped since the chunks of audio are so small, they lack the context to properly be transcribed. A \"hacky\" fix to this is to simply increase the runtime of the <code>transcribe()</code> function so that longer audio chunks are processed. We can do this by adding a <code>time.sleep()</code> inside the function, as shown below (we'll see a proper fix next) </p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import pipeline\nimport gradio as gr\nimport time\n\np = pipeline(\"automatic-speech-recognition\")\n\ndef transcribe(audio, state=\"\"):\n    time.sleep(2)\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\"\n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n</code></pre></div>\n\n<p>Try the demo below to see the difference (or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/streaming-asr-paused\">open in a new tab</a>)!</p>\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"4-create-a-streaming-asr-demo-with-deepspeech\">4. Create a Streaming ASR Demo with DeepSpeech</h2>\n\n<p>You're not restricted to ASR models from the <code>transformers</code> library -- you can use your own models or models from other libraries. The <code>DeepSpeech</code> library contains models that are specifically designed to handle streaming audio data. These models perform really well with  streaming data as they are able to account for previous chunks of audio data when making predictions.</p>\n\n<p>Going through the DeepSpeech library is beyond the scope of this Guide (check out their <a rel=\"noopener\" target=\"_blank\" href=\"https://deepspeech.readthedocs.io/en/r0.9/\">excellent documentation here</a>), but you can use Gradio very similarly with a DeepSpeech ASR model as with a Transformers ASR model. </p>\n\n<p>Here's a complete example (on Linux):</p>\n\n<p>First install the DeepSpeech library and download the pretrained models from the terminal:</p>\n\n<div class='codeblock'><pre><code class='lang-bash'>wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.pbmm\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.scorer\napt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\npip install deepspeech==0.8.2\n</code></pre></div>\n\n<p>Then, create a similar <code>transcribe()</code> function as before:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from deepspeech import Model\nimport numpy as np\n\nmodel_file_path = \"deepspeech-0.8.2-models.pbmm\"\nlm_file_path = \"deepspeech-0.8.2-models.scorer\"\nbeam_width = 100\nlm_alpha = 0.93\nlm_beta = 1.18\n\nmodel = Model(model_file_path)\nmodel.enableExternalScorer(lm_file_path)\nmodel.setScorerAlphaBeta(lm_alpha, lm_beta)\nmodel.setBeamWidth(beam_width)\n\n\ndef reformat_freq(sr, y):\n    if sr not in (\n        48000,\n        16000,\n    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)\n        raise ValueError(\"Unsupported rate\", sr)\n    if sr == 48000:\n        y = (\n            ((y / max(np.max(y), 1)) * 32767)\n            .reshape((-1, 3))\n            .mean(axis=1)\n            .astype(\"int16\")\n        )\n        sr = 16000\n    return sr, y\n\n\ndef transcribe(speech, stream):\n    _, y = reformat_freq(*speech)\n    if stream is None:\n        stream = model.createStream()\n    stream.feedAudioContent(y)\n    text = stream.intermediateDecode()\n    return text, stream\n\n</code></pre></div>\n\n<p>Then, create a Gradio Interface as before (the only difference being that the return type should be <code>numpy</code> instead of a <code>filepath</code> to be compatible with the DeepSpeech models)</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"numpy\"), \n        \"state\" \n    ], \n    outputs= [\n        \"text\", \n        \"state\"\n    ], \n    live=True).launch()\n</code></pre></div>\n\n<p>Running all of this should allow you to deploy your realtime ASR model with a nice GUI. Try it out and see how well it works for you.</p>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a web-based GUI for your ASR model. </p>\n\n<p>Fun tip: you can share your ASR model instantly with others simply by setting <code>share=True</code> in <code>launch()</code>. </p>\n", "tags": ["ASR", "SPEECH", "STREAMING"], "spaces": ["https://huggingface.co/spaces/abidlabs/streaming-asr-paused", "https://huggingface.co/spaces/abidlabs/full-context-asr"], "url": "/guides/real-time-speech-recognition/", "contributor": null}], "preprocessing": "passes the uploaded audio as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >Tuple(int, numpy.array)</span> corresponding to (sample rate, data) or as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath, depending on `type`", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >Tuple(int, numpy.array)</span> corresponding to (sample rate, data) or as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath or URL to an audio file, which gets displayed", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath to a local file that contains audio.", "parent": "gradio", "prev_obj": "Components", "next_obj": "BarPlot"}, "barplot": {"class": null, "name": "BarPlot", "description": "Create a bar plot. <br> <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects a pandas dataframe with the data to plot.", "demos": "native_plots, chicago-bikeshare-dashboard"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "pd.DataFrame | Callable | None", "doc": "The pandas dataframe containing the data to display in a scatter plot.", "default": "None"}, {"name": "x", "annotation": "str | None", "doc": "Column corresponding to the x axis.", "default": "None"}, {"name": "y", "annotation": "str | None", "doc": "Column corresponding to the y axis.", "default": "None"}, {"name": "color", "annotation": "str | None", "doc": "The column to determine the bar color. Must be categorical (discrete values).", "default": "None"}, {"name": "vertical", "annotation": "bool", "doc": "If True, the bars will be displayed vertically. If False, the x and y axis will be switched, displaying the bars horizontally. Default is True.", "default": "True"}, {"name": "group", "annotation": "str | None", "doc": "The column with which to split the overall plot into smaller subplots.", "default": "None"}, {"name": "title", "annotation": "str | None", "doc": "The title to display on top of the chart.", "default": "None"}, {"name": "tooltip", "annotation": "List[str] | str | None", "doc": "The column (or list of columns) to display on the tooltip when a user hovers over a bar.", "default": "None"}, {"name": "x_title", "annotation": "str | None", "doc": "The title given to the x axis. By default, uses the value of the x parameter.", "default": "None"}, {"name": "y_title", "annotation": "str | None", "doc": "The title given to the y axis. By default, uses the value of the y parameter.", "default": "None"}, {"name": "color_legend_title", "annotation": "str | None", "doc": "The title given to the color legend. By default, uses the value of color parameter.", "default": "None"}, {"name": "group_title", "annotation": "str | None", "doc": "The label displayed on top of the subplot columns (or rows if vertical=True). Use an empty string to omit.", "default": "None"}, {"name": "color_legend_position", "annotation": "str | None", "doc": "The position of the color legend. If the string value 'none' is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.", "default": "None"}, {"name": "height", "annotation": "int | None", "doc": "The height of the plot in pixels.", "default": "None"}, {"name": "width", "annotation": "int | None", "doc": "The width of the plot in pixels.", "default": "None"}, {"name": "y_lim", "annotation": "List[int] | None", "doc": "A tuple of list containing the limits for the y-axis, specified as [y_min, y_max].", "default": "None"}, {"name": "caption", "annotation": "str | None", "doc": "The (optional) caption to display below the plot.", "default": "None"}, {"name": "interactive", "annotation": "bool | None", "doc": "Whether users should be able to interact with the plot by panning or zooming with their mouse or trackpad.", "default": "True"}, {"name": "label", "annotation": "str | None", "doc": "The (optional) label to display on the top left corner of the plot.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "Whether the label should be displayed.", "default": "True"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "Whether the plot should be visible.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "Unique id used for custom css targetting.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.BarPlot"}, {"fn": null, "name": "clear", "description": "This event is triggered when the user clears the component (e.g. image or audio) using the X button for the component. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.BarPlot"}], "string_shortcuts": [["BarPlot", "barplot", "Uses default values"]], "demos": [["native_plots", "import gradio as gr\n\nfrom scatter_plot_demo import scatter_plot\nfrom line_plot_demo import line_plot\nfrom bar_plot_demo import bar_plot\n\n\nwith gr.Blocks() as demo:\n    with gr.Tabs():\n        with gr.TabItem(\"Scatter Plot\"):\n            scatter_plot.render()\n        with gr.TabItem(\"Line Plot\"):\n            line_plot.render()\n        with gr.TabItem(\"Bar Plot\"):\n            bar_plot.render()\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["chicago-bikeshare-dashboard", "import os\nimport gradio as gr\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nmatplotlib.use(\"Agg\")\n\nDB_USER = os.getenv(\"DB_USER\")\nDB_PASSWORD = os.getenv(\"DB_PASSWORD\")\nDB_HOST = os.getenv(\"DB_HOST\")\nPORT = 8080\nDB_NAME = \"bikeshare\"\n\nconnection_string = (\n    f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}?port={PORT}&dbname={DB_NAME}\"\n)\n\n\ndef get_count_ride_type():\n    df = pd.read_sql(\n        \"\"\"\n        SELECT COUNT(ride_id) as n, rideable_type\n        FROM rides\n        GROUP BY rideable_type\n        ORDER BY n DESC\n    \"\"\",\n        con=connection_string,\n    )\n    return df\n\n\ndef get_most_popular_stations():\n\n    df = pd.read_sql(\n        \"\"\"\n    SELECT COUNT(ride_id) as n, MAX(start_station_name) as station\n    FROM RIDES\n    WHERE start_station_name is NOT NULL\n    GROUP BY start_station_id\n    ORDER BY n DESC\n    LIMIT 5\n    \"\"\",\n        con=connection_string,\n    )\n    return df\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n        \"\"\"\n    # Chicago Bike Share Dashboard\n    \n    This demo pulls Chicago bike share data for March 2022 from a postgresql database hosted on AWS.\n    This demo uses psycopg2 but any postgresql client library (SQLAlchemy)\n    is compatible with gradio.\n    \n    Connection credentials are handled by environment variables\n    defined as secrets in the Space.\n\n    If data were added to the database, the plots in this demo would update\n    whenever the webpage is reloaded.\n    \n    This demo serves as a starting point for your database-connected apps!\n    \"\"\"\n    )\n    with gr.Row():\n        bike_type = gr.BarPlot(\n            x=\"rideable_type\",\n            y='n',\n            title=\"Number of rides per bicycle type\",\n            y_title=\"Number of Rides\",\n            x_title=\"Bicycle Type\",\n            vertical=False,\n            tooltip=['rideable_type', \"n\"],\n            height=300,\n            width=300,\n        )\n        station = gr.BarPlot(\n            x='station',\n            y='n',\n            title=\"Most Popular Stations\",\n            y_title=\"Number of Rides\",\n            x_title=\"Station Name\",\n            vertical=False,\n            tooltip=['station', 'n'],\n            height=300,\n            width=300\n        )\n\n    demo.load(get_count_ride_type, inputs=None, outputs=bike_type)\n    demo.load(get_most_popular_stations, inputs=None, outputs=station)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()", "clear()"], "events": "change(), clear()", "preprocessing": "this component does *not* accept input.", "postprocessing": "expects a pandas dataframe with the data to plot.", "parent": "gradio", "prev_obj": "Audio", "next_obj": "Button"}, "button": {"class": null, "name": "Button", "description": "Used to create a button, that can be assigned arbitrary click() events. The label (value) of the button can be used as an input or set via the output of a function. <br>", "tags": {"preprocessing": "passes the button value as a {str} into the function", "postprocessing": "expects a {str} to be returned from a function, which is set as the label of the button", "demos": "blocks_inputs, blocks_kinematics"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | Callable", "doc": "Default text for the button to display. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "\"Run\""}, {"name": "variant", "annotation": "str", "doc": "'primary' for main call-to-action, 'secondary' for a more subdued style", "default": "\"secondary\""}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "click", "description": "This event is triggered when the component (e.g. a button) is clicked. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "<class 'inspect._empty'>", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Button"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the button component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "full_width", "annotation": "bool | None", "doc": "If True, will expand to fill parent container.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Button"}], "string_shortcuts": [["Button", "button", "Uses default values"]], "demos": [["blocks_inputs", "import gradio as gr\nimport os\n\n\ndef combine(a, b):\n    return a + \" \" + b\n\n\ndef mirror(x):\n    return x\n\n\nwith gr.Blocks() as demo:\n\n    txt = gr.Textbox(label=\"Input\", lines=2)\n    txt_2 = gr.Textbox(label=\"Input 2\")\n    txt_3 = gr.Textbox(value=\"\", label=\"Output\")\n    btn = gr.Button(value=\"Submit\")\n    btn.click(combine, inputs=[txt, txt_2], outputs=[txt_3])\n\n    with gr.Row():\n        im = gr.Image()\n        im_2 = gr.Image()\n\n    btn = gr.Button(value=\"Mirror Image\")\n    btn.click(mirror, inputs=[im], outputs=[im_2])\n\n    gr.Markdown(\"## Text Examples\")\n    gr.Examples(\n        [[\"hi\", \"Adam\"], [\"hello\", \"Eve\"]],\n        [txt, txt_2],\n        txt_3,\n        combine,\n        cache_examples=True,\n    )\n    gr.Markdown(\"## Image Examples\")\n    gr.Examples(\n        examples=[os.path.join(os.path.dirname(__file__), \"lion.jpg\")],\n        inputs=im,\n        outputs=im_2,\n        fn=mirror,\n        cache_examples=True,\n    )\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["blocks_kinematics", "import pandas as pd\nimport numpy as np\n\nimport gradio as gr\n\n\ndef plot(v, a):\n    g = 9.81\n    theta = a / 180 * 3.14\n    tmax = ((2 * v) * np.sin(theta)) / g\n    timemat = tmax * np.linspace(0, 1, 40)\n\n    x = (v * timemat) * np.cos(theta)\n    y = ((v * timemat) * np.sin(theta)) - ((0.5 * g) * (timemat**2))\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n    return df\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n        r\"Let's do some kinematics! Choose the speed and angle to see the trajectory. Remember that the range $R = v_0^2 \\cdot \\frac{\\sin(2\\theta)}{g}$\"\n    )\n\n    with gr.Row():\n        speed = gr.Slider(1, 30, 25, label=\"Speed\")\n        angle = gr.Slider(0, 90, 45, label=\"Angle\")\n    output = gr.LinePlot(\n        x=\"x\",\n        y=\"y\",\n        overlay_point=True,\n        tooltip=[\"x\", \"y\"],\n        x_lim=[0, 100],\n        y_lim=[0, 60],\n        width=350,\n        height=300,\n    )\n    btn = gr.Button(value=\"Run\")\n    btn.click(plot, [speed, angle], output)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["click()"], "events": "click()", "preprocessing": "passes the button value as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> into the function", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> to be returned from a function, which is set as the label of the button", "parent": "gradio", "prev_obj": "BarPlot", "next_obj": "Chatbot"}, "chatbot": {"class": null, "name": "Chatbot", "description": "Displays a chatbot output showing both user submitted messages and responses. Supports a subset of Markdown including bold, italics, code, and images. <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects function to return a {List[Tuple[str | None, str | None]]}, a list of tuples with user inputs and responses as strings of HTML or Nones. Messages that are `None` are not displayed.", "demos": "chatbot_demo, chatbot_multimodal"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "List[Tuple[str | None, str | None]] | Callable | None", "doc": "Default value to show in chatbot. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "color_map", "annotation": "Dict[str, str] | None", "doc": null, "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Chatbot"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the Chatbot component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "color_map", "annotation": "Tuple[str, str] | None", "doc": "Tuple containing colors to apply to user and response chat bubbles.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Chatbot"}], "string_shortcuts": [["Chatbot", "chatbot", "Uses default values"]], "demos": [["chatbot_demo", "import gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n\ndef predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response \n    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n    return response, history\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    state = gr.State([])\n\n    with gr.Row():\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n\n    txt.submit(predict, [txt, state], [chatbot, state])\n            \nif __name__ == \"__main__\":\n    demo.launch()\n"], ["chatbot_multimodal", "import gradio as gr\n\ndef add_text(state, text):\n    state = state + [(text, text + \"?\")]\n    return state, state\n\ndef add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n\n\nwith gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n    state = gr.State([])\n    \n    with gr.Row():\n        with gr.Column(scale=0.85):\n            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\n        with gr.Column(scale=0.15, min_width=0):\n            btn = gr.UploadButton(\"\ud83d\uddbc\ufe0f\", file_types=[\"image\"])\n            \n    txt.submit(add_text, [state, txt], [state, chatbot])\n    txt.submit(lambda :\"\", None, txt)\n    btn.upload(add_image, [state, btn], [state, chatbot])\n            \nif __name__ == \"__main__\":\n    demo.launch()"]], "events_list": ["change()"], "events": "change()", "preprocessing": "this component does *not* accept input.", "postprocessing": "expects function to return a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[Tuple[str | None, str | None]]</span>, a list of tuples with user inputs and responses as strings of HTML or Nones. Messages that are `None` are not displayed.", "parent": "gradio", "prev_obj": "Button", "next_obj": "Checkbox"}, "checkbox": {"class": null, "name": "Checkbox", "description": "Creates a checkbox that can be set to `True` or `False`. <br>", "tags": {"preprocessing": "passes the status of the checkbox as a {bool} into the function.", "postprocessing": "expects a {bool} returned from the function and, if it is True, checks the checkbox.", "examples-format": "a {bool} representing whether the box is checked.", "demos": "sentence_builder, titanic_survival"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "bool | Callable", "doc": "if True, checked by default. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "False"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, this checkbox can be checked; if False, checking will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Checkbox"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place the component in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Checkbox"}], "string_shortcuts": [["Checkbox", "checkbox", "Uses default values"]], "demos": [["sentence_builder", "import gradio as gr\n\n\ndef sentence_builder(quantity, animal, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4),\n        gr.Dropdown([\"cat\", \"dog\", \"bird\"]),\n        gr.Radio([\"park\", \"zoo\", \"road\"]),\n        gr.Dropdown([\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True),\n        gr.Checkbox(label=\"Is it the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", \"park\", [\"ran\", \"swam\"], True],\n        [4, \"dog\", \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", \"road\", [\"ran\"], False],\n        [8, \"cat\", \"zoo\", [\"ate\"], True],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["titanic_survival", "import os\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nimport gradio as gr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata = pd.read_csv(os.path.join(current_dir, \"files/titanic.csv\"))\n\n\ndef encode_age(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    categories = pd.cut(df.Age, bins, labels=False)\n    df.Age = categories\n    return df\n\n\ndef encode_fare(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    categories = pd.cut(df.Fare, bins, labels=False)\n    df.Fare = categories\n    return df\n\n\ndef encode_df(df):\n    df = encode_age(df)\n    df = encode_fare(df)\n    sex_mapping = {\"male\": 0, \"female\": 1}\n    df = df.replace({\"Sex\": sex_mapping})\n    embark_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n    df = df.replace({\"Embarked\": embark_mapping})\n    df.Embarked = df.Embarked.fillna(0)\n    df[\"Company\"] = 0\n    df.loc[(df[\"SibSp\"] > 0), \"Company\"] = 1\n    df.loc[(df[\"Parch\"] > 0), \"Company\"] = 2\n    df.loc[(df[\"SibSp\"] > 0) & (df[\"Parch\"] > 0), \"Company\"] = 3\n    df = df[\n        [\n            \"PassengerId\",\n            \"Pclass\",\n            \"Sex\",\n            \"Age\",\n            \"Fare\",\n            \"Embarked\",\n            \"Company\",\n            \"Survived\",\n        ]\n    ]\n    return df\n\n\ntrain = encode_df(data)\n\nX_all = train.drop([\"Survived\", \"PassengerId\"], axis=1)\ny_all = train[\"Survived\"]\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=num_test, random_state=23\n)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\n\ndef predict_survival(passenger_class, is_male, age, company, fare, embark_point):\n    if passenger_class is None or embark_point is None:\n        return None\n    df = pd.DataFrame.from_dict(\n        {\n            \"Pclass\": [passenger_class + 1],\n            \"Sex\": [0 if is_male else 1],\n            \"Age\": [age],\n            \"Company\": [\n                (1 if \"Sibling\" in company else 0) + (2 if \"Child\" in company else 0)\n            ],\n            \"Fare\": [fare],\n            \"Embarked\": [embark_point + 1],\n        }\n    )\n    df = encode_age(df)\n    df = encode_fare(df)\n    pred = clf.predict_proba(df)[0]\n    return {\"Perishes\": float(pred[0]), \"Survives\": float(pred[1])}\n\n\ndemo = gr.Interface(\n    predict_survival,\n    [\n        gr.Dropdown([\"first\", \"second\", \"third\"], type=\"index\"),\n        \"checkbox\",\n        gr.Slider(0, 80, value=25),\n        gr.CheckboxGroup([\"Sibling\", \"Child\"], label=\"Travelling with (select all)\"),\n        gr.Number(value=20),\n        gr.Radio([\"S\", \"C\", \"Q\"], type=\"index\"),\n    ],\n    \"label\",\n    examples=[\n        [\"first\", True, 30, [], 50, \"S\"],\n        [\"second\", False, 40, [\"Sibling\", \"Child\"], 10, \"Q\"],\n        [\"third\", True, 30, [\"Child\"], 20, \"S\"],\n    ],\n    interpretation=\"default\",\n    live=True,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()"], "events": "change()", "preprocessing": "passes the status of the checkbox as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >bool</span> into the function.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >bool</span> returned from the function and, if it is True, checks the checkbox.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >bool</span> representing whether the box is checked.", "parent": "gradio", "prev_obj": "Chatbot", "next_obj": "CheckboxGroup"}, "checkboxgroup": {"class": null, "name": "CheckboxGroup", "description": "Creates a set of checkboxes of which a subset can be checked.", "tags": {"preprocessing": "passes the list of checked checkboxes as a {List[str]} or their indices as a {List[int]} into the function, depending on `type`.", "postprocessing": "expects a {List[str]}, each element of which becomes a checked checkbox.", "examples-format": "a {List[str]} representing the values to be checked.", "demos": "sentence_builder, titanic_survival"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "choices", "annotation": "List[str] | None", "doc": "list of options to select from.", "default": "None"}, {"name": "value", "annotation": "List[str] | str | Callable | None", "doc": "default selected list of options. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "type", "annotation": "str", "doc": "Type of value to be returned by component. \"value\" returns the list of strings of the choices selected, \"index\" returns the list of indicies of the choices selected.", "default": "\"value\""}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, choices in this checkbox group will be checkable; if False, checking will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.CheckboxGroup"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the CheckboxGroup.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "item_container", "annotation": "bool | None", "doc": "If True, will place the items in a container.", "default": "None"}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place the component in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.CheckboxGroup"}], "string_shortcuts": [["CheckboxGroup", "checkboxgroup", "Uses default values"]], "demos": [["sentence_builder", "import gradio as gr\n\n\ndef sentence_builder(quantity, animal, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4),\n        gr.Dropdown([\"cat\", \"dog\", \"bird\"]),\n        gr.Radio([\"park\", \"zoo\", \"road\"]),\n        gr.Dropdown([\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True),\n        gr.Checkbox(label=\"Is it the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", \"park\", [\"ran\", \"swam\"], True],\n        [4, \"dog\", \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", \"road\", [\"ran\"], False],\n        [8, \"cat\", \"zoo\", [\"ate\"], True],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["titanic_survival", "import os\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nimport gradio as gr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata = pd.read_csv(os.path.join(current_dir, \"files/titanic.csv\"))\n\n\ndef encode_age(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    categories = pd.cut(df.Age, bins, labels=False)\n    df.Age = categories\n    return df\n\n\ndef encode_fare(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    categories = pd.cut(df.Fare, bins, labels=False)\n    df.Fare = categories\n    return df\n\n\ndef encode_df(df):\n    df = encode_age(df)\n    df = encode_fare(df)\n    sex_mapping = {\"male\": 0, \"female\": 1}\n    df = df.replace({\"Sex\": sex_mapping})\n    embark_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n    df = df.replace({\"Embarked\": embark_mapping})\n    df.Embarked = df.Embarked.fillna(0)\n    df[\"Company\"] = 0\n    df.loc[(df[\"SibSp\"] > 0), \"Company\"] = 1\n    df.loc[(df[\"Parch\"] > 0), \"Company\"] = 2\n    df.loc[(df[\"SibSp\"] > 0) & (df[\"Parch\"] > 0), \"Company\"] = 3\n    df = df[\n        [\n            \"PassengerId\",\n            \"Pclass\",\n            \"Sex\",\n            \"Age\",\n            \"Fare\",\n            \"Embarked\",\n            \"Company\",\n            \"Survived\",\n        ]\n    ]\n    return df\n\n\ntrain = encode_df(data)\n\nX_all = train.drop([\"Survived\", \"PassengerId\"], axis=1)\ny_all = train[\"Survived\"]\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=num_test, random_state=23\n)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\n\ndef predict_survival(passenger_class, is_male, age, company, fare, embark_point):\n    if passenger_class is None or embark_point is None:\n        return None\n    df = pd.DataFrame.from_dict(\n        {\n            \"Pclass\": [passenger_class + 1],\n            \"Sex\": [0 if is_male else 1],\n            \"Age\": [age],\n            \"Company\": [\n                (1 if \"Sibling\" in company else 0) + (2 if \"Child\" in company else 0)\n            ],\n            \"Fare\": [fare],\n            \"Embarked\": [embark_point + 1],\n        }\n    )\n    df = encode_age(df)\n    df = encode_fare(df)\n    pred = clf.predict_proba(df)[0]\n    return {\"Perishes\": float(pred[0]), \"Survives\": float(pred[1])}\n\n\ndemo = gr.Interface(\n    predict_survival,\n    [\n        gr.Dropdown([\"first\", \"second\", \"third\"], type=\"index\"),\n        \"checkbox\",\n        gr.Slider(0, 80, value=25),\n        gr.CheckboxGroup([\"Sibling\", \"Child\"], label=\"Travelling with (select all)\"),\n        gr.Number(value=20),\n        gr.Radio([\"S\", \"C\", \"Q\"], type=\"index\"),\n    ],\n    \"label\",\n    examples=[\n        [\"first\", True, 30, [], 50, \"S\"],\n        [\"second\", False, 40, [\"Sibling\", \"Child\"], 10, \"Q\"],\n        [\"third\", True, 30, [\"Child\"], 20, \"S\"],\n    ],\n    interpretation=\"default\",\n    live=True,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()"], "events": "change()", "preprocessing": "passes the list of checked checkboxes as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[str]</span> or their indices as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[int]</span> into the function, depending on `type`.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[str]</span>, each element of which becomes a checked checkbox.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[str]</span> representing the values to be checked.", "parent": "gradio", "prev_obj": "Checkbox", "next_obj": "ColorPicker"}, "colorpicker": {"class": null, "name": "ColorPicker", "description": "Creates a color picker for user to select a color as string input.", "tags": {"preprocessing": "passes selected color value as a {str} into the function.", "postprocessing": "expects a {str} returned from function and sets color picker value to it.", "examples-format": "a {str} with a hexadecimal representation of a color, e.g. \"#ff0000\" for red.", "demos": "color_picker, color_generator"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | Callable | None", "doc": "default text to provide in color picker. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, will be rendered as an editable color picker; if False, editing will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.ColorPicker"}, {"fn": null, "name": "submit", "description": "This event is triggered when the user presses the Enter key while the component (e.g. a textbox) is focused. This method can be used when this component is in a Gradio Blocks. <br> <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.ColorPicker"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place the component in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.ColorPicker"}], "string_shortcuts": [["ColorPicker", "colorpicker", "Uses default values"]], "demos": [["color_picker", "import gradio as gr\nimport numpy as np\nimport os\nfrom PIL import Image, ImageColor\n\n\ndef change_color(icon, color):\n\n    \"\"\"\n    Function that given an icon in .png format changes its color\n    Args:\n        icon: Icon whose color needs to be changed.\n        color: Chosen color with which to edit the input icon.\n    Returns:\n        edited_image: Edited icon.\n    \"\"\"\n    img = icon.convert(\"LA\")\n    img = img.convert(\"RGBA\")\n    image_np = np.array(icon)\n    _, _, _, alpha = image_np.T\n    mask = alpha > 0\n    image_np[..., :-1][mask.T] = ImageColor.getcolor(color, \"RGB\")\n    edited_image = Image.fromarray(image_np)\n    return edited_image\n\n\ninputs = [\n    gr.Image(label=\"icon\", type=\"pil\", image_mode=\"RGBA\"),\n    gr.ColorPicker(label=\"color\"),\n]\noutputs = gr.Image(label=\"colored icon\")\n\ndemo = gr.Interface(\n    fn=change_color,\n    inputs=inputs,\n    outputs=outputs,\n    examples=[\n        [os.path.join(os.path.dirname(__file__), \"rabbit.png\"), \"#ff0000\"],\n        [os.path.join(os.path.dirname(__file__), \"rabbit.png\"), \"#0000FF\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["color_generator", "import gradio as gr\nimport cv2\nimport numpy as np\nimport random\n\n\n# Convert decimal color to hexadecimal color\ndef RGB_to_Hex(rgb):\n    color = \"#\"\n    for i in rgb:\n        num = int(i)\n        color += str(hex(num))[-2:].replace(\"x\", \"0\").upper()\n    return color\n\n\n# Randomly generate light or dark colors\ndef random_color(is_light=True):\n    return (\n        random.randint(0, 127) + int(is_light) * 128,\n        random.randint(0, 127) + int(is_light) * 128,\n        random.randint(0, 127) + int(is_light) * 128,\n    )\n\n\ndef switch_color(color_style):\n    if color_style == \"light\":\n        is_light = True\n    elif color_style == \"dark\":\n        is_light = False\n    back_color_ = random_color(is_light)  # Randomly generate colors\n    back_color = RGB_to_Hex(back_color_)  # Convert to hexadecimal\n\n    # Draw color pictures.\n    w, h = 50, 50\n    img = np.zeros((h, w, 3), np.uint8)\n    cv2.rectangle(img, (0, 0), (w, h), back_color_, thickness=-1)\n\n    return back_color, back_color, img\n\n\ninputs = [gr.Radio([\"light\", \"dark\"], value=\"light\")]\n\noutputs = [\n    gr.ColorPicker(label=\"color\"),\n    gr.Textbox(label=\"hexadecimal color\"),\n    gr.Image(type=\"numpy\", label=\"color picture\"),\n]\n\ntitle = \"Color Generator\"\ndescription = (\n    \"Click the Submit button, and a dark or light color will be randomly generated.\"\n)\n\ndemo = gr.Interface(\n    fn=switch_color,\n    inputs=inputs,\n    outputs=outputs,\n    title=title,\n    description=description,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()", "submit()"], "events": "change(), submit()", "preprocessing": "passes selected color value as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> into the function.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> returned from function and sets color picker value to it.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> with a hexadecimal representation of a color, e.g. \"#ff0000\" for red.", "parent": "gradio", "prev_obj": "CheckboxGroup", "next_obj": "Dataframe"}, "dataframe": {"class": null, "name": "Dataframe", "description": "Accepts or displays 2D input through a spreadsheet-like component for dataframes.", "tags": {"preprocessing": "passes the uploaded spreadsheet data as a {pandas.DataFrame}, {numpy.array}, {List[List]}, or {List} depending on `type`", "postprocessing": "expects a {pandas.DataFrame}, {numpy.array}, {List[List]}, {List}, a {Dict} with keys `data` (and optionally `headers`), or {str} path to a csv, which is rendered in the spreadsheet.", "examples-format": "a {str} filepath to a csv with data, a pandas dataframe, or a list of lists (excluding headers) where each sublist is a row of data.", "demos": "filter_records, matrix_transpose, tax_calculator"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "List[List[Any]] | Callable | None", "doc": "Default value as a 2-dimensional list of values. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "headers", "annotation": "List[str] | None", "doc": "List of str header names. If None, no headers are shown.", "default": "None"}, {"name": "row_count", "annotation": "int | Tuple[int, str]", "doc": "Limit number of rows for input and decide whether user can create new rows. The first element of the tuple is an `int`, the row count; the second should be 'fixed' or 'dynamic', the new row behaviour. If an `int` is passed the rows default to 'dynamic'", "default": "(1, 'dynamic')"}, {"name": "col_count", "annotation": "int | Tuple[int, str] | None", "doc": "Limit number of columns for input and decide whether user can create new columns. The first element of the tuple is an `int`, the number of columns; the second should be 'fixed' or 'dynamic', the new column behaviour. If an `int` is passed the columns default to 'dynamic'", "default": "None"}, {"name": "datatype", "annotation": "str | List[str]", "doc": "Datatype of values in sheet. Can be provided per column as a list of strings, or for the entire sheet as a single string. Valid datatypes are \"str\", \"number\", \"bool\", \"date\", and \"markdown\".", "default": "\"str\""}, {"name": "type", "annotation": "str", "doc": "Type of value to be returned by component. \"pandas\" for pandas dataframe, \"numpy\" for numpy array, or \"array\" for a Python array.", "default": "\"pandas\""}, {"name": "max_rows", "annotation": "int | None", "doc": "Maximum number of rows to display at once. Set to None for infinite.", "default": "20"}, {"name": "max_cols", "annotation": "int | None", "doc": "Maximum number of columns to display at once. Set to None for infinite.", "default": "None"}, {"name": "overflow_row_behaviour", "annotation": "str", "doc": "If set to \"paginate\", will create pages for overflow rows. If set to \"show_ends\", will show initial and final rows and truncate middle rows.", "default": "\"paginate\""}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, will allow users to edit the dataframe; if False, can only be used to display data. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}, {"name": "wrap", "annotation": "bool", "doc": "if True text in table cells will wrap when appropriate, if False the table will scroll horiztonally. Defaults to False.", "default": "False"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Dataframe"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the DataFrame component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}], "returns": {}, "example": null, "parent": "gradio.Dataframe"}], "string_shortcuts": [["Dataframe", "dataframe", "Uses default values"], ["Numpy", "numpy", "Uses type=\"numpy\""], ["Matrix", "matrix", "Uses type=\"array\""], ["List", "list", "Uses type=\"array\", col_count=1"]], "demos": [["filter_records", "import gradio as gr\n\n\ndef filter_records(records, gender):\n    return records[records[\"gender\"] == gender]\n\n\ndemo = gr.Interface(\n    filter_records,\n    [\n        gr.Dataframe(\n            headers=[\"name\", \"age\", \"gender\"],\n            datatype=[\"str\", \"number\", \"str\"],\n            row_count=5,\n            col_count=(3, \"fixed\"),\n        ),\n        gr.Dropdown([\"M\", \"F\", \"O\"]),\n    ],\n    \"dataframe\",\n    description=\"Enter gender as 'M', 'F', or 'O' for other.\",\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["matrix_transpose", "import numpy as np\n\nimport gradio as gr\n\n\ndef transpose(matrix):\n    return matrix.T\n\n\ndemo = gr.Interface(\n    transpose,\n    gr.Dataframe(type=\"numpy\", datatype=\"number\", row_count=5, col_count=3),\n    \"numpy\",\n    examples=[\n        [np.zeros((3, 3)).tolist()],\n        [np.ones((2, 2)).tolist()],\n        [np.random.randint(0, 10, (3, 10)).tolist()],\n        [np.random.randint(0, 10, (10, 3)).tolist()],\n        [np.random.randint(0, 10, (10, 10)).tolist()],\n    ],\n    cache_examples=False\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["tax_calculator", "import gradio as gr\n\ndef tax_calculator(income, marital_status, assets):\n    tax_brackets = [(10, 0), (25, 8), (60, 12), (120, 20), (250, 30)]\n    total_deductible = sum(assets[\"Cost\"])\n    taxable_income = income - total_deductible\n\n    total_tax = 0\n    for bracket, rate in tax_brackets:\n        if taxable_income > bracket:\n            total_tax += (taxable_income - bracket) * rate / 100\n\n    if marital_status == \"Married\":\n        total_tax *= 0.75\n    elif marital_status == \"Divorced\":\n        total_tax *= 0.8\n\n    return round(total_tax)\n\ndemo = gr.Interface(\n    tax_calculator,\n    [\n        \"number\",\n        gr.Radio([\"Single\", \"Married\", \"Divorced\"]),\n        gr.Dataframe(\n            headers=[\"Item\", \"Cost\"],\n            datatype=[\"str\", \"number\"],\n            label=\"Assets Purchased this Year\",\n        ),\n    ],\n    \"number\",\n    examples=[\n        [10000, \"Married\", [[\"Suit\", 5000], [\"Laptop\", 800], [\"Car\", 1800]]],\n        [80000, \"Single\", [[\"Suit\", 800], [\"Watch\", 1800], [\"Car\", 800]]],\n    ],\n)\n\ndemo.launch()\n"]], "events_list": ["change()"], "events": "change()", "preprocessing": "passes the uploaded spreadsheet data as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >pandas.DataFrame</span>, <span class='text-orange-500' style='font-family: monospace; font-size: large;' >numpy.array</span>, <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[List]</span>, or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List</span> depending on `type`", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >pandas.DataFrame</span>, <span class='text-orange-500' style='font-family: monospace; font-size: large;' >numpy.array</span>, <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[List]</span>, <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List</span>, a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >Dict</span> with keys `data` (and optionally `headers`), or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> path to a csv, which is rendered in the spreadsheet.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath to a csv with data, a pandas dataframe, or a list of lists (excluding headers) where each sublist is a row of data.", "parent": "gradio", "prev_obj": "ColorPicker", "next_obj": "Dataset"}, "dataset": {"class": null, "name": "Dataset", "description": "Used to create an output widget for showing datasets. Used to render the examples box.", "tags": {"preprocessing": "passes the selected sample either as a {list} of data (if type=\"value\") or as an {int} index (if type=\"index\")", "postprocessing": "expects a {list} of {lists} corresponding to the dataset data."}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "label", "annotation": "str | None", "doc": null, "default": "None"}, {"name": "components", "annotation": "List[IOComponent] | List[str]", "doc": "Which component types to show in this dataset widget, can be passed in as a list of string names or Components instances. The following components are supported in a Dataset: Audio, Checkbox, CheckboxGroup, ColorPicker, Dataframe, Dropdown, File, HTML, Image, Markdown, Model3D, Number, Radio, Slider, Textbox, TimeSeries, Video"}, {"name": "samples", "annotation": "List[List[Any]] | None", "doc": "a nested list of samples. Each sublist within the outer list represents a data sample, and each element within the sublist represents an value for each component", "default": "None"}, {"name": "headers", "annotation": "List[str] | None", "doc": "Column headers in the Dataset widget, should be the same len as components. If not provided, inferred from component labels", "default": "None"}, {"name": "type", "annotation": "str", "doc": "'values' if clicking on a sample should pass the value of the sample, or \"index\" if it should pass the index of the sample", "default": "\"values\""}, {"name": "samples_per_page", "annotation": "int", "doc": "how many examples to show per page.", "default": "10"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "click", "description": "This event is triggered when the component (e.g. a button) is clicked. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "<class 'inspect._empty'>", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Dataset"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the Dataset component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}], "returns": {}, "example": null, "parent": "gradio.Dataset"}], "string_shortcuts": [["Dataset", "dataset", "Uses default values"]], "events_list": ["click()"], "events": "click()", "preprocessing": "passes the selected sample either as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >list</span> of data (if type=\"value\") or as an <span class='text-orange-500' style='font-family: monospace; font-size: large;' >int</span> index (if type=\"index\")", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >list</span> of <span class='text-orange-500' style='font-family: monospace; font-size: large;' >lists</span> corresponding to the dataset data.", "override_signature": "gr.Dataset(components, samples)", "parent": "gradio", "prev_obj": "Dataframe", "next_obj": "Dropdown"}, "dropdown": {"class": null, "name": "Dropdown", "description": "Creates a dropdown of choices from which entries can be selected.", "tags": {"preprocessing": "passes the value of the selected dropdown entry as a {str} or its index as an {int} into the function, depending on `type`.", "postprocessing": "expects a {str} corresponding to the value of the dropdown entry to be selected.", "examples-format": "a {str} representing the drop down value to select.", "demos": "sentence_builder, titanic_survival"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "choices", "annotation": "str | List[str] | None", "doc": "list of options to select from.", "default": "None"}, {"name": "value", "annotation": "str | List[str] | Callable | None", "doc": "default value(s) selected in dropdown. If None, no value is selected by default. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "type", "annotation": "str", "doc": "Type of value to be returned by component. \"value\" returns the string of the choice selected, \"index\" returns the index of the choice selected.", "default": "\"value\""}, {"name": "multiselect", "annotation": "bool | None", "doc": "if True, multiple choices can be selected.", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, choices in this dropdown will be selectable; if False, selection will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Dropdown"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the Dropdown.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place the component in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Dropdown"}], "string_shortcuts": [["Dropdown", "dropdown", "Uses default values"]], "demos": [["sentence_builder", "import gradio as gr\n\n\ndef sentence_builder(quantity, animal, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4),\n        gr.Dropdown([\"cat\", \"dog\", \"bird\"]),\n        gr.Radio([\"park\", \"zoo\", \"road\"]),\n        gr.Dropdown([\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True),\n        gr.Checkbox(label=\"Is it the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", \"park\", [\"ran\", \"swam\"], True],\n        [4, \"dog\", \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", \"road\", [\"ran\"], False],\n        [8, \"cat\", \"zoo\", [\"ate\"], True],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["titanic_survival", "import os\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nimport gradio as gr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata = pd.read_csv(os.path.join(current_dir, \"files/titanic.csv\"))\n\n\ndef encode_age(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    categories = pd.cut(df.Age, bins, labels=False)\n    df.Age = categories\n    return df\n\n\ndef encode_fare(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    categories = pd.cut(df.Fare, bins, labels=False)\n    df.Fare = categories\n    return df\n\n\ndef encode_df(df):\n    df = encode_age(df)\n    df = encode_fare(df)\n    sex_mapping = {\"male\": 0, \"female\": 1}\n    df = df.replace({\"Sex\": sex_mapping})\n    embark_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n    df = df.replace({\"Embarked\": embark_mapping})\n    df.Embarked = df.Embarked.fillna(0)\n    df[\"Company\"] = 0\n    df.loc[(df[\"SibSp\"] > 0), \"Company\"] = 1\n    df.loc[(df[\"Parch\"] > 0), \"Company\"] = 2\n    df.loc[(df[\"SibSp\"] > 0) & (df[\"Parch\"] > 0), \"Company\"] = 3\n    df = df[\n        [\n            \"PassengerId\",\n            \"Pclass\",\n            \"Sex\",\n            \"Age\",\n            \"Fare\",\n            \"Embarked\",\n            \"Company\",\n            \"Survived\",\n        ]\n    ]\n    return df\n\n\ntrain = encode_df(data)\n\nX_all = train.drop([\"Survived\", \"PassengerId\"], axis=1)\ny_all = train[\"Survived\"]\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=num_test, random_state=23\n)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\n\ndef predict_survival(passenger_class, is_male, age, company, fare, embark_point):\n    if passenger_class is None or embark_point is None:\n        return None\n    df = pd.DataFrame.from_dict(\n        {\n            \"Pclass\": [passenger_class + 1],\n            \"Sex\": [0 if is_male else 1],\n            \"Age\": [age],\n            \"Company\": [\n                (1 if \"Sibling\" in company else 0) + (2 if \"Child\" in company else 0)\n            ],\n            \"Fare\": [fare],\n            \"Embarked\": [embark_point + 1],\n        }\n    )\n    df = encode_age(df)\n    df = encode_fare(df)\n    pred = clf.predict_proba(df)[0]\n    return {\"Perishes\": float(pred[0]), \"Survives\": float(pred[1])}\n\n\ndemo = gr.Interface(\n    predict_survival,\n    [\n        gr.Dropdown([\"first\", \"second\", \"third\"], type=\"index\"),\n        \"checkbox\",\n        gr.Slider(0, 80, value=25),\n        gr.CheckboxGroup([\"Sibling\", \"Child\"], label=\"Travelling with (select all)\"),\n        gr.Number(value=20),\n        gr.Radio([\"S\", \"C\", \"Q\"], type=\"index\"),\n    ],\n    \"label\",\n    examples=[\n        [\"first\", True, 30, [], 50, \"S\"],\n        [\"second\", False, 40, [\"Sibling\", \"Child\"], 10, \"Q\"],\n        [\"third\", True, 30, [\"Child\"], 20, \"S\"],\n    ],\n    interpretation=\"default\",\n    live=True,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()"], "events": "change()", "preprocessing": "passes the value of the selected dropdown entry as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> or its index as an <span class='text-orange-500' style='font-family: monospace; font-size: large;' >int</span> into the function, depending on `type`.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> corresponding to the value of the dropdown entry to be selected.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> representing the drop down value to select.", "parent": "gradio", "prev_obj": "Dataset", "next_obj": "File"}, "file": {"class": null, "name": "File", "description": "Creates a file component that allows uploading generic file (when used as an input) and or displaying generic files (output).", "tags": {"preprocessing": "passes the uploaded file as a {file-object} or {List[file-object]} depending on `file_count` (or a {bytes}/{List{bytes}} depending on `type`)", "postprocessing": "expects function to return a {str} path to a file, or {List[str]} consisting of paths to files.", "examples-format": "a {str} path to a local file that populates the component.", "demos": "zip_to_json, zip_files"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | List[str] | Callable | None", "doc": "Default file to display, given as str file path. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "file_count", "annotation": "str", "doc": "if single, allows user to upload one file. If \"multiple\", user uploads multiple files. If \"directory\", user uploads all files in selected directory. Return type will be list for each file in case of \"multiple\" or \"directory\".", "default": "\"single\""}, {"name": "file_types", "annotation": "List[str] | None", "doc": "List of file extensions or types of files to be uploaded (e.g. ['image', '.json', '.mp4']). \"file\" allows any file to be uploaded, \"image\" allows only image files to be uploaded, \"audio\" allows only audio files to be uploaded, \"video\" allows only video files to be uploaded, \"text\" allows only text files to be uploaded.", "default": "None"}, {"name": "type", "annotation": "str", "doc": "Type of value to be returned by component. \"file\" returns a temporary file object whose path can be retrieved by file_obj.name and original filename can be retrieved with file_obj.orig_name, \"binary\" returns an bytes object.", "default": "\"file\""}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, will allow users to upload a file; if False, can only be used to display files. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.File"}, {"fn": null, "name": "clear", "description": "This event is triggered when the user clears the component (e.g. image or audio) using the X button for the component. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.File"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the file component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}], "returns": {}, "example": null, "parent": "gradio.File"}], "string_shortcuts": [["File", "file", "Uses default values"], ["Files", "files", "Uses file_count=\"multiple\""]], "demos": [["zip_to_json", "from zipfile import ZipFile\n\nimport gradio as gr\n\n\ndef zip_to_json(file_obj):\n    files = []\n    with ZipFile(file_obj.name) as zfile:\n        for zinfo in zfile.infolist():\n            files.append(\n                {\n                    \"name\": zinfo.filename,\n                    \"file_size\": zinfo.file_size,\n                    \"compressed_size\": zinfo.compress_size,\n                }\n            )\n    return files\n\n\ndemo = gr.Interface(zip_to_json, \"file\", \"json\")\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["zip_files", "import os\nfrom zipfile import ZipFile\n\nimport gradio as gr\n\n\ndef zip_files(files):\n    with ZipFile(\"tmp.zip\", \"w\") as zipObj:\n        for idx, file in enumerate(files):\n            zipObj.write(file.name, file.name.split(\"/\")[-1])\n    return \"tmp.zip\"\n\ndemo = gr.Interface(\n    zip_files,\n    gr.File(file_count=\"multiple\", file_types=[\"text\", \".json\", \".csv\"]),\n    \"file\",\n    examples=[[[os.path.join(os.path.dirname(__file__),\"files/titanic.csv\"), \n    os.path.join(os.path.dirname(__file__),\"files/titanic.csv\"), \n    os.path.join(os.path.dirname(__file__),\"files/titanic.csv\")]]], \n    cache_examples=True\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()", "clear()", "upload()"], "events": "change(), clear(), upload()", "preprocessing": "passes the uploaded file as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >file-object</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[file-object]</span> depending on `file_count` (or a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >bytes</span>/<span class='text-orange-500' style='font-family: monospace; font-size: large;' >List<span class='text-orange-500' style='font-family: monospace; font-size: large;' >bytes</span></span> depending on `type`)", "postprocessing": "expects function to return a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> path to a file, or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[str]</span> consisting of paths to files.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> path to a local file that populates the component.", "parent": "gradio", "prev_obj": "Dropdown", "next_obj": "Gallery"}, "gallery": {"class": null, "name": "Gallery", "description": "Used to display a list of images as a gallery that can be scrolled through. <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects a list of images in any format, {List[numpy.array | PIL.Image | str]}, or a {List} of (image, {str} caption) tuples and displays them.", "demos": "fake_gan"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "List[np.ndarray | _Image.Image | str] | Callable | None", "doc": "List of images to display in the gallery by default. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "style", "description": "This method can be used to change the appearance of the gallery component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "grid", "annotation": "int | Tuple | None", "doc": "Represents the number of images that should be shown in one row, for each of the six standard screen sizes (<576px, <768px, <992px, <1200px, <1400px, >1400px). if fewer that 6 are given then the last will be used for all subsequent breakpoints", "default": "None"}, {"name": "height", "annotation": "str | None", "doc": "Height of the gallery.", "default": "None"}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place gallery in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Gallery"}], "string_shortcuts": [["Gallery", "gallery", "Uses default values"]], "demos": [["fake_gan", "# This demo needs to be run from the repo folder.\n# python demo/fake_gan/run.py\nimport os\nimport random\n\nimport gradio as gr\n\n\ndef fake_gan():\n    images = [\n        (random.choice(\n            [\n                \"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=387&q=80\",\n                \"https://images.unsplash.com/photo-1554151228-14d9def656e4?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=386&q=80\",\n                \"https://images.unsplash.com/photo-1542909168-82c3e7fdca5c?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8aHVtYW4lMjBmYWNlfGVufDB8fDB8fA%3D%3D&w=1000&q=80\",\n                \"https://images.unsplash.com/photo-1546456073-92b9f0a8d413?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=387&q=80\",\n                \"https://images.unsplash.com/photo-1601412436009-d964bd02edbc?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=464&q=80\",\n            ]\n        ), f\"label {i}\" if i != 0 else \"label\" * 50)\n        for i in range(3)\n    ]\n    return images\n\n\nwith gr.Blocks() as demo:\n    with gr.Column(variant=\"panel\"):\n        with gr.Row(variant=\"compact\"):\n            text = gr.Textbox(\n                label=\"Enter your prompt\",\n                show_label=False,\n                max_lines=1,\n                placeholder=\"Enter your prompt\",\n            ).style(\n                container=False,\n            )\n            btn = gr.Button(\"Generate image\").style(full_width=False)\n\n        gallery = gr.Gallery(\n            label=\"Generated images\", show_label=False, elem_id=\"gallery\"\n        ).style(grid=[2], height=\"auto\")\n\n    btn.click(fake_gan, None, gallery)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": [], "preprocessing": "this component does *not* accept input.", "postprocessing": "expects a list of images in any format, <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[numpy.array | PIL.Image | str]</span>, or a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List</span> of (image, <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> caption) tuples and displays them.", "parent": "gradio", "prev_obj": "File", "next_obj": "HTML"}, "html": {"class": null, "name": "HTML", "description": "Used to display arbitrary HTML output. <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects a valid HTML {str}.", "demos": "text_analysis", "guides": "key-features"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | Callable", "doc": "Default value. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "\"\""}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.HTML"}], "string_shortcuts": [["HTML", "html", "Uses default values"]], "demos": [["text_analysis", "import gradio as gr\nimport os\nos.system('python -m spacy download en_core_web_sm')\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef text_analysis(text):\n    doc = nlp(text)\n    html = displacy.render(doc, style=\"dep\", page=True)\n    html = (\n        \"<div style='max-width:100%; max-height:360px; overflow:auto'>\"\n        + html\n        + \"</div>\"\n    )\n    pos_count = {\n        \"char_count\": len(text),\n        \"token_count\": 0,\n    }\n    pos_tokens = []\n\n    for token in doc:\n        pos_tokens.extend([(token.text, token.pos_), (\" \", None)])\n\n    return pos_tokens, pos_count, html\n\ndemo = gr.Interface(\n    text_analysis,\n    gr.Textbox(placeholder=\"Enter sentence here...\"),\n    [\"highlight\", \"json\", \"html\"],\n    examples=[\n        [\"What a beautiful morning for a walk!\"],\n        [\"It was the best of times, it was the worst of times.\"],\n    ],\n)\n\ndemo.launch()\n"]], "events_list": ["change()"], "events": "change()", "guides": [{"name": "key-features", "category": "getting-started", "pretty_category": "Getting Started", "guide_index": 2, "absolute_index": 1, "pretty_name": "Key Features", "content": "# Key Features\n\nLet's go through some of the most popular features of Gradio! Here are Gradio's key features: \n\n1. [Adding example inputs](#example-inputs)\n2. [Passing custom error messages](#errors)\n3. [Adding descriptive content](#descriptive-content)\n4. [Setting up flagging](#flagging)\n5. [Preprocessing and postprocessing](#preprocessing-and-postprocessing)\n6. [Styling demos](#styling)\n7. [Queuing users](#queuing)\n8. [Iterative outputs](#iterative-outputs)\n9. [Progress bars](#progress-bars)\n10. [Batch functions](#batch-functions)\n\n## Example Inputs\n\nYou can provide example data that a user can easily load into `Interface`. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you can provide a **nested list** to the `examples=`  keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the [Docs](https://gradio.app/docs#components).\n\n```python\nimport gradio as gr\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        if num2 == 0:\n            raise gr.Error(\"Cannot divide by zero!\")\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\", \n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    examples=[\n        [5, \"add\", 3],\n        [4, \"divide\", 2],\n        [-4, \"multiply\", 2.5],\n        [0, \"subtract\", 1.2],\n    ],\n    title=\"Toy Calculator\",\n    description=\"Here's a sample toy calculator. Allows you to calculate things like $2+2=4$\",\n)\ndemo.launch()\n\n```\n<gradio-app space='gradio/calculator'></gradio-app>\n\nYou can load a large dataset into the examples to browse and interact with the dataset through Gradio. The examples will be automatically paginated (you can configure this through the `examples_per_page` argument of `Interface`). \n\nContinue learning about examples in the [More On Examples](https://gradio.app/more-on-examples) guide.\n\n## Errors\n\nYou wish to pass custom error messages to the user. To do so, raise a `gr.Error(\"custom message\")` to display an error message. If you try to divide by zero in the calculator demo above, a popup modal will display the custom error message. Learn more about Error in the [docs](https://gradio.app/docs#errors).\n\n## Descriptive Content\n\nIn the previous example, you may have noticed the `title=` and `description=` keyword arguments in the `Interface` constructor that helps users understand your app.\n\nThere are three arguments in the `Interface` constructor to specify where this content should go:\n\n* `title`: which accepts text and can display it at the very top of interface, and also becomes the page title.\n* `description`: which accepts text, markdown or HTML and places it right under the title.\n* `article`: which also accepts text, markdown or HTML and places it below the interface.\n\n![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\n\nIf you're using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor.\n\nAnother useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`.\n\n```python\ngr.Number(label='Age')\n```\n\n## Flagging\n\nBy default, an `Interface` will have \"Flag\" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the  `flagging_dir=`  argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.\n\nFor example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- calculator.py\n+-- flagged/\n|   +-- logs.csv\n```\n\n*flagged/logs.csv*\n```csv\nnum1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n```\n\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- sepia.py\n+-- flagged/\n|   +-- logs.csv\n|   +-- im/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n*flagged/logs.csv*\n```csv\nim,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.\n\n## Preprocessing and Postprocessing\n\n![](https://github.com/gradio-app/gradio/blob/main/ui/packages/_website/src/assets/img/dataflow.svg?raw=true)\n\nAs you've seen, Gradio includes components that can handle a variety of different data types, such as images, audio, and video. Most components can be used both as inputs or outputs.\n\nWhen a component is used as an input, Gradio automatically handles the *preprocessing* needed to convert the data from a type sent by the user's browser (such as a base64 representation of a webcam snapshot) to a form that can be accepted by your function (such as a `numpy` array). \n\nSimilarly, when a component is used as an output, Gradio automatically handles the *postprocessing* needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (such as a `Gallery` of images in base64 format).\n\nYou can control the *preprocessing* using the parameters when constructing the image component. For example, here if you instantiate the `Image` component with the following parameters, it will convert the image to the `PIL` type and reshape it to be `(100, 100)` no matter the original size that it was submitted as:\n\n```py\nimg = gradio.Image(shape=(100, 100), type=\"pil\")\n```\n\nIn contrast, here we keep the original size of the image, but invert the colors before converting it to a numpy array:\n\n```py\nimg = gradio.Image(invert_colors=True, type=\"numpy\")\n```\n\nPostprocessing is a lot easier! Gradio automatically recognizes the format of the returned data (e.g. is the `Image` a `numpy` array or a `str` filepath?) and postprocesses it into a format that can be displayed by the browser.\n\nTake a look at the [Docs](https://gradio.app/docs) to see all the preprocessing-related parameters for each Component.\n\n\n## Styling\n\nMany components can be styled through the `style()` method. For example:\n\n```python\nimg = gr.Image(\"lion.jpg\").style(height='24', rounded=False)\n```\n\nTake a look at the [Docs](https://gradio.app/docs) to see all the styling options for each Component.\n\nFor additional styling ability, you can pass any CSS to your app using the `css=` kwarg.\nThe base class for the Gradio app is `gradio-container`, so here's an example that changes the background color of the Gradio app:\n\n```python\nwith gr.Interface(css=\".gradio-container {background-color: red}\") as demo:\n    ...\n```\n\n## Queuing\n\nIf your app expects heavy traffic, use the `queue()` method to control processing rate. This will queue up calls so only a certain number of requests are processed at a single time. Queueing uses websockets, which also prevent network timeouts, so you should use queueing if the inference time of your function is long (> 1min). \n\nWith `Interface`:\n```python\ndemo = gr.Interface(...).queue()\ndemo.launch()\n```\n\nWith `Blocks`:\n```python\nwith gr.Blocks() as demo:\n    #...\ndemo.queue()\ndemo.launch()\n```\n\nYou can control the number of requests processsed at a single time as such:\n\n```python\ndemo.queue(concurrency_count=3)\n```\n\nSee the [Docs on queueing](/docs/#queue) on configuring other queuing parameters.\n\nTo specify only certain functions for queueing in Blocks:\n```python\nwith gr.Blocks() as demo2:\n    num1 = gr.Number()\n    num2 = gr.Number()\n    output = gr.Number()\n    gr.Button(\"Add\").click(\n        lambda a, b: a + b, [num1, num2], output)\n    gr.Button(\"Multiply\").click(\n        lambda a, b: a * b, [num1, num2], output, queue=True)\ndemo2.launch()\n```\n\n## Iterative Outputs\n\nIn some cases, you may want to show a sequence of outputs rather than a single output. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image.\n\nIn such cases, you can supply a **generator** function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single `return` value, a function should `yield` a series of values instead. Usually the `yield` statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:\n\n```python\ndef my_generator(x):\n    for i in range(x):\n        yield i\n```\n\nYou supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image:\n\n```python\nimport gradio as gr\nimport numpy as np\nimport time\n\n# define core fn, which returns a generator {steps} times before returning the image\ndef fake_diffusion(steps):\n    for _ in range(steps):\n        time.sleep(1)\n        image = np.random.random((600, 600, 3))\n        yield image\n    image = \"https://gradio-builds.s3.amazonaws.com/diffusion_image/cute_dog.jpg\"\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion, inputs=gr.Slider(1, 10, 3), outputs=\"image\")\n\n# define queue - required for generators\ndemo.queue()\n\ndemo.launch()\n\n```\n<gradio-app space='gradio/fake_diffusion'></gradio-app>\n\nNote that we've added a `time.sleep(1)` in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).\n\nSupplying a generator into Gradio **requires** you to enable queuing in the underlying Interface or Blocks (see the queuing section above).\n\n## Progress Bars\n\nGradio supports the ability to create a custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a `gradio.Progress` instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the `tqdm()` method of the `Progress` instance to track progress over an iterable, as shown below. Queueing must be enabled for progress updates.\n\n```python\nimport gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\nif __name__ == \"__main__\":\n    demo.queue(concurrency_count=10).launch()\n\n```\n<gradio-app space='gradio/progress_simple'></gradio-app>\n\nIf you use the `tqdm` library, you can even report progress updates automatically from any `tqdm.tqdm` that already exists within your function by setting the default argument as  `gr.Progress(track_tqdm=True)`!\n\n## Batch Functions\n\nGradio supports the ability to pass *batch* functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.\n\nFor example, here is a batched function that takes in two lists of inputs (a list of \nwords and a list of ints), and returns a list of trimmed words as output:\n\n```py\nimport time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])        \n    return [trimmed_words]\n```\n\nThe advantage of using batched functions is that if you enable queuing, the Gradio\nserver can automatically *batch* incoming requests and process them in parallel, \npotentially speeding up your demo. Here's what the Gradio code looks like (notice\nthe `batch=True` and `max_batch_size=16` -- both of these parameters can be passed\ninto event triggers or into the `Interface` class) \n\nWith `Interface`:\n```python\ndemo = gr.Interface(trim_words, [\"textbox\", \"number\"], [\"output\"], \n                    batch=True, max_batch_size=16)\ndemo.queue()\ndemo.launch()\n```\n\n\nWith `Blocks`:\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.queue()\ndemo.launch()\n```\n\nIn the example above, 16 requests could be processed in parallel (for a total inference\ntime of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face `transformers` and `diffusers` models\nwork very naturally with Gradio's batch mode: here's [an example demo using diffusers to\ngenerate images in batches](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\n\nNote: using batch functions with Gradio **requires** you to enable queuing in the underlying Interface or Blocks (see the queuing section above).\n", "html": "<h1 id=\"key-features\">Key Features</h1>\n\n<p>Let's go through some of the most popular features of Gradio! Here are Gradio's key features: </p>\n\n<ol>\n<li><a href=\"#example-inputs\">Adding example inputs</a></li>\n<li><a href=\"#errors\">Passing custom error messages</a></li>\n<li><a href=\"#descriptive-content\">Adding descriptive content</a></li>\n<li><a href=\"#flagging\">Setting up flagging</a></li>\n<li><a href=\"#preprocessing-and-postprocessing\">Preprocessing and postprocessing</a></li>\n<li><a href=\"#styling\">Styling demos</a></li>\n<li><a href=\"#queuing\">Queuing users</a></li>\n<li><a href=\"#iterative-outputs\">Iterative outputs</a></li>\n<li><a href=\"#progress-bars\">Progress bars</a></li>\n<li><a href=\"#batch-functions\">Batch functions</a></li>\n</ol>\n\n<h2 id=\"example-inputs\">Example Inputs</h2>\n\n<p>You can provide example data that a user can easily load into <code>Interface</code>. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you can provide a <strong>nested list</strong> to the <code>examples=</code>  keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#components\">Docs</a>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        if num2 == 0:\n            raise gr.Error(\"Cannot divide by zero!\")\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\", \n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    examples=[\n        [5, \"add\", 3],\n        [4, \"divide\", 2],\n        [-4, \"multiply\", 2.5],\n        [0, \"subtract\", 1.2],\n    ],\n    title=\"Toy Calculator\",\n    description=\"Here's a sample toy calculator. Allows you to calculate things like $2+2=4$\",\n)\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/calculator'></gradio-app></p>\n\n<p>You can load a large dataset into the examples to browse and interact with the dataset through Gradio. The examples will be automatically paginated (you can configure this through the <code>examples_per_page</code> argument of <code>Interface</code>). </p>\n\n<p>Continue learning about examples in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/more-on-examples\">More On Examples</a> guide.</p>\n\n<h2 id=\"errors\">Errors</h2>\n\n<p>You wish to pass custom error messages to the user. To do so, raise a <code>gr.Error(\"custom message\")</code> to display an error message. If you try to divide by zero in the calculator demo above, a popup modal will display the custom error message. Learn more about Error in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#errors\">docs</a>.</p>\n\n<h2 id=\"descriptive-content\">Descriptive Content</h2>\n\n<p>In the previous example, you may have noticed the <code>title=</code> and <code>description=</code> keyword arguments in the <code>Interface</code> constructor that helps users understand your app.</p>\n\n<p>There are three arguments in the <code>Interface</code> constructor to specify where this content should go:</p>\n\n<ul>\n<li><code>title</code>: which accepts text and can display it at the very top of interface, and also becomes the page title.</li>\n<li><code>description</code>: which accepts text, markdown or HTML and places it right under the title.</li>\n<li><code>article</code>: which also accepts text, markdown or HTML and places it below the interface.</li>\n</ul>\n\n<p><img src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true\" alt=\"annotated\" /></p>\n\n<p>If you're using the <code>Blocks</code> API instead, you can insert text, markdown, or HTML anywhere using the <code>gr.Markdown(...)</code> or <code>gr.HTML(...)</code> components, with descriptive content inside the <code>Component</code> constructor.</p>\n\n<p>Another useful keyword argument is <code>label=</code>, which is present in every <code>Component</code>. This modifies the label text at the top of each <code>Component</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Number(label='Age')\n</code></pre></div>\n\n<h2 id=\"flagging\">Flagging</h2>\n\n<p>By default, an <code>Interface</code> will have \"Flag\" button. When a user testing your <code>Interface</code> sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the  <code>flagging_dir=</code>  argument to the <code>Interface</code> constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.</p>\n\n<p>For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:</p>\n\n<div class='codeblock'><pre><code class='lang-directory'>+-- calculator.py\n+-- flagged/\n|   +-- logs.csv\n</code></pre></div>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>num1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n</code></pre></div>\n\n<p>With the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:</p>\n\n<div class='codeblock'><pre><code class='lang-directory'>+-- sepia.py\n+-- flagged/\n|   +-- logs.csv\n|   +-- im/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n</code></pre></div>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>im,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n</code></pre></div>\n\n<p>If you wish for the user to provide a reason for flagging, you can pass a list of strings to the <code>flagging_options</code> argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.</p>\n\n<h2 id=\"preprocessing-and-postprocessing\">Preprocessing and Postprocessing</h2>\n\n<p><img src=\"https://github.com/gradio-app/gradio/blob/main/ui/packages/_website/src/assets/img/dataflow.svg?raw=true\" alt=\"\" /></p>\n\n<p>As you've seen, Gradio includes components that can handle a variety of different data types, such as images, audio, and video. Most components can be used both as inputs or outputs.</p>\n\n<p>When a component is used as an input, Gradio automatically handles the <em>preprocessing</em> needed to convert the data from a type sent by the user's browser (such as a base64 representation of a webcam snapshot) to a form that can be accepted by your function (such as a <code>numpy</code> array). </p>\n\n<p>Similarly, when a component is used as an output, Gradio automatically handles the <em>postprocessing</em> needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (such as a <code>Gallery</code> of images in base64 format).</p>\n\n<p>You can control the <em>preprocessing</em> using the parameters when constructing the image component. For example, here if you instantiate the <code>Image</code> component with the following parameters, it will convert the image to the <code>PIL</code> type and reshape it to be <code>(100, 100)</code> no matter the original size that it was submitted as:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>img = gradio.Image(shape=(100, 100), type=\"pil\")\n</code></pre></div>\n\n<p>In contrast, here we keep the original size of the image, but invert the colors before converting it to a numpy array:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>img = gradio.Image(invert_colors=True, type=\"numpy\")\n</code></pre></div>\n\n<p>Postprocessing is a lot easier! Gradio automatically recognizes the format of the returned data (e.g. is the <code>Image</code> a <code>numpy</code> array or a <code>str</code> filepath?) and postprocesses it into a format that can be displayed by the browser.</p>\n\n<p>Take a look at the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs\">Docs</a> to see all the preprocessing-related parameters for each Component.</p>\n\n<h2 id=\"styling\">Styling</h2>\n\n<p>Many components can be styled through the <code>style()</code> method. For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>img = gr.Image(\"lion.jpg\").style(height='24', rounded=False)\n</code></pre></div>\n\n<p>Take a look at the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs\">Docs</a> to see all the styling options for each Component.</p>\n\n<p>For additional styling ability, you can pass any CSS to your app using the <code>css=</code> kwarg.\nThe base class for the Gradio app is <code>gradio-container</code>, so here's an example that changes the background color of the Gradio app:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Interface(css=\".gradio-container {background-color: red}\") as demo:\n    ...\n</code></pre></div>\n\n<h2 id=\"queuing\">Queuing</h2>\n\n<p>If your app expects heavy traffic, use the <code>queue()</code> method to control processing rate. This will queue up calls so only a certain number of requests are processed at a single time. Queueing uses websockets, which also prevent network timeouts, so you should use queueing if the inference time of your function is long (&gt; 1min). </p>\n\n<p>With <code>Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo = gr.Interface(...).queue()\ndemo.launch()\n</code></pre></div>\n\n<p>With <code>Blocks</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    #...\ndemo.queue()\ndemo.launch()\n</code></pre></div>\n\n<p>You can control the number of requests processsed at a single time as such:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo.queue(concurrency_count=3)\n</code></pre></div>\n\n<p>See the <a rel=\"noopener\" target=\"_blank\" href=\"/docs/#queue\">Docs on queueing</a> on configuring other queuing parameters.</p>\n\n<p>To specify only certain functions for queueing in Blocks:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo2:\n    num1 = gr.Number()\n    num2 = gr.Number()\n    output = gr.Number()\n    gr.Button(\"Add\").click(\n        lambda a, b: a + b, [num1, num2], output)\n    gr.Button(\"Multiply\").click(\n        lambda a, b: a * b, [num1, num2], output, queue=True)\ndemo2.launch()\n</code></pre></div>\n\n<h2 id=\"iterative-outputs\">Iterative Outputs</h2>\n\n<p>In some cases, you may want to show a sequence of outputs rather than a single output. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image.</p>\n\n<p>In such cases, you can supply a <strong>generator</strong> function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single <code>return</code> value, a function should <code>yield</code> a series of values instead. Usually the <code>yield</code> statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def my_generator(x):\n    for i in range(x):\n        yield i\n</code></pre></div>\n\n<p>You supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nimport numpy as np\nimport time\n\n# define core fn, which returns a generator {steps} times before returning the image\ndef fake_diffusion(steps):\n    for _ in range(steps):\n        time.sleep(1)\n        image = np.random.random((600, 600, 3))\n        yield image\n    image = \"https://gradio-builds.s3.amazonaws.com/diffusion_image/cute_dog.jpg\"\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion, inputs=gr.Slider(1, 10, 3), outputs=\"image\")\n\n# define queue - required for generators\ndemo.queue()\n\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/fake_diffusion'></gradio-app></p>\n\n<p>Note that we've added a <code>time.sleep(1)</code> in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).</p>\n\n<p>Supplying a generator into Gradio <strong>requires</strong> you to enable queuing in the underlying Interface or Blocks (see the queuing section above).</p>\n\n<h2 id=\"progress-bars\">Progress Bars</h2>\n\n<p>Gradio supports the ability to create a custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a <code>gradio.Progress</code> instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the <code>tqdm()</code> method of the <code>Progress</code> instance to track progress over an iterable, as shown below. Queueing must be enabled for progress updates.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\nif __name__ == \"__main__\":\n    demo.queue(concurrency_count=10).launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/progress_simple'></gradio-app></p>\n\n<p>If you use the <code>tqdm</code> library, you can even report progress updates automatically from any <code>tqdm.tqdm</code> that already exists within your function by setting the default argument as  <code>gr.Progress(track_tqdm=True)</code>!</p>\n\n<h2 id=\"batch-functions\">Batch Functions</h2>\n\n<p>Gradio supports the ability to pass <em>batch</em> functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.</p>\n\n<p>For example, here is a batched function that takes in two lists of inputs (a list of \nwords and a list of ints), and returns a list of trimmed words as output:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])        \n    return [trimmed_words]\n</code></pre></div>\n\n<p>The advantage of using batched functions is that if you enable queuing, the Gradio\nserver can automatically <em>batch</em> incoming requests and process them in parallel, \npotentially speeding up your demo. Here's what the Gradio code looks like (notice\nthe <code>batch=True</code> and <code>max_batch_size=16</code> -- both of these parameters can be passed\ninto event triggers or into the <code>Interface</code> class) </p>\n\n<p>With <code>Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo = gr.Interface(trim_words, [\"textbox\", \"number\"], [\"output\"], \n                    batch=True, max_batch_size=16)\ndemo.queue()\ndemo.launch()\n</code></pre></div>\n\n<p>With <code>Blocks</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.queue()\ndemo.launch()\n</code></pre></div>\n\n<p>In the example above, 16 requests could be processed in parallel (for a total inference\ntime of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face <code>transformers</code> and <code>diffusers</code> models\nwork very naturally with Gradio's batch mode: here's <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py\">an example demo using diffusers to\ngenerate images in batches</a></p>\n\n<p>Note: using batch functions with Gradio <strong>requires</strong> you to enable queuing in the underlying Interface or Blocks (see the queuing section above).</p>\n", "tags": [], "spaces": [], "url": "/guides/key-features/", "contributor": null}], "preprocessing": "this component does *not* accept input.", "postprocessing": "expects a valid HTML <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span>.", "parent": "gradio", "prev_obj": "Gallery", "next_obj": "HighlightedText"}, "highlightedtext": {"class": null, "name": "HighlightedText", "description": "Displays text that contains spans that are highlighted by category or numerical value. <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects a {List[Tuple[str, float | str]]]} consisting of spans of text and their associated labels, or a {Dict} with two keys: (1) \"text\" whose value is the complete text, and \"entities\", which is a list of dictionaries, each of which have the keys: \"entity\" (consisting of the entity label), \"start\" (the character index where the label starts), and \"end\" (the character index where the label ends). Entities should not overlap.", "demos": "diff_texts, text_analysis", "guides": "named-entity-recognition"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "List[Tuple[str, str | float | None]] | Dict | Callable | None", "doc": "Default value to show. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "color_map", "annotation": "Dict[str, str] | None", "doc": null, "default": "None"}, {"name": "show_legend", "annotation": "bool", "doc": "whether to show span categories in a separate legend or inline.", "default": "False"}, {"name": "combine_adjacent", "annotation": "bool", "doc": "If True, will merge the labels of adjacent tokens belonging to the same category.", "default": "False"}, {"name": "adjacent_separator", "annotation": "str", "doc": "Specifies the separator to be used between tokens if combine_adjacent is True.", "default": "\"\""}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.HighlightedText"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the HighlightedText component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "color_map", "annotation": "Dict[str, str] | None", "doc": "Map between category and respective colors.", "default": "None"}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place the component in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.HighlightedText"}], "string_shortcuts": [["HighlightedText", "highlightedtext", "Uses default values"]], "demos": [["diff_texts", "from difflib import Differ\n\nimport gradio as gr\n\n\ndef diff_texts(text1, text2):\n    d = Differ()\n    return [\n        (token[2:], token[0] if token[0] != \" \" else None)\n        for token in d.compare(text1, text2)\n    ]\n\n\ndemo = gr.Interface(\n    diff_texts,\n    [\n        gr.Textbox(\n            label=\"Initial text\",\n            lines=3,\n            value=\"The quick brown fox jumped over the lazy dogs.\",\n        ),\n        gr.Textbox(\n            label=\"Text to compare\",\n            lines=3,\n            value=\"The fast brown fox jumps over lazy dogs.\",\n        ),\n    ],\n    gr.HighlightedText(\n        label=\"Diff\",\n        combine_adjacent=True,\n    ).style(color_map={\"+\": \"red\", \"-\": \"green\"}),\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["text_analysis", "import gradio as gr\nimport os\nos.system('python -m spacy download en_core_web_sm')\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef text_analysis(text):\n    doc = nlp(text)\n    html = displacy.render(doc, style=\"dep\", page=True)\n    html = (\n        \"<div style='max-width:100%; max-height:360px; overflow:auto'>\"\n        + html\n        + \"</div>\"\n    )\n    pos_count = {\n        \"char_count\": len(text),\n        \"token_count\": 0,\n    }\n    pos_tokens = []\n\n    for token in doc:\n        pos_tokens.extend([(token.text, token.pos_), (\" \", None)])\n\n    return pos_tokens, pos_count, html\n\ndemo = gr.Interface(\n    text_analysis,\n    gr.Textbox(placeholder=\"Enter sentence here...\"),\n    [\"highlight\", \"json\", \"html\"],\n    examples=[\n        [\"What a beautiful morning for a walk!\"],\n        [\"It was the best of times, it was the worst of times.\"],\n    ],\n)\n\ndemo.launch()\n"]], "events_list": ["change()"], "events": "change()", "guides": [{"name": "named-entity-recognition", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 32, "pretty_name": "Named Entity Recognition", "content": "# Named-Entity Recognition \n\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert_ner_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial_Analyst_AI\nTags: NER, TEXT, HIGHLIGHT\n\n## Introduction\n\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech. \n\nFor example, given the sentence:\n\n> Does Chicago have any Pakistani restaurants?\n\nA named-entity recognition algorithm may  identify:\n\n* \"Chicago\" as a **location**\n* \"Pakistani\" as an **ethnicity**  \n\n\nand so on. \n\nUsing `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team.\n\nHere is an example of a demo that you'll be able to build:\n\n<gradio-app space='gradio/ner_pipeline'></gradio-app>\n\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn! \n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained named-entity recognition model. You can use your own, while in this tutorial, we will use one from the `transformers` library.\n\n### Approach 1: List of Entity Dictionaries\n\nMany named-entity recognition models output a list of dictionaries. Each dictionary consists of an *entity*, a \"start\" index, and an \"end\" index. This is, for example, how NER models in the `transformers` library operate:\n\n```py\nfrom transformers import pipeline \nner_pipeline = pipeline(\"ner\")\nner_pipeline(\"Does Chicago have any Pakistani restaurants\")\n```\n\nOutput:\n\n```bash\n[{'entity': 'I-LOC',\n  'score': 0.9988978,\n  'index': 2,\n  'word': 'Chicago',\n  'start': 5,\n  'end': 12},\n {'entity': 'I-MISC',\n  'score': 0.9958592,\n  'index': 5,\n  'word': 'Pakistani',\n  'start': 22,\n  'end': 31}]\n```\n\nIf you have such a model, it is very easy to hook it up to Gradio's `HighlightedText` component. All you need to do is pass in this **list of entities**, along with the **original text** to the model, together as dictionary, with the keys being `\"entities\"` and `\"text\"` respectively.\n\nHere is a complete example:\n\n```python\nfrom transformers import pipeline\n\nimport gradio as gr\n\nner_pipeline = pipeline(\"ner\")\n\nexamples = [\n    \"Does Chicago have any stores and does Joe live here?\",\n]\n\ndef ner(text):\n    output = ner_pipeline(text)\n    return {\"text\": text, \"entities\": output}    \n\ndemo = gr.Interface(ner,\n             gr.Textbox(placeholder=\"Enter sentence here...\"), \n             gr.HighlightedText(),\n             examples=examples)\n\ndemo.launch()\n\n```\n<gradio-app space='gradio/ner_pipeline'></gradio-app>\n\n### Approach 2: List of Tuples\n\nAn alternative way to pass data into the `HighlightedText` component is a list of tuples. The first element of each tuple should be the word or words that are being classified into a particular entity. The second element should be the entity label (or `None` if they should be unlabeled). The `HighlightedText` component automatically strings together the words and labels to display the entities.\n\nIn some cases, this can be easier than the first approach. Here is a demo showing this approach using Spacy's parts-of-speech tagger:\n\n```python\nimport gradio as gr\nimport os\nos.system('python -m spacy download en_core_web_sm')\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef text_analysis(text):\n    doc = nlp(text)\n    html = displacy.render(doc, style=\"dep\", page=True)\n    html = (\n        \"<div style='max-width:100%; max-height:360px; overflow:auto'>\"\n        + html\n        + \"</div>\"\n    )\n    pos_count = {\n        \"char_count\": len(text),\n        \"token_count\": 0,\n    }\n    pos_tokens = []\n\n    for token in doc:\n        pos_tokens.extend([(token.text, token.pos_), (\" \", None)])\n\n    return pos_tokens, pos_count, html\n\ndemo = gr.Interface(\n    text_analysis,\n    gr.Textbox(placeholder=\"Enter sentence here...\"),\n    [\"highlight\", \"json\", \"html\"],\n    examples=[\n        [\"What a beautiful morning for a walk!\"],\n        [\"It was the best of times, it was the worst of times.\"],\n    ],\n)\n\ndemo.launch()\n\n```\n<gradio-app space='gradio/text_analysis'></gradio-app>\n\n\n--------------------------------------------\n\n\nAnd you're done! That's all you need to know to build a web-based GUI for your NER model. \n\nFun tip: you can share your NER demo instantly with others simply by setting `share=True` in `launch()`. \n\n\n", "html": "<h1 id=\"named-entity-recognition\">Named-Entity Recognition</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Named-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech. </p>\n\n<p>For example, given the sentence:</p>\n\n<blockquote>\n  <p>Does Chicago have any Pakistani restaurants?</p>\n</blockquote>\n\n<p>A named-entity recognition algorithm may  identify:</p>\n\n<ul>\n<li>\"Chicago\" as a <strong>location</strong></li>\n<li>\"Pakistani\" as an <strong>ethnicity</strong>  </li>\n</ul>\n\n<p>and so on. </p>\n\n<p>Using <code>gradio</code> (specifically the <code>HighlightedText</code> component), you can easily build a web demo of your NER model and share that with the rest of your team.</p>\n\n<p>Here is an example of a demo that you'll be able to build:</p>\n\n<p><gradio-app space='gradio/ner_pipeline'></gradio-app></p>\n\n<p>This tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the <code>HighlightedText</code> component -- depending on your NER model, either of these two ways may be easier to learn! </p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. You will also need a pretrained named-entity recognition model. You can use your own, while in this tutorial, we will use one from the <code>transformers</code> library.</p>\n\n<h3 id=\"approach-1-list-of-entity-dictionaries\">Approach 1: List of Entity Dictionaries</h3>\n\n<p>Many named-entity recognition models output a list of dictionaries. Each dictionary consists of an <em>entity</em>, a \"start\" index, and an \"end\" index. This is, for example, how NER models in the <code>transformers</code> library operate:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>from transformers import pipeline \nner_pipeline = pipeline(\"ner\")\nner_pipeline(\"Does Chicago have any Pakistani restaurants\")\n</code></pre></div>\n\n<p>Output:</p>\n\n<div class='codeblock'><pre><code class='lang-bash'>[{'entity': 'I-LOC',\n  'score': 0.9988978,\n  'index': 2,\n  'word': 'Chicago',\n  'start': 5,\n  'end': 12},\n {'entity': 'I-MISC',\n  'score': 0.9958592,\n  'index': 5,\n  'word': 'Pakistani',\n  'start': 22,\n  'end': 31}]\n</code></pre></div>\n\n<p>If you have such a model, it is very easy to hook it up to Gradio's <code>HighlightedText</code> component. All you need to do is pass in this <strong>list of entities</strong>, along with the <strong>original text</strong> to the model, together as dictionary, with the keys being <code>\"entities\"</code> and <code>\"text\"</code> respectively.</p>\n\n<p>Here is a complete example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import pipeline\n\nimport gradio as gr\n\nner_pipeline = pipeline(\"ner\")\n\nexamples = [\n    \"Does Chicago have any stores and does Joe live here?\",\n]\n\ndef ner(text):\n    output = ner_pipeline(text)\n    return {\"text\": text, \"entities\": output}    \n\ndemo = gr.Interface(ner,\n             gr.Textbox(placeholder=\"Enter sentence here...\"), \n             gr.HighlightedText(),\n             examples=examples)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/ner_pipeline'></gradio-app></p>\n\n<h3 id=\"approach-2-list-of-tuples\">Approach 2: List of Tuples</h3>\n\n<p>An alternative way to pass data into the <code>HighlightedText</code> component is a list of tuples. The first element of each tuple should be the word or words that are being classified into a particular entity. The second element should be the entity label (or <code>None</code> if they should be unlabeled). The <code>HighlightedText</code> component automatically strings together the words and labels to display the entities.</p>\n\n<p>In some cases, this can be easier than the first approach. Here is a demo showing this approach using Spacy's parts-of-speech tagger:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nimport os\nos.system('python -m spacy download en_core_web_sm')\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef text_analysis(text):\n    doc = nlp(text)\n    html = displacy.render(doc, style=\"dep\", page=True)\n    html = (\n        \"<div style='max-width:100%; max-height:360px; overflow:auto'>\"\n        + html\n        + \"</div>\"\n    )\n    pos_count = {\n        \"char_count\": len(text),\n        \"token_count\": 0,\n    }\n    pos_tokens = []\n\n    for token in doc:\n        pos_tokens.extend([(token.text, token.pos_), (\" \", None)])\n\n    return pos_tokens, pos_count, html\n\ndemo = gr.Interface(\n    text_analysis,\n    gr.Textbox(placeholder=\"Enter sentence here...\"),\n    [\"highlight\", \"json\", \"html\"],\n    examples=[\n        [\"What a beautiful morning for a walk!\"],\n        [\"It was the best of times, it was the worst of times.\"],\n    ],\n)\n\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/text_analysis'></gradio-app></p>\n\n<hr />\n\n<p>And you're done! That's all you need to know to build a web-based GUI for your NER model. </p>\n\n<p>Fun tip: you can share your NER demo instantly with others simply by setting <code>share=True</code> in <code>launch()</code>. </p>\n", "tags": ["NER", "TEXT", "HIGHLIGHT"], "spaces": ["https://huggingface.co/spaces/rajistics/biobert_ner_demo", "https://huggingface.co/spaces/abidlabs/ner", "https://huggingface.co/spaces/rajistics/Financial_Analyst_AI"], "url": "/guides/named-entity-recognition/", "contributor": null}], "preprocessing": "this component does *not* accept input.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[Tuple[str, float | str]]]</span> consisting of spans of text and their associated labels, or a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >Dict</span> with two keys: (1) \"text\" whose value is the complete text, and \"entities\", which is a list of dictionaries, each of which have the keys: \"entity\" (consisting of the entity label), \"start\" (the character index where the label starts), and \"end\" (the character index where the label ends). Entities should not overlap.", "parent": "gradio", "prev_obj": "HTML", "next_obj": "Image"}, "image": {"class": null, "name": "Image", "description": "Creates an image component that can be used to upload/draw images (as an input) or display images (as an output).", "tags": {"preprocessing": "passes the uploaded image as a {numpy.array}, {PIL.Image} or {str} filepath depending on `type` -- unless `tool` is `sketch` AND source is one of `upload` or `webcam`. In these cases, a {dict} with keys `image` and `mask` is passed, and the format of the corresponding values depends on `type`.", "postprocessing": "expects a {numpy.array}, {PIL.Image} or {str} or {pathlib.Path} filepath to an image and displays the image.", "examples-format": "a {str} filepath to a local file that contains the image.", "demos": "image_mod, image_mod_default_image", "guides": "Gradio-and-ONNX-on-Hugging-Face, image-classification-in-pytorch, image-classification-in-tensorflow, image-classification-with-vision-transformers, building-a-pictionary-app, create-your-own-friends-with-a-gan"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | _Image.Image | np.ndarray | None", "doc": "A PIL Image, numpy array, path or URL for the default value that Image component is going to take. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "shape", "annotation": "Tuple[int, int] | None", "doc": "(width, height) shape to crop and resize image to; if None, matches input image size. Pass None for either width or height to only crop and resize the other.", "default": "None"}, {"name": "image_mode", "annotation": "str", "doc": "\"RGB\" if color, or \"L\" if black and white.", "default": "\"RGB\""}, {"name": "invert_colors", "annotation": "bool", "doc": "whether to invert the image as a preprocessing step.", "default": "False"}, {"name": "source", "annotation": "str", "doc": "Source of image. \"upload\" creates a box where user can drop an image file, \"webcam\" allows user to take snapshot from their webcam, \"canvas\" defaults to a white image that can be edited and drawn upon with tools.", "default": "\"upload\""}, {"name": "tool", "annotation": "str | None", "doc": "Tools used for editing. \"editor\" allows a full screen editor (and is the default if source is \"upload\" or \"webcam\"), \"select\" provides a cropping and zoom tool, \"sketch\" allows you to create a binary sketch (and is the default if source=\"canvas\"), and \"color-sketch\" allows you to created a sketch in different colors. \"color-sketch\" can be used with source=\"upload\" or \"webcam\" to allow sketching on an image. \"sketch\" can also be used with \"upload\" or \"webcam\" to create a mask over an image and in that case both the image and mask are passed into the function as a dictionary with keys \"image\" and \"mask\" respectively.", "default": "None"}, {"name": "type", "annotation": "str", "doc": "The format the image is converted to before being passed into the prediction function. \"numpy\" converts the image to a numpy array with shape (width, height, 3) and values from 0 to 255, \"pil\" converts the image to a PIL image object, \"filepath\" passes a str path to a temporary file containing the image.", "default": "\"numpy\""}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, will allow users to upload and edit an image; if False, can only be used to display images. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "streaming", "annotation": "bool", "doc": "If True when used in a `live` interface, will automatically stream webcam feed. Only valid is source is 'webcam'.", "default": "False"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}, {"name": "mirror_webcam", "annotation": "bool", "doc": "If True webcam will be mirrored. Default is True.", "default": "True"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "edit", "description": "This event is triggered when the user edits the component (e.g. image) using the built-in editor. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Image"}, {"fn": null, "name": "clear", "description": "This event is triggered when the user clears the component (e.g. image or audio) using the X button for the component. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Image"}, {"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Image"}, {"fn": null, "name": "stream", "description": "This event is triggered when the user streams the component (e.g. a live webcam component)", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable", "doc": "Callable function"}, {"name": "inputs", "annotation": "List[Component]", "doc": "List of inputs"}, {"name": "outputs", "annotation": "List[Component]", "doc": "List of outputs"}, {"name": "api_name", "annotation": "str | None", "doc": null, "default": "None"}, {"name": "preprocess", "annotation": "bool", "doc": null, "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": null, "default": "True"}], "returns": {}, "example": null, "parent": "gradio.Image"}, {"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Image"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the Image component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "height", "annotation": "int | None", "doc": "Height of the image.", "default": "None"}, {"name": "width", "annotation": "int | None", "doc": "Width of the image.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Image"}], "string_shortcuts": [["Image", "image", "Uses default values"], ["Webcam", "webcam", "Uses source=\"webcam\", interactive=True"], ["Sketchpad", "sketchpad", "Uses image_mode=\"L\", source=\"canvas\", shape=(28, 28), invert_colors=True, interactive=True"], ["Paint", "paint", "Uses source=\"canvas\", tool=\"color-sketch\", interactive=True"], ["ImageMask", "imagemask", "Uses source=\"upload\", tool=\"sketch\", interactive=True"], ["ImagePaint", "imagepaint", "Uses source=\"upload\", tool=\"color-sketch\", interactive=True"], ["Pil", "pil", "Uses type=\"pil\""]], "demos": [["image_mod", "import gradio as gr\nimport os\n\n\ndef image_mod(image):\n    return image.rotate(45)\n\n\ndemo = gr.Interface(\n    image_mod,\n    gr.Image(type=\"pil\"),\n    \"image\",\n    flagging_options=[\"blurry\", \"incorrect\", \"other\"],\n    examples=[\n        os.path.join(os.path.dirname(__file__), \"images/cheetah1.jpg\"),\n        os.path.join(os.path.dirname(__file__), \"images/lion.jpg\"),\n        os.path.join(os.path.dirname(__file__), \"images/logo.png\"),\n        os.path.join(os.path.dirname(__file__), \"images/tower.jpg\"),\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["image_mod_default_image", "import gradio as gr\nimport os\n\n\ndef image_mod(image):\n    return image.rotate(45)\n\n\ncheetah = os.path.join(os.path.dirname(__file__), \"images/cheetah1.jpg\")\n\ndemo = gr.Interface(image_mod, gr.Image(type=\"pil\", value=cheetah), \"image\",\n    flagging_options=[\"blurry\", \"incorrect\", \"other\"], examples=[\n        os.path.join(os.path.dirname(__file__), \"images/lion.jpg\"),\n        os.path.join(os.path.dirname(__file__), \"images/logo.png\")\n        ])\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()", "edit()", "clear()", "stream()", "upload()"], "events": "change(), edit(), clear(), stream(), upload()", "guides": [{"name": "Gradio-and-ONNX-on-Hugging-Face", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 15, "pretty_name": "Gradio And ONNX On Hugging Face", "content": "# Gradio and ONNX on Hugging Face\n\nRelated spaces: https://huggingface.co/spaces/onnx/EfficientNet-Lite4\nTags: ONNX, SPACES\nContributed by Gradio and the <a href=\"https://onnx.ai/\">ONNX</a> team\n\n## Introduction\n\nIn this Guide, we'll walk you through:\n\n* Introduction of ONNX, ONNX model zoo, Gradio, and Hugging Face Spaces\n* How to setup a Gradio demo for EfficientNet-Lite4\n* How to contribute your own Gradio demos for the ONNX organization on Hugging Face\n\nHere's an example of an ONNX model: try out the EfficientNet-Lite4 demo below.\n\n<iframe src=\"https://onnx-efficientnet-lite4.hf.space\" frameBorder=\"0\" height=\"810\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n## What is the ONNX Model Zoo?\nOpen Neural Network Exchange ([ONNX](https://onnx.ai/)) is an open standard format for representing machine learning models. ONNX is supported by a community of partners who have implemented it in many frameworks and tools. For example, if you have trained a model in TensorFlow or PyTorch, you can convert it to ONNX easily, and from there run it on a variety of devices using an engine/compiler like ONNX Runtime.\n\nThe [ONNX Model Zoo](https://github.com/onnx/models) is a collection of pre-trained, state-of-the-art models in the ONNX format contributed by community members. Accompanying each model are Jupyter notebooks for model training and running inference with the trained model. The notebooks are written in Python and include links to the training dataset as well as references to the original paper that describes the model architecture.\n\n\n## What are Hugging Face Spaces & Gradio?\n\n### Gradio\n\nGradio lets users demo their machine learning models as a web app all in python code. Gradio wraps a python function into a user inferface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\n\nGet started [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces\n\nHugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).\n\n### Hugging Face Models\n\nHugging Face Model Hub also supports ONNX models and ONNX models can be filtered through the [ONNX tag](https://huggingface.co/models?library=onnx&sort=downloads)\n\n## How did Hugging Face help the ONNX Model Zoo?\nThere are a lot of Jupyter notebooks in the ONNX Model Zoo for users to test models. Previously, users needed to download the models themselves and run those notebooks locally for testing. With Hugging Face, the testing process can be much simpler and more user-friendly. Users can easily try certain ONNX Model Zoo model on Hugging Face Spaces and run a quick demo powered by Gradio with ONNX Runtime, all on cloud without downloading anything locally. Note, there are various runtimes for ONNX, e.g., [ONNX Runtime](https://github.com/microsoft/onnxruntime), [MXNet](https://github.com/apache/incubator-mxnet).\n\n## What is the role of ONNX Runtime?\nONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live Gradio demos with ONNX Model Zoo model on Hugging Face possible.\n\nONNX Runtime inference can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. For more information please see the [official website](https://onnxruntime.ai/).\n\n## Setting up a Gradio Demo for EfficientNet-Lite4\n\nEfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite models. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU. To learn more read the [model card](https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4)\n\nHere we walk through setting up a example demo for EfficientNet-Lite4 using Gradio\n\nFirst we import our dependencies and download and load the efficientnet-lite4 model from the onnx model zoo. Then load the labels from the labels_map.txt file. We then setup our preprocessing functions, load the model for inference, and setup the inference function. Finally, the inference function is wrapped into a gradio inferface for a user to interact with. See the full code below.\n\n\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nimport gradio as gr\nfrom huggingface_hub import hf_hub_download\nfrom onnx import hub\nimport onnxruntime as ort\n\n# loads ONNX model from ONNX Model Zoo\nmodel = hub.load(\"efficientnet-lite4\")\n# loads the labels text file\nlabels = json.load(open(\"labels_map.txt\", \"r\"))\n\n# sets image file dimensions to 224x224 by resizing and cropping image from center\ndef pre_process_edgetpu(img, dims):\n    output_height, output_width, _ = dims\n    img = resize_with_aspectratio(img, output_height, output_width, inter_pol=cv2.INTER_LINEAR)\n    img = center_crop(img, output_height, output_width)\n    img = np.asarray(img, dtype='float32')\n    # converts jpg pixel value from [0 - 255] to float array [-1.0 - 1.0]\n    img -= [127.0, 127.0, 127.0]\n    img /= [128.0, 128.0, 128.0]\n    return img\n\n# resizes the image with a proportional scale\ndef resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.INTER_LINEAR):\n    height, width, _ = img.shape\n    new_height = int(100. * out_height / scale)\n    new_width = int(100. * out_width / scale)\n    if height > width:\n        w = new_width\n        h = int(new_height * height / width)\n    else:\n        h = new_height\n        w = int(new_width * width / height)\n    img = cv2.resize(img, (w, h), interpolation=inter_pol)\n    return img\n\n# crops the image around the center based on given height and width\ndef center_crop(img, out_height, out_width):\n    height, width, _ = img.shape\n    left = int((width - out_width) / 2)\n    right = int((width + out_width) / 2)\n    top = int((height - out_height) / 2)\n    bottom = int((height + out_height) / 2)\n    img = img[top:bottom, left:right]\n    return img\n\n\nsess = ort.InferenceSession(model)\n\ndef inference(img):\n  img = cv2.imread(img)\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  \n  img = pre_process_edgetpu(img, (224, 224, 3))\n  \n  img_batch = np.expand_dims(img, axis=0)\n\n  results = sess.run([\"Softmax:0\"], {\"images:0\": img_batch})[0]\n  result = reversed(results[0].argsort()[-5:])\n  resultdic = {}\n  for r in result:\n      resultdic[labels[str(r)]] = float(results[0][r])\n  return resultdic\n  \ntitle = \"EfficientNet-Lite4\"\ndescription = \"EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite model. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU.\"\nexamples = [['catonnx.jpg']]\ngr.Interface(inference, gr.Image(type=\"filepath\"), \"label\", title=title, description=description, examples=examples).launch()\n```\n\n\n## How to contribute Gradio demos on HF spaces using ONNX models\n\n* Add model to the [onnx model zoo](https://github.com/onnx/models/blob/main/.github/PULL_REQUEST_TEMPLATE.md)\n* Create an account on Hugging Face [here](https://huggingface.co/join).\n* See list of models left to add to ONNX organization, please refer to the table with the [Models list](https://github.com/onnx/models#models)\n* Add Gradio Demo under your username, see this [blog post](https://huggingface.co/blog/gradio-spaces) for setting up Gradio Demo on Hugging Face. \n* Request to join ONNX Organization [here](https://huggingface.co/onnx).\n* Once approved transfer model from your username to ONNX organization\n* Add a badge for model in model table, see examples in [Models list](https://github.com/onnx/models#models)\n", "html": "<h1 id=\"gradio-and-onnx-on-hugging-face\">Gradio and ONNX on Hugging Face</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>In this Guide, we'll walk you through:</p>\n\n<ul>\n<li>Introduction of ONNX, ONNX model zoo, Gradio, and Hugging Face Spaces</li>\n<li>How to setup a Gradio demo for EfficientNet-Lite4</li>\n<li>How to contribute your own Gradio demos for the ONNX organization on Hugging Face</li>\n</ul>\n\n<p>Here's an example of an ONNX model: try out the EfficientNet-Lite4 demo below.</p>\n\n<iframe src=\"https://onnx-efficientnet-lite4.hf.space\" frameBorder=\"0\" height=\"810\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"what-is-the-onnx-model-zoo\">What is the ONNX Model Zoo?</h2>\n\n<p>Open Neural Network Exchange (<a rel=\"noopener\" target=\"_blank\" href=\"https://onnx.ai/\">ONNX</a>) is an open standard format for representing machine learning models. ONNX is supported by a community of partners who have implemented it in many frameworks and tools. For example, if you have trained a model in TensorFlow or PyTorch, you can convert it to ONNX easily, and from there run it on a variety of devices using an engine/compiler like ONNX Runtime.</p>\n\n<p>The <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/onnx/models\">ONNX Model Zoo</a> is a collection of pre-trained, state-of-the-art models in the ONNX format contributed by community members. Accompanying each model are Jupyter notebooks for model training and running inference with the trained model. The notebooks are written in Python and include links to the training dataset as well as references to the original paper that describes the model architecture.</p>\n\n<h2 id=\"what-are-hugging-face-spaces-gradio\">What are Hugging Face Spaces &amp; Gradio?</h2>\n\n<h3 id=\"gradio\">Gradio</h3>\n\n<p>Gradio lets users demo their machine learning models as a web app all in python code. Gradio wraps a python function into a user inferface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.</p>\n\n<p>Get started <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/getting_started\">here</a></p>\n\n<h3 id=\"hugging-face-spaces\">Hugging Face Spaces</h3>\n\n<p>Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/launch\">here</a>.</p>\n\n<h3 id=\"hugging-face-models\">Hugging Face Models</h3>\n\n<p>Hugging Face Model Hub also supports ONNX models and ONNX models can be filtered through the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/models?library=onnx&amp;sort=downloads\">ONNX tag</a></p>\n\n<h2 id=\"how-did-hugging-face-help-the-onnx-model-zoo\">How did Hugging Face help the ONNX Model Zoo?</h2>\n\n<p>There are a lot of Jupyter notebooks in the ONNX Model Zoo for users to test models. Previously, users needed to download the models themselves and run those notebooks locally for testing. With Hugging Face, the testing process can be much simpler and more user-friendly. Users can easily try certain ONNX Model Zoo model on Hugging Face Spaces and run a quick demo powered by Gradio with ONNX Runtime, all on cloud without downloading anything locally. Note, there are various runtimes for ONNX, e.g., <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/microsoft/onnxruntime\">ONNX Runtime</a>, <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/apache/incubator-mxnet\">MXNet</a>.</p>\n\n<h2 id=\"what-is-the-role-of-onnx-runtime\">What is the role of ONNX Runtime?</h2>\n\n<p>ONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live Gradio demos with ONNX Model Zoo model on Hugging Face possible.</p>\n\n<p>ONNX Runtime inference can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. For more information please see the <a rel=\"noopener\" target=\"_blank\" href=\"https://onnxruntime.ai/\">official website</a>.</p>\n\n<h2 id=\"setting-up-a-gradio-demo-for-efficientnet-lite4\">Setting up a Gradio Demo for EfficientNet-Lite4</h2>\n\n<p>EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite models. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU. To learn more read the <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4\">model card</a></p>\n\n<p>Here we walk through setting up a example demo for EfficientNet-Lite4 using Gradio</p>\n\n<p>First we import our dependencies and download and load the efficientnet-lite4 model from the onnx model zoo. Then load the labels from the labels_map.txt file. We then setup our preprocessing functions, load the model for inference, and setup the inference function. Finally, the inference function is wrapped into a gradio inferface for a user to interact with. See the full code below.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nimport gradio as gr\nfrom huggingface_hub import hf_hub_download\nfrom onnx import hub\nimport onnxruntime as ort\n\n# loads ONNX model from ONNX Model Zoo\nmodel = hub.load(\"efficientnet-lite4\")\n# loads the labels text file\nlabels = json.load(open(\"labels_map.txt\", \"r\"))\n\n# sets image file dimensions to 224x224 by resizing and cropping image from center\ndef pre_process_edgetpu(img, dims):\n    output_height, output_width, _ = dims\n    img = resize_with_aspectratio(img, output_height, output_width, inter_pol=cv2.INTER_LINEAR)\n    img = center_crop(img, output_height, output_width)\n    img = np.asarray(img, dtype='float32')\n    # converts jpg pixel value from [0 - 255] to float array [-1.0 - 1.0]\n    img -= [127.0, 127.0, 127.0]\n    img /= [128.0, 128.0, 128.0]\n    return img\n\n# resizes the image with a proportional scale\ndef resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.INTER_LINEAR):\n    height, width, _ = img.shape\n    new_height = int(100. * out_height / scale)\n    new_width = int(100. * out_width / scale)\n    if height > width:\n        w = new_width\n        h = int(new_height * height / width)\n    else:\n        h = new_height\n        w = int(new_width * width / height)\n    img = cv2.resize(img, (w, h), interpolation=inter_pol)\n    return img\n\n# crops the image around the center based on given height and width\ndef center_crop(img, out_height, out_width):\n    height, width, _ = img.shape\n    left = int((width - out_width) / 2)\n    right = int((width + out_width) / 2)\n    top = int((height - out_height) / 2)\n    bottom = int((height + out_height) / 2)\n    img = img[top:bottom, left:right]\n    return img\n\n\nsess = ort.InferenceSession(model)\n\ndef inference(img):\n  img = cv2.imread(img)\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n  img = pre_process_edgetpu(img, (224, 224, 3))\n\n  img_batch = np.expand_dims(img, axis=0)\n\n  results = sess.run([\"Softmax:0\"], {\"images:0\": img_batch})[0]\n  result = reversed(results[0].argsort()[-5:])\n  resultdic = {}\n  for r in result:\n      resultdic[labels[str(r)]] = float(results[0][r])\n  return resultdic\n\ntitle = \"EfficientNet-Lite4\"\ndescription = \"EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite model. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU.\"\nexamples = [['catonnx.jpg']]\ngr.Interface(inference, gr.Image(type=\"filepath\"), \"label\", title=title, description=description, examples=examples).launch()\n</code></pre></div>\n\n<h2 id=\"how-to-contribute-gradio-demos-on-hf-spaces-using-onnx-models\">How to contribute Gradio demos on HF spaces using ONNX models</h2>\n\n<ul>\n<li>Add model to the <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/onnx/models/blob/main/.github/PULL_REQUEST_TEMPLATE.md\">onnx model zoo</a></li>\n<li>Create an account on Hugging Face <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/join\">here</a>.</li>\n<li>See list of models left to add to ONNX organization, please refer to the table with the <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/onnx/models#models\">Models list</a></li>\n<li>Add Gradio Demo under your username, see this <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/blog/gradio-spaces\">blog post</a> for setting up Gradio Demo on Hugging Face. </li>\n<li>Request to join ONNX Organization <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/onnx\">here</a>.</li>\n<li>Once approved transfer model from your username to ONNX organization</li>\n<li>Add a badge for model in model table, see examples in <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/onnx/models#models\">Models list</a></li>\n</ul>\n", "tags": ["ONNX", "SPACES"], "spaces": ["https://huggingface.co/spaces/onnx/EfficientNet-Lite4"], "url": "/guides/Gradio-and-ONNX-on-Hugging-Face/", "contributor": "Gradio and the <a href=\"https://onnx.ai/\">ONNX</a> team"}, {"name": "image-classification-in-pytorch", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 17, "pretty_name": "Image Classification In Pytorch", "content": "# Image Classification in PyTorch\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/pytorch-image-classifier, https://huggingface.co/spaces/pytorch/ResNet, https://huggingface.co/spaces/pytorch/ResNext, https://huggingface.co/spaces/pytorch/SqueezeNet\nTags: VISION, RESNET, PYTORCH\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging. \n\nSuch models are perfect to use with Gradio's *image* input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained image classification model, so you should also have `torch` installed.\n\n## Step 1 \u2014 Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from [PyTorch Hub](https://pytorch.org/hub/pytorch_vision_resnet/). You can use a different pretrained model or train your own. \n\n```python\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n```\n\nBecause we will be using the model for inference, we have called the `.eval()` method.\n\n## Step 2 \u2014 Defining a `predict` function\n\nNext, we will need to define a function that takes in the *user input*, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}    \n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n* `inp`: the input image as a `PIL` image\n\nThen, the function converts the image to a PIL Image and then eventually a PyTorch `tensor`, passes it through the model, and returns:\n\n* `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 \u2014 Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. \n\nIn this case, the input component is a drag-and-drop image component. To create this input, we use `Image(type=\"pil\")` which creates the component and handles the preprocessing to convert that to a `PIL` image. \n\nThe output component will be a `Label`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as `Label(num_top_classes=3)`.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=predict, \n             inputs=gr.Image(type=\"pil\"),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n\n", "html": "<h1 id=\"image-classification-in-pytorch\">Image Classification in PyTorch</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging. </p>\n\n<p>Such models are perfect to use with Gradio's <em>image</em> input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):</p>\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started!</p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. We will be using a pretrained image classification model, so you should also have <code>torch</code> installed.</p>\n\n<h2 id=\"step-1-setting-up-the-image-classification-model\">Step 1 \u2014 Setting up the Image Classification Model</h2>\n\n<p>First, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from <a rel=\"noopener\" target=\"_blank\" href=\"https://pytorch.org/hub/pytorch_vision_resnet/\">PyTorch Hub</a>. You can use a different pretrained model or train your own. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import torch\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n</code></pre></div>\n\n<p>Because we will be using the model for inference, we have called the <code>.eval()</code> method.</p>\n\n<h2 id=\"step-2-defining-a-predict-function\">Step 2 \u2014 Defining a <code>predict</code> function</h2>\n\n<p>Next, we will need to define a function that takes in the <em>user input</em>, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this <a rel=\"noopener\" target=\"_blank\" href=\"https://git.io/JJkYN\">text file</a>.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import requests\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}    \n  return confidences\n</code></pre></div>\n\n<p>Let's break this down. The function takes one parameter:</p>\n\n<ul>\n<li><code>inp</code>: the input image as a <code>PIL</code> image</li>\n</ul>\n\n<p>Then, the function converts the image to a PIL Image and then eventually a PyTorch <code>tensor</code>, passes it through the model, and returns:</p>\n\n<ul>\n<li><code>confidences</code>: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities</li>\n</ul>\n\n<h2 id=\"step-3-creating-a-gradio-interface\">Step 3 \u2014 Creating a Gradio Interface</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio Interface around it. </p>\n\n<p>In this case, the input component is a drag-and-drop image component. To create this input, we use <code>Image(type=\"pil\")</code> which creates the component and handles the preprocessing to convert that to a <code>PIL</code> image. </p>\n\n<p>The output component will be a <code>Label</code>, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as <code>Label(num_top_classes=3)</code>.</p>\n\n<p>Finally, we'll add one more parameter, the <code>examples</code>, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(fn=predict, \n             inputs=gr.Image(type=\"pil\"),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()\n</code></pre></div>\n\n<p>This produces the following interface, which you can try right here in your browser (try uploading your own examples!):</p>\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting <code>share=True</code> when you <code>launch()</code> the Interface!</p>\n", "tags": ["VISION", "RESNET", "PYTORCH"], "spaces": ["https://huggingface.co/spaces/abidlabs/pytorch-image-classifier", "https://huggingface.co/spaces/pytorch/ResNet", "https://huggingface.co/spaces/pytorch/ResNext", "https://huggingface.co/spaces/pytorch/SqueezeNet"], "url": "/guides/image-classification-in-pytorch/", "contributor": null}, {"name": "image-classification-in-tensorflow", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 18, "pretty_name": "Image Classification In Tensorflow", "content": "# Image Classification in TensorFlow and Keras\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/keras-image-classifier\nTags: VISION, MOBILENET, TENSORFLOW\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging. \n\nSuch models are perfect to use with Gradio's *image* input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained Keras image classification model, so you should also have `tensorflow` installed.\n\n## Step 1 \u2014 Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from [Keras](https://keras.io/api/applications/mobilenet/). You can use a different pretrained model or train your own. \n\n```python\nimport tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n```\n\nThis line automatically downloads the MobileNet model and weights using the Keras library.  \n\n## Step 2 \u2014 Defining a `predict` function\n\nNext, we will need to define a function that takes in the *user input*, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n* `inp`: the input image as a `numpy` array\n\nThen, the function adds a batch dimension, passes it through the model, and returns:\n\n* `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 \u2014 Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. \n\nIn this case, the input component is a drag-and-drop image component. To create this input, we can use the `\"gradio.inputs.Image\"` class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\n\nThe output component will be a `\"label\"`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=classify_image, \n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n\n", "html": "<h1 id=\"image-classification-in-tensorflow-and-keras\">Image Classification in TensorFlow and Keras</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging. </p>\n\n<p>Such models are perfect to use with Gradio's <em>image</em> input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):</p>\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started!</p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. We will be using a pretrained Keras image classification model, so you should also have <code>tensorflow</code> installed.</p>\n\n<h2 id=\"step-1-setting-up-the-image-classification-model\">Step 1 \u2014 Setting up the Image Classification Model</h2>\n\n<p>First, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from <a rel=\"noopener\" target=\"_blank\" href=\"https://keras.io/api/applications/mobilenet/\">Keras</a>. You can use a different pretrained model or train your own. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n</code></pre></div>\n\n<p>This line automatically downloads the MobileNet model and weights using the Keras library.  </p>\n\n<h2 id=\"step-2-defining-a-predict-function\">Step 2 \u2014 Defining a <code>predict</code> function</h2>\n\n<p>Next, we will need to define a function that takes in the <em>user input</em>, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this <a rel=\"noopener\" target=\"_blank\" href=\"https://git.io/JJkYN\">text file</a>.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n</code></pre></div>\n\n<p>Let's break this down. The function takes one parameter:</p>\n\n<ul>\n<li><code>inp</code>: the input image as a <code>numpy</code> array</li>\n</ul>\n\n<p>Then, the function adds a batch dimension, passes it through the model, and returns:</p>\n\n<ul>\n<li><code>confidences</code>: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities</li>\n</ul>\n\n<h2 id=\"step-3-creating-a-gradio-interface\">Step 3 \u2014 Creating a Gradio Interface</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio Interface around it. </p>\n\n<p>In this case, the input component is a drag-and-drop image component. To create this input, we can use the <code>\"gradio.inputs.Image\"</code> class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.</p>\n\n<p>The output component will be a <code>\"label\"</code>, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.</p>\n\n<p>Finally, we'll add one more parameter, the <code>examples</code>, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(fn=classify_image, \n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n</code></pre></div>\n\n<p>This produces the following interface, which you can try right here in your browser (try uploading your own examples!):</p>\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting <code>share=True</code> when you <code>launch()</code> the Interface!</p>\n", "tags": ["VISION", "MOBILENET", "TENSORFLOW"], "spaces": ["https://huggingface.co/spaces/abidlabs/keras-image-classifier"], "url": "/guides/image-classification-in-tensorflow/", "contributor": null}, {"name": "image-classification-with-vision-transformers", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 19, "pretty_name": "Image Classification With Vision Transformers", "content": "# Image Classification with Vision Transformers\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/vision-transformer\nTags: VISION, TRANSFORMERS, HUB\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control. \n\nState-of-the-art image classifiers are based on the *transformers* architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's *image* input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like this (try one of the examples!):\n\n<iframe src=\"https://abidlabs-vision-transformer.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 \u2014 Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks. \n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo. \n\n## Step 2 \u2014 Loading the Vision Transformer Model with Gradio\n\nWhen using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing. \nAll of these are automatically inferred from the model tags.\n\nBesides the import statement, it only takes a single line of Python to load and launch the demo. \n\nWe use the `gr.Interface.load()` method and pass in the path to the model including the  `huggingface/` to designate that it is from the Hugging Face Hub.\n\n```python\nimport gradio as gr\n\ngr.Interface.load(\n             \"huggingface/google/vit-base-patch16-224\",\n             examples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\n```\n\nNotice that we have added one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. \n\nThis produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction.  Try uploading your own image!\n\n<iframe src=\"https://abidlabs-vision-transformer.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! In one line of code, you have built a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n\n", "html": "<h1 id=\"image-classification-with-vision-transformers\">Image Classification with Vision Transformers</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control. </p>\n\n<p>State-of-the-art image classifiers are based on the <em>transformers</em> architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's <em>image</em> input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a <strong>single line of Python</strong>, and it will look like this (try one of the examples!):</p>\n\n<iframe src=\"https://abidlabs-vision-transformer.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started!</p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>.</p>\n\n<h2 id=\"step-1-choosing-a-vision-image-classification-model\">Step 1 \u2014 Choosing a Vision Image Classification Model</h2>\n\n<p>First, we will need an image classification model. For this tutorial, we will use a model from the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/models?pipeline_tag=image-classification\">Hugging Face Model Hub</a>. The Hub contains thousands of models covering dozens of different machine learning tasks. </p>\n\n<p>Expand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.</p>\n\n<p>At the time of writing, the most popular one is <code>google/vit-base-patch16-224</code>, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo. </p>\n\n<h2 id=\"step-2-loading-the-vision-transformer-model-with-gradio\">Step 2 \u2014 Loading the Vision Transformer Model with Gradio</h2>\n\n<p>When using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing. \nAll of these are automatically inferred from the model tags.</p>\n\n<p>Besides the import statement, it only takes a single line of Python to load and launch the demo. </p>\n\n<p>We use the <code>gr.Interface.load()</code> method and pass in the path to the model including the  <code>huggingface/</code> to designate that it is from the Hugging Face Hub.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface.load(\n             \"huggingface/google/vit-base-patch16-224\",\n             examples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\n</code></pre></div>\n\n<p>Notice that we have added one more parameter, the <code>examples</code>, which allows us to prepopulate our interfaces with a few predefined examples. </p>\n\n<p>This produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction.  Try uploading your own image!</p>\n\n<iframe src=\"https://abidlabs-vision-transformer.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! In one line of code, you have built a web demo for an image classifier. If you'd like to share with others, try setting <code>share=True</code> when you <code>launch()</code> the Interface!</p>\n", "tags": ["VISION", "TRANSFORMERS", "HUB"], "spaces": ["https://huggingface.co/spaces/abidlabs/vision-transformer"], "url": "/guides/image-classification-with-vision-transformers/", "contributor": null}, {"name": "building-a-pictionary-app", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 25, "pretty_name": "Building A Pictionary App", "content": "# Building a Pictionary App\n\nRelated spaces: https://huggingface.co/spaces/nateraw/quickdraw\nTags: SKETCHPAD, LABELS, LIVE\n\n## Introduction\n\nHow well can an algorithm guess what you're drawing? A few years ago, Google released the **Quick Draw** dataset, which contains drawings made by humans of a variety of every objects. Researchers have used this dataset to train models to guess Pictionary-style drawings. \n\nSuch models are perfect to use with Gradio's *sketchpad* input, so in this tutorial we will build a Pictionary web application using Gradio. We will be able to build the whole web application in Python, and will look like this (try drawing something!):\n\n<iframe src=\"https://abidlabs-draw2.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nLet's get started! This guide covers how to build a pictionary app (step-by-step): \n\n1. [Set up the Sketch Recognition Model](#1-set-up-the-sketch-recognition-model)\n2. [Define a `predict` function](#2-define-a-predict-function)\n3. [Create a Gradio Interface](#3-create-a-gradio-interface)\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). To use the pretrained sketchpad model, also install `torch`.\n\n## 1. Set up the Sketch Recognition Model\n\nFirst, you will need a sketch recognition model. Since many researchers have already trained their own models on the Quick Draw dataset, we will use a pretrained model in this tutorial. Our model is a light 1.5 MB  model trained by Nate Raw, that [you can download here](https://huggingface.co/spaces/nateraw/quickdraw/blob/main/pytorch_model.bin). \n\nIf you are interested, here [is the code](https://github.com/nateraw/quickdraw-pytorch) that was used to train the model. We will simply load the pretrained model in PyTorch, as follows:\n\n```python\nimport torch\nfrom torch import nn\n\nmodel = nn.Sequential(\n    nn.Conv2d(1, 32, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(32, 64, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(64, 128, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Flatten(),\n    nn.Linear(1152, 256),\n    nn.ReLU(),\n    nn.Linear(256, len(LABELS)),\n)\nstate_dict = torch.load('pytorch_model.bin',    map_location='cpu')\nmodel.load_state_dict(state_dict, strict=False)\nmodel.eval()\n```\n\n## 2. Define a `predict` function\n\nNext, you will need to define a function that takes in the *user input*, which in this case is a sketched image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://huggingface.co/spaces/nateraw/quickdraw/blob/main/class_names.txt).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nfrom pathlib import Path\n\nLABELS = Path('class_names.txt').read_text().splitlines()\n\ndef predict(img):\n    x = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.\n    with torch.no_grad():\n        out = model(x)\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n    values, indices = torch.topk(probabilities, 5)\n    confidences = {LABELS[i]: v.item() for i, v in zip(indices, values)}\n    return confidences\n```\n\nLet's break this down. The function takes one parameters:\n\n* `img`: the input image as a `numpy` array\n\nThen, the function converts the image to a PyTorch `tensor`, passes it through the model, and returns:\n\n* `confidences`: the top five predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## 3. Create a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. \n\nIn this case, the input component is a sketchpad. To create a sketchpad input, we can use the convenient string shortcut, `\"sketchpad\"` which creates a canvas for a user to draw on and handles the preprocessing to convert that to a numpy array. \n\nThe output component will be a `\"label\"`, which displays the top labels in a nice form.\n\nFinally, we'll add one more parameter, setting `live=True`, which allows our interface to run in real time, adjusting its predictions every time a user draws on the sketchpad. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=predict, \n             inputs=\"sketchpad\",\n             outputs=\"label\",\n             live=True).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try drawing something, like a \"snake\" or a \"laptop\"):\n\n<iframe src=\"https://abidlabs-draw2.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! That's all the code you need to build a Pictionary-style guessing app. Have fun and try to find some edge cases \ud83e\uddd0\n\n", "html": "<h1 id=\"building-a-pictionary-app\">Building a Pictionary App</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>How well can an algorithm guess what you're drawing? A few years ago, Google released the <strong>Quick Draw</strong> dataset, which contains drawings made by humans of a variety of every objects. Researchers have used this dataset to train models to guess Pictionary-style drawings. </p>\n\n<p>Such models are perfect to use with Gradio's <em>sketchpad</em> input, so in this tutorial we will build a Pictionary web application using Gradio. We will be able to build the whole web application in Python, and will look like this (try drawing something!):</p>\n\n<iframe src=\"https://abidlabs-draw2.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started! This guide covers how to build a pictionary app (step-by-step): </p>\n\n<ol>\n<li><a href=\"#1-set-up-the-sketch-recognition-model\">Set up the Sketch Recognition Model</a></li>\n<li><a href=\"#2-define-a-predict-function\">Define a <code>predict</code> function</a></li>\n<li><a href=\"#3-create-a-gradio-interface\">Create a Gradio Interface</a></li>\n</ol>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. To use the pretrained sketchpad model, also install <code>torch</code>.</p>\n\n<h2 id=\"1-set-up-the-sketch-recognition-model\">1. Set up the Sketch Recognition Model</h2>\n\n<p>First, you will need a sketch recognition model. Since many researchers have already trained their own models on the Quick Draw dataset, we will use a pretrained model in this tutorial. Our model is a light 1.5 MB  model trained by Nate Raw, that <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/nateraw/quickdraw/blob/main/pytorch_model.bin\">you can download here</a>. </p>\n\n<p>If you are interested, here <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/nateraw/quickdraw-pytorch\">is the code</a> that was used to train the model. We will simply load the pretrained model in PyTorch, as follows:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import torch\nfrom torch import nn\n\nmodel = nn.Sequential(\n    nn.Conv2d(1, 32, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(32, 64, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(64, 128, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Flatten(),\n    nn.Linear(1152, 256),\n    nn.ReLU(),\n    nn.Linear(256, len(LABELS)),\n)\nstate_dict = torch.load('pytorch_model.bin',    map_location='cpu')\nmodel.load_state_dict(state_dict, strict=False)\nmodel.eval()\n</code></pre></div>\n\n<h2 id=\"2-define-a-predict-function\">2. Define a <code>predict</code> function</h2>\n\n<p>Next, you will need to define a function that takes in the <em>user input</em>, which in this case is a sketched image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/nateraw/quickdraw/blob/main/class_names.txt\">text file</a>.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from pathlib import Path\n\nLABELS = Path('class_names.txt').read_text().splitlines()\n\ndef predict(img):\n    x = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.\n    with torch.no_grad():\n        out = model(x)\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n    values, indices = torch.topk(probabilities, 5)\n    confidences = {LABELS[i]: v.item() for i, v in zip(indices, values)}\n    return confidences\n</code></pre></div>\n\n<p>Let's break this down. The function takes one parameters:</p>\n\n<ul>\n<li><code>img</code>: the input image as a <code>numpy</code> array</li>\n</ul>\n\n<p>Then, the function converts the image to a PyTorch <code>tensor</code>, passes it through the model, and returns:</p>\n\n<ul>\n<li><code>confidences</code>: the top five predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities</li>\n</ul>\n\n<h2 id=\"3-create-a-gradio-interface\">3. Create a Gradio Interface</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio Interface around it. </p>\n\n<p>In this case, the input component is a sketchpad. To create a sketchpad input, we can use the convenient string shortcut, <code>\"sketchpad\"</code> which creates a canvas for a user to draw on and handles the preprocessing to convert that to a numpy array. </p>\n\n<p>The output component will be a <code>\"label\"</code>, which displays the top labels in a nice form.</p>\n\n<p>Finally, we'll add one more parameter, setting <code>live=True</code>, which allows our interface to run in real time, adjusting its predictions every time a user draws on the sketchpad. The code for Gradio looks like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(fn=predict, \n             inputs=\"sketchpad\",\n             outputs=\"label\",\n             live=True).launch()\n</code></pre></div>\n\n<p>This produces the following interface, which you can try right here in your browser (try drawing something, like a \"snake\" or a \"laptop\"):</p>\n\n<iframe src=\"https://abidlabs-draw2.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a Pictionary-style guessing app. Have fun and try to find some edge cases \ud83e\uddd0</p>\n", "tags": ["SKETCHPAD", "LABELS", "LIVE"], "spaces": ["https://huggingface.co/spaces/nateraw/quickdraw"], "url": "/guides/building-a-pictionary-app/", "contributor": null}, {"name": "create-your-own-friends-with-a-gan", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 26, "pretty_name": "Create Your Own Friends With A Gan", "content": "# Create Your Own Friends with a GAN\n\nRelated spaces: https://huggingface.co/spaces/NimaBoscarino/cryptopunks, https://huggingface.co/spaces/nateraw/cryptopunks-generator\nTags: GAN, IMAGE, HUB\n\nContributed by <a href=\"https://huggingface.co/NimaBoscarino\">Nima Boscarino</a> and <a href=\"https://huggingface.co/nateraw\">Nate Raw</a>\n\n\n## Introduction\n\nIt seems that cryptocurrencies, [NFTs](https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html), and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets [may be taxable, such as in Canada](https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html), today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated [CryptoPunks](https://www.larvalabs.com/cryptopunks).\n\nGenerative Adversarial Networks, often known just as *GANs*, are a specific class of deep-learning models that are designed to learn from an input dataset to create (*generate!*) new material that is convincingly similar to elements of the original training set. Famously, the website [thispersondoesnotexist.com](https://thispersondoesnotexist.com/) went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even [music](https://salu133445.github.io/musegan/)!\n\nToday we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a peek at what we're going to be putting together:\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). To use the pretrained model, also install `torch` and `torchvision`.\n\n## GANs: a very brief introduction\n\nOriginally proposed in [Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661), GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the *generator*, is responsible for generating images. The other network, the *discriminator*, receives an image at a time from the generator along with a **real** image from the training data set. The discriminator then has to guess: which image is the fake?\n\nThe generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (*adversarial!*) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!\n\nFor a more in-depth look at GANs, you can take a look at [this excellent post on Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/) or this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). For now, though, we'll dive into a demo!\n\n## Step 1 \u2014 Create the Generator model\n\nTo generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:\n\n```python\nfrom torch import nn\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n```\n\nWe're taking the generator from [this repo by @teddykoker](https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90), where you can also see the original discriminator model structure.\n\nAfter instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at [nateraw/cryptopunks-gan](https://huggingface.co/nateraw/cryptopunks-gan):\n\n```python\nfrom huggingface_hub import hf_hub_download\nimport torch\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n```\n\n## Step 2 \u2014 Defining a `predict` function\n\nThe `predict` function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our `predict` function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use `torchvision`'s `save_image` function to save the output of the model as a `png` file, and return the file name:\n\n```python\nfrom torchvision.utils import save_image\n\ndef predict(seed):\n    num_punks = 4\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWe're giving our `predict` function a `seed` parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.\n\n*Note!* Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.\n\n## Step 3 \u2014 Creating a Gradio interface\n\nAt this point you can even run the code you have with `predict(<SOME_NUMBER>)`, and you'll find your freshly generated punks in your file system at `./punks.png`. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:\n\n* Set a slider input so users can choose the \"seed\" value\n* Use an image component for our output to showcase the generated punks\n* Use our `predict()` to take the seed and generate the images\n\nWith `gr.Interface()`, we can define all of that with a single function call:\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nLaunching the interface should present you with something like this:\n\n<iframe src=\"https://nimaboscarino-cryptopunks-1.hf.space\" frameBorder=\"0\" height=\"365\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n## Step 4 \u2014 Even more punks!\n\nGenerating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the `inputs` list that we pass to `gr.Interface`:\n\n```python\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nThe new input will be passed to our `predict()` function, so we have to make some changes to that function to accept a new parameter:\n\n```python\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWhen you relaunch your interface, you should see a second slider that'll let you control the number of punks!\n\n## Step 5 - Polishing it up\n\nYour Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight \u2728\n\nWe can add some examples that users can easily try out by adding this to the `gr.Interface`:\n\n```python\ngr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n```\n\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the `inputs`. So in our case, `[seed, num_punks]`. Give it a try!\n\nYou can also try adding a `title`, `description`, and `article` to the `gr.Interface`. Each of those parameters accepts a string, so try it out and see what happens \ud83d\udc40 `article` will also accept HTML, as [explored in a previous guide](./key_features/#descriptive-content)!\n\nWhen you're all done, you may end up with something like this:\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nFor reference, here is our full code:\n\n```python\nimport torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),\n    ],\n    outputs=\"image\",\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True)\n```\n----------\n\nCongratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can [scour the Hub for more GANs](https://huggingface.co/models?other=gan) (or train your own) and continue making even more awesome demos \ud83e\udd17", "html": "<h1 id=\"create-your-own-friends-with-a-gan\">Create Your Own Friends with a GAN</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>It seems that cryptocurrencies, <a rel=\"noopener\" target=\"_blank\" href=\"https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html\">NFTs</a>, and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets <a rel=\"noopener\" target=\"_blank\" href=\"https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html\">may be taxable, such as in Canada</a>, today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated <a rel=\"noopener\" target=\"_blank\" href=\"https://www.larvalabs.com/cryptopunks\">CryptoPunks</a>.</p>\n\n<p>Generative Adversarial Networks, often known just as <em>GANs</em>, are a specific class of deep-learning models that are designed to learn from an input dataset to create (<em>generate!</em>) new material that is convincingly similar to elements of the original training set. Famously, the website <a rel=\"noopener\" target=\"_blank\" href=\"https://thispersondoesnotexist.com/\">thispersondoesnotexist.com</a> went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even <a rel=\"noopener\" target=\"_blank\" href=\"https://salu133445.github.io/musegan/\">music</a>!</p>\n\n<p>Today we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a peek at what we're going to be putting together:</p>\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. To use the pretrained model, also install <code>torch</code> and <code>torchvision</code>.</p>\n\n<h2 id=\"gans-a-very-brief-introduction\">GANs: a very brief introduction</h2>\n\n<p>Originally proposed in <a rel=\"noopener\" target=\"_blank\" href=\"https://arxiv.org/abs/1406.2661\">Goodfellow et al. 2014</a>, GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the <em>generator</em>, is responsible for generating images. The other network, the <em>discriminator</em>, receives an image at a time from the generator along with a <strong>real</strong> image from the training data set. The discriminator then has to guess: which image is the fake?</p>\n\n<p>The generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (<em>adversarial!</em>) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!</p>\n\n<p>For a more in-depth look at GANs, you can take a look at <a rel=\"noopener\" target=\"_blank\" href=\"https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/\">this excellent post on Analytics Vidhya</a> or this <a rel=\"noopener\" target=\"_blank\" href=\"https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\">PyTorch tutorial</a>. For now, though, we'll dive into a demo!</p>\n\n<h2 id=\"step-1-create-the-generator-model\">Step 1 \u2014 Create the Generator model</h2>\n\n<p>To generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from torch import nn\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n</code></pre></div>\n\n<p>We're taking the generator from <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90\">this repo by @teddykoker</a>, where you can also see the original discriminator model structure.</p>\n\n<p>After instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/nateraw/cryptopunks-gan\">nateraw/cryptopunks-gan</a>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from huggingface_hub import hf_hub_download\nimport torch\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n</code></pre></div>\n\n<h2 id=\"step-2-defining-a-predict-function\">Step 2 \u2014 Defining a <code>predict</code> function</h2>\n\n<p>The <code>predict</code> function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our <code>predict</code> function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use <code>torchvision</code>'s <code>save_image</code> function to save the output of the model as a <code>png</code> file, and return the file name:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from torchvision.utils import save_image\n\ndef predict(seed):\n    num_punks = 4\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n</code></pre></div>\n\n<p>We're giving our <code>predict</code> function a <code>seed</code> parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.</p>\n\n<p><em>Note!</em> Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.</p>\n\n<h2 id=\"step-3-creating-a-gradio-interface\">Step 3 \u2014 Creating a Gradio interface</h2>\n\n<p>At this point you can even run the code you have with <code>predict(&lt;SOME_NUMBER&gt;)</code>, and you'll find your freshly generated punks in your file system at <code>./punks.png</code>. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:</p>\n\n<ul>\n<li>Set a slider input so users can choose the \"seed\" value</li>\n<li>Use an image component for our output to showcase the generated punks</li>\n<li>Use our <code>predict()</code> to take the seed and generate the images</li>\n</ul>\n\n<p>With <code>gr.Interface()</code>, we can define all of that with a single function call:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n</code></pre></div>\n\n<p>Launching the interface should present you with something like this:</p>\n\n<iframe src=\"https://nimaboscarino-cryptopunks-1.hf.space\" frameBorder=\"0\" height=\"365\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"step-4-even-more-punks\">Step 4 \u2014 Even more punks!</h2>\n\n<p>Generating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the <code>inputs</code> list that we pass to <code>gr.Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n</code></pre></div>\n\n<p>The new input will be passed to our <code>predict()</code> function, so we have to make some changes to that function to accept a new parameter:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n</code></pre></div>\n\n<p>When you relaunch your interface, you should see a second slider that'll let you control the number of punks!</p>\n\n<h2 id=\"step-5-polishing-it-up\">Step 5 - Polishing it up</h2>\n\n<p>Your Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight \u2728</p>\n\n<p>We can add some examples that users can easily try out by adding this to the <code>gr.Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n</code></pre></div>\n\n<p>The <code>examples</code> parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the <code>inputs</code>. So in our case, <code>[seed, num_punks]</code>. Give it a try!</p>\n\n<p>You can also try adding a <code>title</code>, <code>description</code>, and <code>article</code> to the <code>gr.Interface</code>. Each of those parameters accepts a string, so try it out and see what happens \ud83d\udc40 <code>article</code> will also accept HTML, as <a rel=\"noopener\" target=\"_blank\" href=\"./key_features/#descriptive-content\">explored in a previous guide</a>!</p>\n\n<p>When you're all done, you may end up with something like this:</p>\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>For reference, here is our full code:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),\n    ],\n    outputs=\"image\",\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True)\n</code></pre></div>\n\n<hr />\n\n<p>Congratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/models?other=gan\">scour the Hub for more GANs</a> (or train your own) and continue making even more awesome demos \ud83e\udd17</p>\n", "tags": ["GAN", "IMAGE", "HUB"], "spaces": ["https://huggingface.co/spaces/NimaBoscarino/cryptopunks", "https://huggingface.co/spaces/nateraw/cryptopunks-generator"], "url": "/guides/create-your-own-friends-with-a-gan/", "contributor": "<a href=\"https://huggingface.co/NimaBoscarino\">Nima Boscarino</a> and <a href=\"https://huggingface.co/nateraw\">Nate Raw</a>"}], "preprocessing": "passes the uploaded image as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >numpy.array</span>, <span class='text-orange-500' style='font-family: monospace; font-size: large;' >PIL.Image</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath depending on `type` -- unless `tool` is `sketch` AND source is one of `upload` or `webcam`. In these cases, a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >dict</span> with keys `image` and `mask` is passed, and the format of the corresponding values depends on `type`.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >numpy.array</span>, <span class='text-orange-500' style='font-family: monospace; font-size: large;' >PIL.Image</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >pathlib.Path</span> filepath to an image and displays the image.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath to a local file that contains the image.", "parent": "gradio", "prev_obj": "HighlightedText", "next_obj": "Interpretation"}, "interpretation": {"class": null, "name": "Interpretation", "description": "Used to create an interpretation widget for a component. <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects a {dict} with keys \"original\" and \"interpretation\".", "guides": "custom-interpretations-with-blocks"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "component", "annotation": "Component", "doc": "Which component to show in the interpretation widget."}, {"name": "visible", "annotation": "bool", "doc": "Whether or not the interpretation is visible.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [], "string_shortcuts": [["Interpretation", "interpretation", "Uses default values"]], "events_list": [], "guides": [{"name": "custom-interpretations-with-blocks", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 29, "pretty_name": "Custom Interpretations With Blocks", "content": "# Custom Machine Learning Interpretations with Blocks\nTags: INTERPRETATION, SENTIMENT ANALYSIS\n\n**Prerequisite**: This Guide requires you to know about Blocks and the interpretation feature of Interfaces.\nMake sure to [read the Guide to Blocks first](https://gradio.app/quickstart/#blocks-more-flexibility-and-control) as well as the\ninterpretation section of the [Advanced Interface Features Guide](/advanced-interface-features#interpreting-your-predictions).\n\n## Introduction\n\nIf you have experience working with the Interface class, then you know that interpreting the prediction of your machine learning model\nis as easy as setting the `interpretation` parameter to either \"default\" or \"shap\".\n\nYou may be wondering if it is possible to add the same interpretation functionality to an app built with the Blocks API.\nNot only is it possible, but the flexibility of Blocks lets you display the interpretation output in ways that are\nimpossible to do with Interfaces!\n\nThis guide will show how to:\n\n1. Recreate the behavior of Interfaces's interpretation feature in a Blocks app.\n2. Customize how interpretations are displayed in a Blocks app.\n\nLet's get started!\n\n## Setting up the Blocks app\n\nLet's build a sentiment classification app with the Blocks API.\nThis app will take text as input and output the probability that this text expresses either negative or positive sentiment.\nWe'll have a single input `Textbox` and a single output `Label` component.\nBelow is the code for the app as well as the app itself.\n\n```python\nimport gradio as gr \nfrom transformers import pipeline\n\nsentiment_classifier = pipeline(\"text-classification\", return_all_scores=True)\n\ndef classifier(text):\n    pred = sentiment_classifier(text)\n    return {p[\"label\"]: p[\"score\"] for p in pred[0]}\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n\n    classify.click(classifier, input_text, label)\ndemo.launch()\n```\n\n<gradio-app space=\"freddyaboulton/sentiment-classification\"> </gradio-app>\n\n## Adding interpretations to the app\n\nOur goal is to present to our users how the words in the input contribute to the model's prediction.\nThis will help our users understand how the model works and also evaluate its effectiveness.\nFor example, we should expect our model to identify the words \"happy\" and \"love\" with positive sentiment - if not it's a sign we made a mistake in training it!\n\nFor each word in the input, we will compute a score of how much the model's prediction of positive sentiment is changed by that word.\nOnce we have those `(word, score)` pairs we can use gradio to visualize them for the user.\n\nThe [shap](https://shap.readthedocs.io/en/stable/index.html) library will help us compute the `(word, score)` pairs and\ngradio will take care of displaying the output to the user.\n\nThe following code computes the `(word, score)` pairs:\n\n```python\ndef interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n    \n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n    # Scores contains (word, score) pairs\n    \n    \n    # Format expected by gr.components.Interpretation\n    return {\"original\": text, \"interpretation\": scores}\n```\n\nNow, all we have to do is add a button that runs this function when clicked.\nTo display the interpretation, we will use `gr.components.Interpretation`.\nThis will color each word in the input either red or blue.\nRed if it contributes to positive sentiment and blue if it contributes to negative sentiment.\nThis is how `Interface` displays the interpretation output for text.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            interpretation = gr.components.Interpretation(input_text)\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, interpretation)\n\ndemo.launch()\n```\n\n<gradio-app space=\"freddyaboulton/sentiment-classification-interpretation\"> </gradio-app>\n\n\n## Customizing how the interpretation is displayed\n\nThe `gr.components.Interpretation` component does a good job of showing how individual words contribute to the sentiment prediction,\nbut what if we also wanted to display the score themselves along with the words?\n\nOne way to do this would be to generate a bar plot where the words are on the horizontal axis and the bar height corresponds\nto the shap score.\n\nWe can do this by modifying our `interpretation_function` to additionally return a matplotlib bar plot.\nWe will display it with the `gr.Plot` component in a separate tab.\n\nThis is how the interpretation function will look:\n```python\ndef interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n\n    scores_desc = sorted(scores, key=lambda t: t[1])[::-1]\n\n    # Filter out empty string added by shap\n    scores_desc = [t for t in scores_desc if t[0] != \"\"]\n\n    fig_m = plt.figure()\n    \n    # Select top 5 words that contribute to positive sentiment\n    plt.bar(x=[s[0] for s in scores_desc[:5]],\n            height=[s[1] for s in scores_desc[:5]])\n    plt.title(\"Top words contributing to positive sentiment\")\n    plt.ylabel(\"Shap Value\")\n    plt.xlabel(\"Word\")\n    return {\"original\": text, \"interpretation\": scores}, fig_m\n```\n\nAnd this is how the app code will look:\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            with gr.Tabs():\n                with gr.TabItem(\"Display interpretation with built-in component\"):\n                    interpretation = gr.components.Interpretation(input_text)\n                with gr.TabItem(\"Display interpretation with plot\"):\n                    interpretation_plot = gr.Plot()\n\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, [interpretation, interpretation_plot])\n\ndemo.launch()\n```\n\nYou can see the demo below!\n\n<gradio-app space=\"freddyaboulton/sentiment-classification-interpretation-tabs\"> </gradio-app>\n\n## Beyond Sentiment Classification\nAlthough we have focused on sentiment classification so far, you can add interpretations to almost any machine learning model.\nThe output must be an `gr.Image` or `gr.Label` but the input can be almost anything (`gr.Number`, `gr.Slider`, `gr.Radio`, `gr.Image`).\n\nHere is a demo built with blocks of interpretations for an image classification model:\n\n<gradio-app space=\"freddyaboulton/image-classification-interpretation-blocks\"> </gradio-app>\n\n\n## Closing remarks\n\nWe did a deep dive \ud83e\udd3f into how interpretations work and how you can add them to your Blocks app.\n\nWe also showed how the Blocks API gives you the power to control how the interpretation is visualized in your app.\n\nAdding interpretations is a helpful way to make your users understand and gain trust in your model.\nNow you have all the tools you need to add them to all of your apps!\n", "html": "<h1 id=\"custom-machine-learning-interpretations-with-blocks\">Custom Machine Learning Interpretations with Blocks</h1>\n\n<p><strong>Prerequisite</strong>: This Guide requires you to know about Blocks and the interpretation feature of Interfaces.\nMake sure to <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/quickstart/#blocks-more-flexibility-and-control\">read the Guide to Blocks first</a> as well as the\ninterpretation section of the <a rel=\"noopener\" target=\"_blank\" href=\"/advanced-interface-features#interpreting-your-predictions\">Advanced Interface Features Guide</a>.</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>If you have experience working with the Interface class, then you know that interpreting the prediction of your machine learning model\nis as easy as setting the <code>interpretation</code> parameter to either \"default\" or \"shap\".</p>\n\n<p>You may be wondering if it is possible to add the same interpretation functionality to an app built with the Blocks API.\nNot only is it possible, but the flexibility of Blocks lets you display the interpretation output in ways that are\nimpossible to do with Interfaces!</p>\n\n<p>This guide will show how to:</p>\n\n<ol>\n<li>Recreate the behavior of Interfaces's interpretation feature in a Blocks app.</li>\n<li>Customize how interpretations are displayed in a Blocks app.</li>\n</ol>\n\n<p>Let's get started!</p>\n\n<h2 id=\"setting-up-the-blocks-app\">Setting up the Blocks app</h2>\n\n<p>Let's build a sentiment classification app with the Blocks API.\nThis app will take text as input and output the probability that this text expresses either negative or positive sentiment.\nWe'll have a single input <code>Textbox</code> and a single output <code>Label</code> component.\nBelow is the code for the app as well as the app itself.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr \nfrom transformers import pipeline\n\nsentiment_classifier = pipeline(\"text-classification\", return_all_scores=True)\n\ndef classifier(text):\n    pred = sentiment_classifier(text)\n    return {p[\"label\"]: p[\"score\"] for p in pred[0]}\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n\n    classify.click(classifier, input_text, label)\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space=\"freddyaboulton/sentiment-classification\"> </gradio-app></p>\n\n<h2 id=\"adding-interpretations-to-the-app\">Adding interpretations to the app</h2>\n\n<p>Our goal is to present to our users how the words in the input contribute to the model's prediction.\nThis will help our users understand how the model works and also evaluate its effectiveness.\nFor example, we should expect our model to identify the words \"happy\" and \"love\" with positive sentiment - if not it's a sign we made a mistake in training it!</p>\n\n<p>For each word in the input, we will compute a score of how much the model's prediction of positive sentiment is changed by that word.\nOnce we have those <code>(word, score)</code> pairs we can use gradio to visualize them for the user.</p>\n\n<p>The <a rel=\"noopener\" target=\"_blank\" href=\"https://shap.readthedocs.io/en/stable/index.html\">shap</a> library will help us compute the <code>(word, score)</code> pairs and\ngradio will take care of displaying the output to the user.</p>\n\n<p>The following code computes the <code>(word, score)</code> pairs:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n\n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n    # Scores contains (word, score) pairs\n\n\n    # Format expected by gr.components.Interpretation\n    return {\"original\": text, \"interpretation\": scores}\n</code></pre></div>\n\n<p>Now, all we have to do is add a button that runs this function when clicked.\nTo display the interpretation, we will use <code>gr.components.Interpretation</code>.\nThis will color each word in the input either red or blue.\nRed if it contributes to positive sentiment and blue if it contributes to negative sentiment.\nThis is how <code>Interface</code> displays the interpretation output for text.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            interpretation = gr.components.Interpretation(input_text)\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, interpretation)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space=\"freddyaboulton/sentiment-classification-interpretation\"> </gradio-app></p>\n\n<h2 id=\"customizing-how-the-interpretation-is-displayed\">Customizing how the interpretation is displayed</h2>\n\n<p>The <code>gr.components.Interpretation</code> component does a good job of showing how individual words contribute to the sentiment prediction,\nbut what if we also wanted to display the score themselves along with the words?</p>\n\n<p>One way to do this would be to generate a bar plot where the words are on the horizontal axis and the bar height corresponds\nto the shap score.</p>\n\n<p>We can do this by modifying our <code>interpretation_function</code> to additionally return a matplotlib bar plot.\nWe will display it with the <code>gr.Plot</code> component in a separate tab.</p>\n\n<p>This is how the interpretation function will look:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n\n    scores_desc = sorted(scores, key=lambda t: t[1])[::-1]\n\n    # Filter out empty string added by shap\n    scores_desc = [t for t in scores_desc if t[0] != \"\"]\n\n    fig_m = plt.figure()\n\n    # Select top 5 words that contribute to positive sentiment\n    plt.bar(x=[s[0] for s in scores_desc[:5]],\n            height=[s[1] for s in scores_desc[:5]])\n    plt.title(\"Top words contributing to positive sentiment\")\n    plt.ylabel(\"Shap Value\")\n    plt.xlabel(\"Word\")\n    return {\"original\": text, \"interpretation\": scores}, fig_m\n</code></pre></div>\n\n<p>And this is how the app code will look:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            with gr.Tabs():\n                with gr.TabItem(\"Display interpretation with built-in component\"):\n                    interpretation = gr.components.Interpretation(input_text)\n                with gr.TabItem(\"Display interpretation with plot\"):\n                    interpretation_plot = gr.Plot()\n\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, [interpretation, interpretation_plot])\n\ndemo.launch()\n</code></pre></div>\n\n<p>You can see the demo below!</p>\n\n<p><gradio-app space=\"freddyaboulton/sentiment-classification-interpretation-tabs\"> </gradio-app></p>\n\n<h2 id=\"beyond-sentiment-classification\">Beyond Sentiment Classification</h2>\n\n<p>Although we have focused on sentiment classification so far, you can add interpretations to almost any machine learning model.\nThe output must be an <code>gr.Image</code> or <code>gr.Label</code> but the input can be almost anything (<code>gr.Number</code>, <code>gr.Slider</code>, <code>gr.Radio</code>, <code>gr.Image</code>).</p>\n\n<p>Here is a demo built with blocks of interpretations for an image classification model:</p>\n\n<p><gradio-app space=\"freddyaboulton/image-classification-interpretation-blocks\"> </gradio-app></p>\n\n<h2 id=\"closing-remarks\">Closing remarks</h2>\n\n<p>We did a deep dive \ud83e\udd3f into how interpretations work and how you can add them to your Blocks app.</p>\n\n<p>We also showed how the Blocks API gives you the power to control how the interpretation is visualized in your app.</p>\n\n<p>Adding interpretations is a helpful way to make your users understand and gain trust in your model.\nNow you have all the tools you need to add them to all of your apps!</p>\n", "tags": ["INTERPRETATION", "SENTIMENT ANALYSIS"], "spaces": [], "url": "/guides/custom-interpretations-with-blocks/", "contributor": null}], "preprocessing": "this component does *not* accept input.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >dict</span> with keys \"original\" and \"interpretation\".", "parent": "gradio", "prev_obj": "Image", "next_obj": "JSON"}, "json": {"class": null, "name": "JSON", "description": "Used to display arbitrary JSON output prettily. <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects a valid JSON {str} -- or a {list} or {dict} that is JSON serializable.", "demos": "zip_to_json, blocks_xray"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | Callable | None", "doc": "Default value. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.JSON"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the JSON component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place the JSON in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.JSON"}], "string_shortcuts": [["JSON", "json", "Uses default values"]], "demos": [["zip_to_json", "from zipfile import ZipFile\n\nimport gradio as gr\n\n\ndef zip_to_json(file_obj):\n    files = []\n    with ZipFile(file_obj.name) as zfile:\n        for zinfo in zfile.infolist():\n            files.append(\n                {\n                    \"name\": zinfo.filename,\n                    \"file_size\": zinfo.file_size,\n                    \"compressed_size\": zinfo.compress_size,\n                }\n            )\n    return files\n\n\ndemo = gr.Interface(zip_to_json, \"file\", \"json\")\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["blocks_xray", "import gradio as gr\nimport random\nimport time\n\n\ndef xray_model(diseases, img):\n    time.sleep(4)\n    return [{disease: random.random() for disease in diseases}]\n\n\ndef ct_model(diseases, img):\n    time.sleep(3)\n    return [{disease: 0.1 for disease in diseases}]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n        \"\"\"\n# Detect Disease From Scan\nWith this model you can lorem ipsum\n- ipsum 1\n- ipsum 2\n\"\"\"\n    )\n    disease = gr.CheckboxGroup(\n        choices=[\"Covid\", \"Malaria\", \"Lung Cancer\"], label=\"Disease to Scan For\"\n    )\n\n    with gr.Tab(\"X-ray\") as x_tab:\n        with gr.Row():\n            xray_scan = gr.Image()\n            xray_results = gr.JSON()\n        xray_run = gr.Button(\"Run\")\n        xray_run.click(\n            xray_model,\n            inputs=[disease, xray_scan],\n            outputs=xray_results,\n            api_name=\"xray_model\"\n        )\n\n    with gr.Tab(\"CT Scan\"):\n        with gr.Row():\n            ct_scan = gr.Image()\n            ct_results = gr.JSON()\n        ct_run = gr.Button(\"Run\")\n        ct_run.click(\n            ct_model,\n            inputs=[disease, ct_scan],\n            outputs=ct_results,\n            api_name=\"ct_model\"\n        )\n\n    upload_btn = gr.Button(\"Upload Results\")\n    upload_btn.click(\n        lambda ct, xr: time.sleep(5),\n        inputs=[ct_results, xray_results],\n        outputs=[],\n    )\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()"], "events": "change()", "preprocessing": "this component does *not* accept input.", "postprocessing": "expects a valid JSON <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> -- or a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >list</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >dict</span> that is JSON serializable.", "parent": "gradio", "prev_obj": "Interpretation", "next_obj": "Label"}, "label": {"class": null, "name": "Label", "description": "Displays a classification label, along with confidence scores of top categories, if provided. <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects a {Dict[str, float]} of classes and confidences, or {str} with just the class or an {int}/{float} for regression outputs, or a {str} path to a .json file containing a json dictionary in the structure produced by Label.postprocess().", "demos": "main_note, titanic_survival", "guides": "Gradio-and-ONNX-on-Hugging-Face, image-classification-in-pytorch, image-classification-in-tensorflow, image-classification-with-vision-transformers, building-a-pictionary-app"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "Dict[str, float] | str | float | Callable | None", "doc": "Default value to show in the component. If a str or number is provided, simply displays the string or number. If a {Dict[str, float]} of classes and confidences is provided, displays the top class on top and the `num_top_classes` below, along with their confidence bars. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "num_top_classes", "annotation": "int | None", "doc": "number of most confident classes to show.", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}, {"name": "color", "annotation": "str | None", "doc": "The background color of the label (either a valid css color name or hexadecimal string).", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Label"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the label component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "container", "annotation": "bool | None", "doc": "If True, will add a container to the label - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Label"}], "string_shortcuts": [["Label", "label", "Uses default values"]], "demos": [["main_note", "from math import log2, pow\nimport os\n\nimport numpy as np\nfrom scipy.fftpack import fft\n\nimport gradio as gr\n\nA4 = 440\nC0 = A4 * pow(2, -4.75)\nname = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n\n\ndef get_pitch(freq):\n    h = round(12 * log2(freq / C0))\n    n = h % 12\n    return name[n]\n\n\ndef main_note(audio):\n    rate, y = audio\n    if len(y.shape) == 2:\n        y = y.T[0]\n    N = len(y)\n    T = 1.0 / rate\n    x = np.linspace(0.0, N * T, N)\n    yf = fft(y)\n    yf2 = 2.0 / N * np.abs(yf[0 : N // 2])\n    xf = np.linspace(0.0, 1.0 / (2.0 * T), N // 2)\n\n    volume_per_pitch = {}\n    total_volume = np.sum(yf2)\n    for freq, volume in zip(xf, yf2):\n        if freq == 0:\n            continue\n        pitch = get_pitch(freq)\n        if pitch not in volume_per_pitch:\n            volume_per_pitch[pitch] = 0\n        volume_per_pitch[pitch] += 1.0 * volume / total_volume\n    volume_per_pitch = {k: float(v) for k, v in volume_per_pitch.items()}\n    return volume_per_pitch\n\n\ndemo = gr.Interface(\n    main_note,\n    gr.Audio(source=\"microphone\"),\n    gr.Label(num_top_classes=4),\n    examples=[\n        [os.path.join(os.path.dirname(__file__),\"audio/recording1.wav\")],\n        [os.path.join(os.path.dirname(__file__),\"audio/cantina.wav\")],\n    ],\n    interpretation=\"default\",\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["titanic_survival", "import os\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nimport gradio as gr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata = pd.read_csv(os.path.join(current_dir, \"files/titanic.csv\"))\n\n\ndef encode_age(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    categories = pd.cut(df.Age, bins, labels=False)\n    df.Age = categories\n    return df\n\n\ndef encode_fare(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    categories = pd.cut(df.Fare, bins, labels=False)\n    df.Fare = categories\n    return df\n\n\ndef encode_df(df):\n    df = encode_age(df)\n    df = encode_fare(df)\n    sex_mapping = {\"male\": 0, \"female\": 1}\n    df = df.replace({\"Sex\": sex_mapping})\n    embark_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n    df = df.replace({\"Embarked\": embark_mapping})\n    df.Embarked = df.Embarked.fillna(0)\n    df[\"Company\"] = 0\n    df.loc[(df[\"SibSp\"] > 0), \"Company\"] = 1\n    df.loc[(df[\"Parch\"] > 0), \"Company\"] = 2\n    df.loc[(df[\"SibSp\"] > 0) & (df[\"Parch\"] > 0), \"Company\"] = 3\n    df = df[\n        [\n            \"PassengerId\",\n            \"Pclass\",\n            \"Sex\",\n            \"Age\",\n            \"Fare\",\n            \"Embarked\",\n            \"Company\",\n            \"Survived\",\n        ]\n    ]\n    return df\n\n\ntrain = encode_df(data)\n\nX_all = train.drop([\"Survived\", \"PassengerId\"], axis=1)\ny_all = train[\"Survived\"]\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=num_test, random_state=23\n)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\n\ndef predict_survival(passenger_class, is_male, age, company, fare, embark_point):\n    if passenger_class is None or embark_point is None:\n        return None\n    df = pd.DataFrame.from_dict(\n        {\n            \"Pclass\": [passenger_class + 1],\n            \"Sex\": [0 if is_male else 1],\n            \"Age\": [age],\n            \"Company\": [\n                (1 if \"Sibling\" in company else 0) + (2 if \"Child\" in company else 0)\n            ],\n            \"Fare\": [fare],\n            \"Embarked\": [embark_point + 1],\n        }\n    )\n    df = encode_age(df)\n    df = encode_fare(df)\n    pred = clf.predict_proba(df)[0]\n    return {\"Perishes\": float(pred[0]), \"Survives\": float(pred[1])}\n\n\ndemo = gr.Interface(\n    predict_survival,\n    [\n        gr.Dropdown([\"first\", \"second\", \"third\"], type=\"index\"),\n        \"checkbox\",\n        gr.Slider(0, 80, value=25),\n        gr.CheckboxGroup([\"Sibling\", \"Child\"], label=\"Travelling with (select all)\"),\n        gr.Number(value=20),\n        gr.Radio([\"S\", \"C\", \"Q\"], type=\"index\"),\n    ],\n    \"label\",\n    examples=[\n        [\"first\", True, 30, [], 50, \"S\"],\n        [\"second\", False, 40, [\"Sibling\", \"Child\"], 10, \"Q\"],\n        [\"third\", True, 30, [\"Child\"], 20, \"S\"],\n    ],\n    interpretation=\"default\",\n    live=True,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()"], "events": "change()", "guides": [{"name": "Gradio-and-ONNX-on-Hugging-Face", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 15, "pretty_name": "Gradio And ONNX On Hugging Face", "content": "# Gradio and ONNX on Hugging Face\n\nRelated spaces: https://huggingface.co/spaces/onnx/EfficientNet-Lite4\nTags: ONNX, SPACES\nContributed by Gradio and the <a href=\"https://onnx.ai/\">ONNX</a> team\n\n## Introduction\n\nIn this Guide, we'll walk you through:\n\n* Introduction of ONNX, ONNX model zoo, Gradio, and Hugging Face Spaces\n* How to setup a Gradio demo for EfficientNet-Lite4\n* How to contribute your own Gradio demos for the ONNX organization on Hugging Face\n\nHere's an example of an ONNX model: try out the EfficientNet-Lite4 demo below.\n\n<iframe src=\"https://onnx-efficientnet-lite4.hf.space\" frameBorder=\"0\" height=\"810\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n## What is the ONNX Model Zoo?\nOpen Neural Network Exchange ([ONNX](https://onnx.ai/)) is an open standard format for representing machine learning models. ONNX is supported by a community of partners who have implemented it in many frameworks and tools. For example, if you have trained a model in TensorFlow or PyTorch, you can convert it to ONNX easily, and from there run it on a variety of devices using an engine/compiler like ONNX Runtime.\n\nThe [ONNX Model Zoo](https://github.com/onnx/models) is a collection of pre-trained, state-of-the-art models in the ONNX format contributed by community members. Accompanying each model are Jupyter notebooks for model training and running inference with the trained model. The notebooks are written in Python and include links to the training dataset as well as references to the original paper that describes the model architecture.\n\n\n## What are Hugging Face Spaces & Gradio?\n\n### Gradio\n\nGradio lets users demo their machine learning models as a web app all in python code. Gradio wraps a python function into a user inferface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\n\nGet started [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces\n\nHugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).\n\n### Hugging Face Models\n\nHugging Face Model Hub also supports ONNX models and ONNX models can be filtered through the [ONNX tag](https://huggingface.co/models?library=onnx&sort=downloads)\n\n## How did Hugging Face help the ONNX Model Zoo?\nThere are a lot of Jupyter notebooks in the ONNX Model Zoo for users to test models. Previously, users needed to download the models themselves and run those notebooks locally for testing. With Hugging Face, the testing process can be much simpler and more user-friendly. Users can easily try certain ONNX Model Zoo model on Hugging Face Spaces and run a quick demo powered by Gradio with ONNX Runtime, all on cloud without downloading anything locally. Note, there are various runtimes for ONNX, e.g., [ONNX Runtime](https://github.com/microsoft/onnxruntime), [MXNet](https://github.com/apache/incubator-mxnet).\n\n## What is the role of ONNX Runtime?\nONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live Gradio demos with ONNX Model Zoo model on Hugging Face possible.\n\nONNX Runtime inference can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. For more information please see the [official website](https://onnxruntime.ai/).\n\n## Setting up a Gradio Demo for EfficientNet-Lite4\n\nEfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite models. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU. To learn more read the [model card](https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4)\n\nHere we walk through setting up a example demo for EfficientNet-Lite4 using Gradio\n\nFirst we import our dependencies and download and load the efficientnet-lite4 model from the onnx model zoo. Then load the labels from the labels_map.txt file. We then setup our preprocessing functions, load the model for inference, and setup the inference function. Finally, the inference function is wrapped into a gradio inferface for a user to interact with. See the full code below.\n\n\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nimport gradio as gr\nfrom huggingface_hub import hf_hub_download\nfrom onnx import hub\nimport onnxruntime as ort\n\n# loads ONNX model from ONNX Model Zoo\nmodel = hub.load(\"efficientnet-lite4\")\n# loads the labels text file\nlabels = json.load(open(\"labels_map.txt\", \"r\"))\n\n# sets image file dimensions to 224x224 by resizing and cropping image from center\ndef pre_process_edgetpu(img, dims):\n    output_height, output_width, _ = dims\n    img = resize_with_aspectratio(img, output_height, output_width, inter_pol=cv2.INTER_LINEAR)\n    img = center_crop(img, output_height, output_width)\n    img = np.asarray(img, dtype='float32')\n    # converts jpg pixel value from [0 - 255] to float array [-1.0 - 1.0]\n    img -= [127.0, 127.0, 127.0]\n    img /= [128.0, 128.0, 128.0]\n    return img\n\n# resizes the image with a proportional scale\ndef resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.INTER_LINEAR):\n    height, width, _ = img.shape\n    new_height = int(100. * out_height / scale)\n    new_width = int(100. * out_width / scale)\n    if height > width:\n        w = new_width\n        h = int(new_height * height / width)\n    else:\n        h = new_height\n        w = int(new_width * width / height)\n    img = cv2.resize(img, (w, h), interpolation=inter_pol)\n    return img\n\n# crops the image around the center based on given height and width\ndef center_crop(img, out_height, out_width):\n    height, width, _ = img.shape\n    left = int((width - out_width) / 2)\n    right = int((width + out_width) / 2)\n    top = int((height - out_height) / 2)\n    bottom = int((height + out_height) / 2)\n    img = img[top:bottom, left:right]\n    return img\n\n\nsess = ort.InferenceSession(model)\n\ndef inference(img):\n  img = cv2.imread(img)\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  \n  img = pre_process_edgetpu(img, (224, 224, 3))\n  \n  img_batch = np.expand_dims(img, axis=0)\n\n  results = sess.run([\"Softmax:0\"], {\"images:0\": img_batch})[0]\n  result = reversed(results[0].argsort()[-5:])\n  resultdic = {}\n  for r in result:\n      resultdic[labels[str(r)]] = float(results[0][r])\n  return resultdic\n  \ntitle = \"EfficientNet-Lite4\"\ndescription = \"EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite model. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU.\"\nexamples = [['catonnx.jpg']]\ngr.Interface(inference, gr.Image(type=\"filepath\"), \"label\", title=title, description=description, examples=examples).launch()\n```\n\n\n## How to contribute Gradio demos on HF spaces using ONNX models\n\n* Add model to the [onnx model zoo](https://github.com/onnx/models/blob/main/.github/PULL_REQUEST_TEMPLATE.md)\n* Create an account on Hugging Face [here](https://huggingface.co/join).\n* See list of models left to add to ONNX organization, please refer to the table with the [Models list](https://github.com/onnx/models#models)\n* Add Gradio Demo under your username, see this [blog post](https://huggingface.co/blog/gradio-spaces) for setting up Gradio Demo on Hugging Face. \n* Request to join ONNX Organization [here](https://huggingface.co/onnx).\n* Once approved transfer model from your username to ONNX organization\n* Add a badge for model in model table, see examples in [Models list](https://github.com/onnx/models#models)\n", "html": "<h1 id=\"gradio-and-onnx-on-hugging-face\">Gradio and ONNX on Hugging Face</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>In this Guide, we'll walk you through:</p>\n\n<ul>\n<li>Introduction of ONNX, ONNX model zoo, Gradio, and Hugging Face Spaces</li>\n<li>How to setup a Gradio demo for EfficientNet-Lite4</li>\n<li>How to contribute your own Gradio demos for the ONNX organization on Hugging Face</li>\n</ul>\n\n<p>Here's an example of an ONNX model: try out the EfficientNet-Lite4 demo below.</p>\n\n<iframe src=\"https://onnx-efficientnet-lite4.hf.space\" frameBorder=\"0\" height=\"810\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"what-is-the-onnx-model-zoo\">What is the ONNX Model Zoo?</h2>\n\n<p>Open Neural Network Exchange (<a rel=\"noopener\" target=\"_blank\" href=\"https://onnx.ai/\">ONNX</a>) is an open standard format for representing machine learning models. ONNX is supported by a community of partners who have implemented it in many frameworks and tools. For example, if you have trained a model in TensorFlow or PyTorch, you can convert it to ONNX easily, and from there run it on a variety of devices using an engine/compiler like ONNX Runtime.</p>\n\n<p>The <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/onnx/models\">ONNX Model Zoo</a> is a collection of pre-trained, state-of-the-art models in the ONNX format contributed by community members. Accompanying each model are Jupyter notebooks for model training and running inference with the trained model. The notebooks are written in Python and include links to the training dataset as well as references to the original paper that describes the model architecture.</p>\n\n<h2 id=\"what-are-hugging-face-spaces-gradio\">What are Hugging Face Spaces &amp; Gradio?</h2>\n\n<h3 id=\"gradio\">Gradio</h3>\n\n<p>Gradio lets users demo their machine learning models as a web app all in python code. Gradio wraps a python function into a user inferface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.</p>\n\n<p>Get started <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/getting_started\">here</a></p>\n\n<h3 id=\"hugging-face-spaces\">Hugging Face Spaces</h3>\n\n<p>Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/launch\">here</a>.</p>\n\n<h3 id=\"hugging-face-models\">Hugging Face Models</h3>\n\n<p>Hugging Face Model Hub also supports ONNX models and ONNX models can be filtered through the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/models?library=onnx&amp;sort=downloads\">ONNX tag</a></p>\n\n<h2 id=\"how-did-hugging-face-help-the-onnx-model-zoo\">How did Hugging Face help the ONNX Model Zoo?</h2>\n\n<p>There are a lot of Jupyter notebooks in the ONNX Model Zoo for users to test models. Previously, users needed to download the models themselves and run those notebooks locally for testing. With Hugging Face, the testing process can be much simpler and more user-friendly. Users can easily try certain ONNX Model Zoo model on Hugging Face Spaces and run a quick demo powered by Gradio with ONNX Runtime, all on cloud without downloading anything locally. Note, there are various runtimes for ONNX, e.g., <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/microsoft/onnxruntime\">ONNX Runtime</a>, <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/apache/incubator-mxnet\">MXNet</a>.</p>\n\n<h2 id=\"what-is-the-role-of-onnx-runtime\">What is the role of ONNX Runtime?</h2>\n\n<p>ONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live Gradio demos with ONNX Model Zoo model on Hugging Face possible.</p>\n\n<p>ONNX Runtime inference can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. For more information please see the <a rel=\"noopener\" target=\"_blank\" href=\"https://onnxruntime.ai/\">official website</a>.</p>\n\n<h2 id=\"setting-up-a-gradio-demo-for-efficientnet-lite4\">Setting up a Gradio Demo for EfficientNet-Lite4</h2>\n\n<p>EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite models. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU. To learn more read the <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4\">model card</a></p>\n\n<p>Here we walk through setting up a example demo for EfficientNet-Lite4 using Gradio</p>\n\n<p>First we import our dependencies and download and load the efficientnet-lite4 model from the onnx model zoo. Then load the labels from the labels_map.txt file. We then setup our preprocessing functions, load the model for inference, and setup the inference function. Finally, the inference function is wrapped into a gradio inferface for a user to interact with. See the full code below.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nimport gradio as gr\nfrom huggingface_hub import hf_hub_download\nfrom onnx import hub\nimport onnxruntime as ort\n\n# loads ONNX model from ONNX Model Zoo\nmodel = hub.load(\"efficientnet-lite4\")\n# loads the labels text file\nlabels = json.load(open(\"labels_map.txt\", \"r\"))\n\n# sets image file dimensions to 224x224 by resizing and cropping image from center\ndef pre_process_edgetpu(img, dims):\n    output_height, output_width, _ = dims\n    img = resize_with_aspectratio(img, output_height, output_width, inter_pol=cv2.INTER_LINEAR)\n    img = center_crop(img, output_height, output_width)\n    img = np.asarray(img, dtype='float32')\n    # converts jpg pixel value from [0 - 255] to float array [-1.0 - 1.0]\n    img -= [127.0, 127.0, 127.0]\n    img /= [128.0, 128.0, 128.0]\n    return img\n\n# resizes the image with a proportional scale\ndef resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.INTER_LINEAR):\n    height, width, _ = img.shape\n    new_height = int(100. * out_height / scale)\n    new_width = int(100. * out_width / scale)\n    if height > width:\n        w = new_width\n        h = int(new_height * height / width)\n    else:\n        h = new_height\n        w = int(new_width * width / height)\n    img = cv2.resize(img, (w, h), interpolation=inter_pol)\n    return img\n\n# crops the image around the center based on given height and width\ndef center_crop(img, out_height, out_width):\n    height, width, _ = img.shape\n    left = int((width - out_width) / 2)\n    right = int((width + out_width) / 2)\n    top = int((height - out_height) / 2)\n    bottom = int((height + out_height) / 2)\n    img = img[top:bottom, left:right]\n    return img\n\n\nsess = ort.InferenceSession(model)\n\ndef inference(img):\n  img = cv2.imread(img)\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n  img = pre_process_edgetpu(img, (224, 224, 3))\n\n  img_batch = np.expand_dims(img, axis=0)\n\n  results = sess.run([\"Softmax:0\"], {\"images:0\": img_batch})[0]\n  result = reversed(results[0].argsort()[-5:])\n  resultdic = {}\n  for r in result:\n      resultdic[labels[str(r)]] = float(results[0][r])\n  return resultdic\n\ntitle = \"EfficientNet-Lite4\"\ndescription = \"EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite model. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU.\"\nexamples = [['catonnx.jpg']]\ngr.Interface(inference, gr.Image(type=\"filepath\"), \"label\", title=title, description=description, examples=examples).launch()\n</code></pre></div>\n\n<h2 id=\"how-to-contribute-gradio-demos-on-hf-spaces-using-onnx-models\">How to contribute Gradio demos on HF spaces using ONNX models</h2>\n\n<ul>\n<li>Add model to the <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/onnx/models/blob/main/.github/PULL_REQUEST_TEMPLATE.md\">onnx model zoo</a></li>\n<li>Create an account on Hugging Face <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/join\">here</a>.</li>\n<li>See list of models left to add to ONNX organization, please refer to the table with the <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/onnx/models#models\">Models list</a></li>\n<li>Add Gradio Demo under your username, see this <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/blog/gradio-spaces\">blog post</a> for setting up Gradio Demo on Hugging Face. </li>\n<li>Request to join ONNX Organization <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/onnx\">here</a>.</li>\n<li>Once approved transfer model from your username to ONNX organization</li>\n<li>Add a badge for model in model table, see examples in <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/onnx/models#models\">Models list</a></li>\n</ul>\n", "tags": ["ONNX", "SPACES"], "spaces": ["https://huggingface.co/spaces/onnx/EfficientNet-Lite4"], "url": "/guides/Gradio-and-ONNX-on-Hugging-Face/", "contributor": "Gradio and the <a href=\"https://onnx.ai/\">ONNX</a> team"}, {"name": "image-classification-in-pytorch", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 17, "pretty_name": "Image Classification In Pytorch", "content": "# Image Classification in PyTorch\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/pytorch-image-classifier, https://huggingface.co/spaces/pytorch/ResNet, https://huggingface.co/spaces/pytorch/ResNext, https://huggingface.co/spaces/pytorch/SqueezeNet\nTags: VISION, RESNET, PYTORCH\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging. \n\nSuch models are perfect to use with Gradio's *image* input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained image classification model, so you should also have `torch` installed.\n\n## Step 1 \u2014 Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from [PyTorch Hub](https://pytorch.org/hub/pytorch_vision_resnet/). You can use a different pretrained model or train your own. \n\n```python\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n```\n\nBecause we will be using the model for inference, we have called the `.eval()` method.\n\n## Step 2 \u2014 Defining a `predict` function\n\nNext, we will need to define a function that takes in the *user input*, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}    \n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n* `inp`: the input image as a `PIL` image\n\nThen, the function converts the image to a PIL Image and then eventually a PyTorch `tensor`, passes it through the model, and returns:\n\n* `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 \u2014 Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. \n\nIn this case, the input component is a drag-and-drop image component. To create this input, we use `Image(type=\"pil\")` which creates the component and handles the preprocessing to convert that to a `PIL` image. \n\nThe output component will be a `Label`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as `Label(num_top_classes=3)`.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=predict, \n             inputs=gr.Image(type=\"pil\"),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n\n", "html": "<h1 id=\"image-classification-in-pytorch\">Image Classification in PyTorch</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging. </p>\n\n<p>Such models are perfect to use with Gradio's <em>image</em> input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):</p>\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started!</p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. We will be using a pretrained image classification model, so you should also have <code>torch</code> installed.</p>\n\n<h2 id=\"step-1-setting-up-the-image-classification-model\">Step 1 \u2014 Setting up the Image Classification Model</h2>\n\n<p>First, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from <a rel=\"noopener\" target=\"_blank\" href=\"https://pytorch.org/hub/pytorch_vision_resnet/\">PyTorch Hub</a>. You can use a different pretrained model or train your own. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import torch\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n</code></pre></div>\n\n<p>Because we will be using the model for inference, we have called the <code>.eval()</code> method.</p>\n\n<h2 id=\"step-2-defining-a-predict-function\">Step 2 \u2014 Defining a <code>predict</code> function</h2>\n\n<p>Next, we will need to define a function that takes in the <em>user input</em>, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this <a rel=\"noopener\" target=\"_blank\" href=\"https://git.io/JJkYN\">text file</a>.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import requests\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}    \n  return confidences\n</code></pre></div>\n\n<p>Let's break this down. The function takes one parameter:</p>\n\n<ul>\n<li><code>inp</code>: the input image as a <code>PIL</code> image</li>\n</ul>\n\n<p>Then, the function converts the image to a PIL Image and then eventually a PyTorch <code>tensor</code>, passes it through the model, and returns:</p>\n\n<ul>\n<li><code>confidences</code>: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities</li>\n</ul>\n\n<h2 id=\"step-3-creating-a-gradio-interface\">Step 3 \u2014 Creating a Gradio Interface</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio Interface around it. </p>\n\n<p>In this case, the input component is a drag-and-drop image component. To create this input, we use <code>Image(type=\"pil\")</code> which creates the component and handles the preprocessing to convert that to a <code>PIL</code> image. </p>\n\n<p>The output component will be a <code>Label</code>, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as <code>Label(num_top_classes=3)</code>.</p>\n\n<p>Finally, we'll add one more parameter, the <code>examples</code>, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(fn=predict, \n             inputs=gr.Image(type=\"pil\"),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()\n</code></pre></div>\n\n<p>This produces the following interface, which you can try right here in your browser (try uploading your own examples!):</p>\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting <code>share=True</code> when you <code>launch()</code> the Interface!</p>\n", "tags": ["VISION", "RESNET", "PYTORCH"], "spaces": ["https://huggingface.co/spaces/abidlabs/pytorch-image-classifier", "https://huggingface.co/spaces/pytorch/ResNet", "https://huggingface.co/spaces/pytorch/ResNext", "https://huggingface.co/spaces/pytorch/SqueezeNet"], "url": "/guides/image-classification-in-pytorch/", "contributor": null}, {"name": "image-classification-in-tensorflow", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 18, "pretty_name": "Image Classification In Tensorflow", "content": "# Image Classification in TensorFlow and Keras\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/keras-image-classifier\nTags: VISION, MOBILENET, TENSORFLOW\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging. \n\nSuch models are perfect to use with Gradio's *image* input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained Keras image classification model, so you should also have `tensorflow` installed.\n\n## Step 1 \u2014 Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from [Keras](https://keras.io/api/applications/mobilenet/). You can use a different pretrained model or train your own. \n\n```python\nimport tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n```\n\nThis line automatically downloads the MobileNet model and weights using the Keras library.  \n\n## Step 2 \u2014 Defining a `predict` function\n\nNext, we will need to define a function that takes in the *user input*, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n* `inp`: the input image as a `numpy` array\n\nThen, the function adds a batch dimension, passes it through the model, and returns:\n\n* `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 \u2014 Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. \n\nIn this case, the input component is a drag-and-drop image component. To create this input, we can use the `\"gradio.inputs.Image\"` class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\n\nThe output component will be a `\"label\"`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=classify_image, \n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n\n", "html": "<h1 id=\"image-classification-in-tensorflow-and-keras\">Image Classification in TensorFlow and Keras</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging. </p>\n\n<p>Such models are perfect to use with Gradio's <em>image</em> input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):</p>\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started!</p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. We will be using a pretrained Keras image classification model, so you should also have <code>tensorflow</code> installed.</p>\n\n<h2 id=\"step-1-setting-up-the-image-classification-model\">Step 1 \u2014 Setting up the Image Classification Model</h2>\n\n<p>First, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from <a rel=\"noopener\" target=\"_blank\" href=\"https://keras.io/api/applications/mobilenet/\">Keras</a>. You can use a different pretrained model or train your own. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n</code></pre></div>\n\n<p>This line automatically downloads the MobileNet model and weights using the Keras library.  </p>\n\n<h2 id=\"step-2-defining-a-predict-function\">Step 2 \u2014 Defining a <code>predict</code> function</h2>\n\n<p>Next, we will need to define a function that takes in the <em>user input</em>, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this <a rel=\"noopener\" target=\"_blank\" href=\"https://git.io/JJkYN\">text file</a>.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n</code></pre></div>\n\n<p>Let's break this down. The function takes one parameter:</p>\n\n<ul>\n<li><code>inp</code>: the input image as a <code>numpy</code> array</li>\n</ul>\n\n<p>Then, the function adds a batch dimension, passes it through the model, and returns:</p>\n\n<ul>\n<li><code>confidences</code>: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities</li>\n</ul>\n\n<h2 id=\"step-3-creating-a-gradio-interface\">Step 3 \u2014 Creating a Gradio Interface</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio Interface around it. </p>\n\n<p>In this case, the input component is a drag-and-drop image component. To create this input, we can use the <code>\"gradio.inputs.Image\"</code> class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.</p>\n\n<p>The output component will be a <code>\"label\"</code>, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.</p>\n\n<p>Finally, we'll add one more parameter, the <code>examples</code>, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(fn=classify_image, \n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n</code></pre></div>\n\n<p>This produces the following interface, which you can try right here in your browser (try uploading your own examples!):</p>\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting <code>share=True</code> when you <code>launch()</code> the Interface!</p>\n", "tags": ["VISION", "MOBILENET", "TENSORFLOW"], "spaces": ["https://huggingface.co/spaces/abidlabs/keras-image-classifier"], "url": "/guides/image-classification-in-tensorflow/", "contributor": null}, {"name": "image-classification-with-vision-transformers", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 19, "pretty_name": "Image Classification With Vision Transformers", "content": "# Image Classification with Vision Transformers\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/vision-transformer\nTags: VISION, TRANSFORMERS, HUB\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control. \n\nState-of-the-art image classifiers are based on the *transformers* architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's *image* input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like this (try one of the examples!):\n\n<iframe src=\"https://abidlabs-vision-transformer.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 \u2014 Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks. \n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo. \n\n## Step 2 \u2014 Loading the Vision Transformer Model with Gradio\n\nWhen using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing. \nAll of these are automatically inferred from the model tags.\n\nBesides the import statement, it only takes a single line of Python to load and launch the demo. \n\nWe use the `gr.Interface.load()` method and pass in the path to the model including the  `huggingface/` to designate that it is from the Hugging Face Hub.\n\n```python\nimport gradio as gr\n\ngr.Interface.load(\n             \"huggingface/google/vit-base-patch16-224\",\n             examples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\n```\n\nNotice that we have added one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. \n\nThis produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction.  Try uploading your own image!\n\n<iframe src=\"https://abidlabs-vision-transformer.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! In one line of code, you have built a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n\n", "html": "<h1 id=\"image-classification-with-vision-transformers\">Image Classification with Vision Transformers</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control. </p>\n\n<p>State-of-the-art image classifiers are based on the <em>transformers</em> architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's <em>image</em> input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a <strong>single line of Python</strong>, and it will look like this (try one of the examples!):</p>\n\n<iframe src=\"https://abidlabs-vision-transformer.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started!</p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>.</p>\n\n<h2 id=\"step-1-choosing-a-vision-image-classification-model\">Step 1 \u2014 Choosing a Vision Image Classification Model</h2>\n\n<p>First, we will need an image classification model. For this tutorial, we will use a model from the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/models?pipeline_tag=image-classification\">Hugging Face Model Hub</a>. The Hub contains thousands of models covering dozens of different machine learning tasks. </p>\n\n<p>Expand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.</p>\n\n<p>At the time of writing, the most popular one is <code>google/vit-base-patch16-224</code>, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo. </p>\n\n<h2 id=\"step-2-loading-the-vision-transformer-model-with-gradio\">Step 2 \u2014 Loading the Vision Transformer Model with Gradio</h2>\n\n<p>When using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing. \nAll of these are automatically inferred from the model tags.</p>\n\n<p>Besides the import statement, it only takes a single line of Python to load and launch the demo. </p>\n\n<p>We use the <code>gr.Interface.load()</code> method and pass in the path to the model including the  <code>huggingface/</code> to designate that it is from the Hugging Face Hub.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface.load(\n             \"huggingface/google/vit-base-patch16-224\",\n             examples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\n</code></pre></div>\n\n<p>Notice that we have added one more parameter, the <code>examples</code>, which allows us to prepopulate our interfaces with a few predefined examples. </p>\n\n<p>This produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction.  Try uploading your own image!</p>\n\n<iframe src=\"https://abidlabs-vision-transformer.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! In one line of code, you have built a web demo for an image classifier. If you'd like to share with others, try setting <code>share=True</code> when you <code>launch()</code> the Interface!</p>\n", "tags": ["VISION", "TRANSFORMERS", "HUB"], "spaces": ["https://huggingface.co/spaces/abidlabs/vision-transformer"], "url": "/guides/image-classification-with-vision-transformers/", "contributor": null}, {"name": "building-a-pictionary-app", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 25, "pretty_name": "Building A Pictionary App", "content": "# Building a Pictionary App\n\nRelated spaces: https://huggingface.co/spaces/nateraw/quickdraw\nTags: SKETCHPAD, LABELS, LIVE\n\n## Introduction\n\nHow well can an algorithm guess what you're drawing? A few years ago, Google released the **Quick Draw** dataset, which contains drawings made by humans of a variety of every objects. Researchers have used this dataset to train models to guess Pictionary-style drawings. \n\nSuch models are perfect to use with Gradio's *sketchpad* input, so in this tutorial we will build a Pictionary web application using Gradio. We will be able to build the whole web application in Python, and will look like this (try drawing something!):\n\n<iframe src=\"https://abidlabs-draw2.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nLet's get started! This guide covers how to build a pictionary app (step-by-step): \n\n1. [Set up the Sketch Recognition Model](#1-set-up-the-sketch-recognition-model)\n2. [Define a `predict` function](#2-define-a-predict-function)\n3. [Create a Gradio Interface](#3-create-a-gradio-interface)\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). To use the pretrained sketchpad model, also install `torch`.\n\n## 1. Set up the Sketch Recognition Model\n\nFirst, you will need a sketch recognition model. Since many researchers have already trained their own models on the Quick Draw dataset, we will use a pretrained model in this tutorial. Our model is a light 1.5 MB  model trained by Nate Raw, that [you can download here](https://huggingface.co/spaces/nateraw/quickdraw/blob/main/pytorch_model.bin). \n\nIf you are interested, here [is the code](https://github.com/nateraw/quickdraw-pytorch) that was used to train the model. We will simply load the pretrained model in PyTorch, as follows:\n\n```python\nimport torch\nfrom torch import nn\n\nmodel = nn.Sequential(\n    nn.Conv2d(1, 32, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(32, 64, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(64, 128, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Flatten(),\n    nn.Linear(1152, 256),\n    nn.ReLU(),\n    nn.Linear(256, len(LABELS)),\n)\nstate_dict = torch.load('pytorch_model.bin',    map_location='cpu')\nmodel.load_state_dict(state_dict, strict=False)\nmodel.eval()\n```\n\n## 2. Define a `predict` function\n\nNext, you will need to define a function that takes in the *user input*, which in this case is a sketched image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://huggingface.co/spaces/nateraw/quickdraw/blob/main/class_names.txt).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nfrom pathlib import Path\n\nLABELS = Path('class_names.txt').read_text().splitlines()\n\ndef predict(img):\n    x = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.\n    with torch.no_grad():\n        out = model(x)\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n    values, indices = torch.topk(probabilities, 5)\n    confidences = {LABELS[i]: v.item() for i, v in zip(indices, values)}\n    return confidences\n```\n\nLet's break this down. The function takes one parameters:\n\n* `img`: the input image as a `numpy` array\n\nThen, the function converts the image to a PyTorch `tensor`, passes it through the model, and returns:\n\n* `confidences`: the top five predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## 3. Create a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. \n\nIn this case, the input component is a sketchpad. To create a sketchpad input, we can use the convenient string shortcut, `\"sketchpad\"` which creates a canvas for a user to draw on and handles the preprocessing to convert that to a numpy array. \n\nThe output component will be a `\"label\"`, which displays the top labels in a nice form.\n\nFinally, we'll add one more parameter, setting `live=True`, which allows our interface to run in real time, adjusting its predictions every time a user draws on the sketchpad. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=predict, \n             inputs=\"sketchpad\",\n             outputs=\"label\",\n             live=True).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try drawing something, like a \"snake\" or a \"laptop\"):\n\n<iframe src=\"https://abidlabs-draw2.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! That's all the code you need to build a Pictionary-style guessing app. Have fun and try to find some edge cases \ud83e\uddd0\n\n", "html": "<h1 id=\"building-a-pictionary-app\">Building a Pictionary App</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>How well can an algorithm guess what you're drawing? A few years ago, Google released the <strong>Quick Draw</strong> dataset, which contains drawings made by humans of a variety of every objects. Researchers have used this dataset to train models to guess Pictionary-style drawings. </p>\n\n<p>Such models are perfect to use with Gradio's <em>sketchpad</em> input, so in this tutorial we will build a Pictionary web application using Gradio. We will be able to build the whole web application in Python, and will look like this (try drawing something!):</p>\n\n<iframe src=\"https://abidlabs-draw2.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started! This guide covers how to build a pictionary app (step-by-step): </p>\n\n<ol>\n<li><a href=\"#1-set-up-the-sketch-recognition-model\">Set up the Sketch Recognition Model</a></li>\n<li><a href=\"#2-define-a-predict-function\">Define a <code>predict</code> function</a></li>\n<li><a href=\"#3-create-a-gradio-interface\">Create a Gradio Interface</a></li>\n</ol>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. To use the pretrained sketchpad model, also install <code>torch</code>.</p>\n\n<h2 id=\"1-set-up-the-sketch-recognition-model\">1. Set up the Sketch Recognition Model</h2>\n\n<p>First, you will need a sketch recognition model. Since many researchers have already trained their own models on the Quick Draw dataset, we will use a pretrained model in this tutorial. Our model is a light 1.5 MB  model trained by Nate Raw, that <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/nateraw/quickdraw/blob/main/pytorch_model.bin\">you can download here</a>. </p>\n\n<p>If you are interested, here <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/nateraw/quickdraw-pytorch\">is the code</a> that was used to train the model. We will simply load the pretrained model in PyTorch, as follows:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import torch\nfrom torch import nn\n\nmodel = nn.Sequential(\n    nn.Conv2d(1, 32, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(32, 64, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(64, 128, 3, padding='same'),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Flatten(),\n    nn.Linear(1152, 256),\n    nn.ReLU(),\n    nn.Linear(256, len(LABELS)),\n)\nstate_dict = torch.load('pytorch_model.bin',    map_location='cpu')\nmodel.load_state_dict(state_dict, strict=False)\nmodel.eval()\n</code></pre></div>\n\n<h2 id=\"2-define-a-predict-function\">2. Define a <code>predict</code> function</h2>\n\n<p>Next, you will need to define a function that takes in the <em>user input</em>, which in this case is a sketched image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/nateraw/quickdraw/blob/main/class_names.txt\">text file</a>.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from pathlib import Path\n\nLABELS = Path('class_names.txt').read_text().splitlines()\n\ndef predict(img):\n    x = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.\n    with torch.no_grad():\n        out = model(x)\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n    values, indices = torch.topk(probabilities, 5)\n    confidences = {LABELS[i]: v.item() for i, v in zip(indices, values)}\n    return confidences\n</code></pre></div>\n\n<p>Let's break this down. The function takes one parameters:</p>\n\n<ul>\n<li><code>img</code>: the input image as a <code>numpy</code> array</li>\n</ul>\n\n<p>Then, the function converts the image to a PyTorch <code>tensor</code>, passes it through the model, and returns:</p>\n\n<ul>\n<li><code>confidences</code>: the top five predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities</li>\n</ul>\n\n<h2 id=\"3-create-a-gradio-interface\">3. Create a Gradio Interface</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio Interface around it. </p>\n\n<p>In this case, the input component is a sketchpad. To create a sketchpad input, we can use the convenient string shortcut, <code>\"sketchpad\"</code> which creates a canvas for a user to draw on and handles the preprocessing to convert that to a numpy array. </p>\n\n<p>The output component will be a <code>\"label\"</code>, which displays the top labels in a nice form.</p>\n\n<p>Finally, we'll add one more parameter, setting <code>live=True</code>, which allows our interface to run in real time, adjusting its predictions every time a user draws on the sketchpad. The code for Gradio looks like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(fn=predict, \n             inputs=\"sketchpad\",\n             outputs=\"label\",\n             live=True).launch()\n</code></pre></div>\n\n<p>This produces the following interface, which you can try right here in your browser (try drawing something, like a \"snake\" or a \"laptop\"):</p>\n\n<iframe src=\"https://abidlabs-draw2.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a Pictionary-style guessing app. Have fun and try to find some edge cases \ud83e\uddd0</p>\n", "tags": ["SKETCHPAD", "LABELS", "LIVE"], "spaces": ["https://huggingface.co/spaces/nateraw/quickdraw"], "url": "/guides/building-a-pictionary-app/", "contributor": null}], "preprocessing": "this component does *not* accept input.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >Dict[str, float]</span> of classes and confidences, or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> with just the class or an <span class='text-orange-500' style='font-family: monospace; font-size: large;' >int</span>/<span class='text-orange-500' style='font-family: monospace; font-size: large;' >float</span> for regression outputs, or a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> path to a .json file containing a json dictionary in the structure produced by Label.postprocess().", "parent": "gradio", "prev_obj": "JSON", "next_obj": "LinePlot"}, "lineplot": {"class": null, "name": "LinePlot", "description": "Create a line plot. <br> <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects a pandas dataframe with the data to plot.", "demos": "native_plots, live_dashboard"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "pd.DataFrame | Callable | None", "doc": "The pandas dataframe containing the data to display in a scatter plot.", "default": "None"}, {"name": "x", "annotation": "str | None", "doc": "Column corresponding to the x axis.", "default": "None"}, {"name": "y", "annotation": "str | None", "doc": "Column corresponding to the y axis.", "default": "None"}, {"name": "color", "annotation": "str | None", "doc": "The column to determine the point color. If the column contains numeric data, gradio will interpolate the column data so that small values correspond to light colors and large values correspond to dark values.", "default": "None"}, {"name": "stroke_dash", "annotation": "str | None", "doc": "The column to determine the symbol used to draw the line, e.g. dashed lines, dashed lines with points.", "default": "None"}, {"name": "overlay_point", "annotation": "bool | None", "doc": "Whether to draw a point on the line for each (x, y) coordinate pair.", "default": "None"}, {"name": "title", "annotation": "str | None", "doc": "The title to display on top of the chart.", "default": "None"}, {"name": "tooltip", "annotation": "List[str] | str | None", "doc": "The column (or list of columns) to display on the tooltip when a user hovers a point on the plot.", "default": "None"}, {"name": "x_title", "annotation": "str | None", "doc": "The title given to the x axis. By default, uses the value of the x parameter.", "default": "None"}, {"name": "y_title", "annotation": "str | None", "doc": "The title given to the y axis. By default, uses the value of the y parameter.", "default": "None"}, {"name": "color_legend_title", "annotation": "str | None", "doc": "The title given to the color legend. By default, uses the value of color parameter.", "default": "None"}, {"name": "stroke_dash_legend_title", "annotation": "str | None", "doc": "The title given to the stroke_dash legend. By default, uses the value of the stroke_dash parameter.", "default": "None"}, {"name": "color_legend_position", "annotation": "str | None", "doc": "The position of the color legend. If the string value 'none' is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.", "default": "None"}, {"name": "stroke_dash_legend_position", "annotation": "str | None", "doc": "The position of the stoke_dash legend. If the string value 'none' is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.", "default": "None"}, {"name": "height", "annotation": "int | None", "doc": "The height of the plot in pixels.", "default": "None"}, {"name": "width", "annotation": "int | None", "doc": "The width of the plot in pixels.", "default": "None"}, {"name": "x_lim", "annotation": "List[int] | None", "doc": "A tuple or list containing the limits for the x-axis, specified as [x_min, x_max].", "default": "None"}, {"name": "y_lim", "annotation": "List[int] | None", "doc": "A tuple of list containing the limits for the y-axis, specified as [y_min, y_max].", "default": "None"}, {"name": "caption", "annotation": "str | None", "doc": "The (optional) caption to display below the plot.", "default": "None"}, {"name": "interactive", "annotation": "bool | None", "doc": "Whether users should be able to interact with the plot by panning or zooming with their mouse or trackpad.", "default": "True"}, {"name": "label", "annotation": "str | None", "doc": "The (optional) label to display on the top left corner of the plot.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "Whether the label should be displayed.", "default": "True"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "Whether the plot should be visible.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "Unique id used for custom css targetting.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.LinePlot"}, {"fn": null, "name": "clear", "description": "This event is triggered when the user clears the component (e.g. image or audio) using the X button for the component. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.LinePlot"}], "string_shortcuts": [["LinePlot", "lineplot", "Uses default values"]], "demos": [["native_plots", "import gradio as gr\n\nfrom scatter_plot_demo import scatter_plot\nfrom line_plot_demo import line_plot\nfrom bar_plot_demo import bar_plot\n\n\nwith gr.Blocks() as demo:\n    with gr.Tabs():\n        with gr.TabItem(\"Scatter Plot\"):\n            scatter_plot.render()\n        with gr.TabItem(\"Line Plot\"):\n            line_plot.render()\n        with gr.TabItem(\"Bar Plot\"):\n            bar_plot.render()\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["live_dashboard", "import math\n\nimport pandas as pd\n\nimport gradio as gr\nimport datetime\nimport numpy as np\n\n\ndef get_time():\n    return datetime.datetime.now()\n\n\nplot_end = 2 * math.pi\n\n\ndef get_plot(period=1):\n    global plot_end\n    x = np.arange(plot_end - 2 * math.pi, plot_end, 0.02)\n    y = np.sin(2 * math.pi * period * x)\n    update = gr.LinePlot.update(\n        value=pd.DataFrame({\"x\": x, \"y\": y}),\n        x=\"x\",\n        y=\"y\",\n        title=\"Plot (updates every second)\",\n        width=600,\n        height=350,\n    )\n    plot_end += 2 * math.pi\n    if plot_end > 1000:\n        plot_end = 2 * math.pi\n    return update\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            c_time2 = gr.Textbox(label=\"Current Time refreshed every second\")\n            gr.Textbox(\n                \"Change the value of the slider to automatically update the plot\",\n                label=\"\",\n            )\n            period = gr.Slider(\n                label=\"Period of plot\", value=1, minimum=0, maximum=10, step=1\n            )\n            plot = gr.LinePlot(show_label=False)\n        with gr.Column():\n            name = gr.Textbox(label=\"Enter your name\")\n            greeting = gr.Textbox(label=\"Greeting\")\n            button = gr.Button(value=\"Greet\")\n            button.click(lambda s: f\"Hello {s}\", name, greeting)\n\n    demo.load(lambda: datetime.datetime.now(), None, c_time2, every=1)\n    dep = demo.load(get_plot, None, plot, every=1)\n    period.change(get_plot, period, plot, every=1, cancels=[dep])\n\nif __name__ == \"__main__\":\n    demo.queue().launch()\n"]], "events_list": ["change()", "clear()"], "events": "change(), clear()", "preprocessing": "this component does *not* accept input.", "postprocessing": "expects a pandas dataframe with the data to plot.", "parent": "gradio", "prev_obj": "Label", "next_obj": "Markdown"}, "markdown": {"class": null, "name": "Markdown", "description": "Used to render arbitrary Markdown output. Can also render latex enclosed by dollar signs. <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects a valid {str} that can be rendered as Markdown.", "demos": "blocks_hello, blocks_kinematics", "guides": "key-features"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | Callable", "doc": "Value to show in Markdown component. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "\"\""}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Markdown"}], "string_shortcuts": [["Markdown", "markdown", "Uses default values"]], "demos": [["blocks_hello", "import gradio as gr\n\ndef welcome(name):\n    return f\"Welcome to Gradio, {name}!\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n    inp.change(welcome, inp, out)\n\nif __name__ == \"__main__\":\n    demo.launch()"], ["blocks_kinematics", "import pandas as pd\nimport numpy as np\n\nimport gradio as gr\n\n\ndef plot(v, a):\n    g = 9.81\n    theta = a / 180 * 3.14\n    tmax = ((2 * v) * np.sin(theta)) / g\n    timemat = tmax * np.linspace(0, 1, 40)\n\n    x = (v * timemat) * np.cos(theta)\n    y = ((v * timemat) * np.sin(theta)) - ((0.5 * g) * (timemat**2))\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n    return df\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n        r\"Let's do some kinematics! Choose the speed and angle to see the trajectory. Remember that the range $R = v_0^2 \\cdot \\frac{\\sin(2\\theta)}{g}$\"\n    )\n\n    with gr.Row():\n        speed = gr.Slider(1, 30, 25, label=\"Speed\")\n        angle = gr.Slider(0, 90, 45, label=\"Angle\")\n    output = gr.LinePlot(\n        x=\"x\",\n        y=\"y\",\n        overlay_point=True,\n        tooltip=[\"x\", \"y\"],\n        x_lim=[0, 100],\n        y_lim=[0, 60],\n        width=350,\n        height=300,\n    )\n    btn = gr.Button(value=\"Run\")\n    btn.click(plot, [speed, angle], output)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()"], "events": "change()", "guides": [{"name": "key-features", "category": "getting-started", "pretty_category": "Getting Started", "guide_index": 2, "absolute_index": 1, "pretty_name": "Key Features", "content": "# Key Features\n\nLet's go through some of the most popular features of Gradio! Here are Gradio's key features: \n\n1. [Adding example inputs](#example-inputs)\n2. [Passing custom error messages](#errors)\n3. [Adding descriptive content](#descriptive-content)\n4. [Setting up flagging](#flagging)\n5. [Preprocessing and postprocessing](#preprocessing-and-postprocessing)\n6. [Styling demos](#styling)\n7. [Queuing users](#queuing)\n8. [Iterative outputs](#iterative-outputs)\n9. [Progress bars](#progress-bars)\n10. [Batch functions](#batch-functions)\n\n## Example Inputs\n\nYou can provide example data that a user can easily load into `Interface`. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you can provide a **nested list** to the `examples=`  keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the [Docs](https://gradio.app/docs#components).\n\n```python\nimport gradio as gr\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        if num2 == 0:\n            raise gr.Error(\"Cannot divide by zero!\")\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\", \n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    examples=[\n        [5, \"add\", 3],\n        [4, \"divide\", 2],\n        [-4, \"multiply\", 2.5],\n        [0, \"subtract\", 1.2],\n    ],\n    title=\"Toy Calculator\",\n    description=\"Here's a sample toy calculator. Allows you to calculate things like $2+2=4$\",\n)\ndemo.launch()\n\n```\n<gradio-app space='gradio/calculator'></gradio-app>\n\nYou can load a large dataset into the examples to browse and interact with the dataset through Gradio. The examples will be automatically paginated (you can configure this through the `examples_per_page` argument of `Interface`). \n\nContinue learning about examples in the [More On Examples](https://gradio.app/more-on-examples) guide.\n\n## Errors\n\nYou wish to pass custom error messages to the user. To do so, raise a `gr.Error(\"custom message\")` to display an error message. If you try to divide by zero in the calculator demo above, a popup modal will display the custom error message. Learn more about Error in the [docs](https://gradio.app/docs#errors).\n\n## Descriptive Content\n\nIn the previous example, you may have noticed the `title=` and `description=` keyword arguments in the `Interface` constructor that helps users understand your app.\n\nThere are three arguments in the `Interface` constructor to specify where this content should go:\n\n* `title`: which accepts text and can display it at the very top of interface, and also becomes the page title.\n* `description`: which accepts text, markdown or HTML and places it right under the title.\n* `article`: which also accepts text, markdown or HTML and places it below the interface.\n\n![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\n\nIf you're using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor.\n\nAnother useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`.\n\n```python\ngr.Number(label='Age')\n```\n\n## Flagging\n\nBy default, an `Interface` will have \"Flag\" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the  `flagging_dir=`  argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.\n\nFor example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- calculator.py\n+-- flagged/\n|   +-- logs.csv\n```\n\n*flagged/logs.csv*\n```csv\nnum1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n```\n\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- sepia.py\n+-- flagged/\n|   +-- logs.csv\n|   +-- im/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n*flagged/logs.csv*\n```csv\nim,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.\n\n## Preprocessing and Postprocessing\n\n![](https://github.com/gradio-app/gradio/blob/main/ui/packages/_website/src/assets/img/dataflow.svg?raw=true)\n\nAs you've seen, Gradio includes components that can handle a variety of different data types, such as images, audio, and video. Most components can be used both as inputs or outputs.\n\nWhen a component is used as an input, Gradio automatically handles the *preprocessing* needed to convert the data from a type sent by the user's browser (such as a base64 representation of a webcam snapshot) to a form that can be accepted by your function (such as a `numpy` array). \n\nSimilarly, when a component is used as an output, Gradio automatically handles the *postprocessing* needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (such as a `Gallery` of images in base64 format).\n\nYou can control the *preprocessing* using the parameters when constructing the image component. For example, here if you instantiate the `Image` component with the following parameters, it will convert the image to the `PIL` type and reshape it to be `(100, 100)` no matter the original size that it was submitted as:\n\n```py\nimg = gradio.Image(shape=(100, 100), type=\"pil\")\n```\n\nIn contrast, here we keep the original size of the image, but invert the colors before converting it to a numpy array:\n\n```py\nimg = gradio.Image(invert_colors=True, type=\"numpy\")\n```\n\nPostprocessing is a lot easier! Gradio automatically recognizes the format of the returned data (e.g. is the `Image` a `numpy` array or a `str` filepath?) and postprocesses it into a format that can be displayed by the browser.\n\nTake a look at the [Docs](https://gradio.app/docs) to see all the preprocessing-related parameters for each Component.\n\n\n## Styling\n\nMany components can be styled through the `style()` method. For example:\n\n```python\nimg = gr.Image(\"lion.jpg\").style(height='24', rounded=False)\n```\n\nTake a look at the [Docs](https://gradio.app/docs) to see all the styling options for each Component.\n\nFor additional styling ability, you can pass any CSS to your app using the `css=` kwarg.\nThe base class for the Gradio app is `gradio-container`, so here's an example that changes the background color of the Gradio app:\n\n```python\nwith gr.Interface(css=\".gradio-container {background-color: red}\") as demo:\n    ...\n```\n\n## Queuing\n\nIf your app expects heavy traffic, use the `queue()` method to control processing rate. This will queue up calls so only a certain number of requests are processed at a single time. Queueing uses websockets, which also prevent network timeouts, so you should use queueing if the inference time of your function is long (> 1min). \n\nWith `Interface`:\n```python\ndemo = gr.Interface(...).queue()\ndemo.launch()\n```\n\nWith `Blocks`:\n```python\nwith gr.Blocks() as demo:\n    #...\ndemo.queue()\ndemo.launch()\n```\n\nYou can control the number of requests processsed at a single time as such:\n\n```python\ndemo.queue(concurrency_count=3)\n```\n\nSee the [Docs on queueing](/docs/#queue) on configuring other queuing parameters.\n\nTo specify only certain functions for queueing in Blocks:\n```python\nwith gr.Blocks() as demo2:\n    num1 = gr.Number()\n    num2 = gr.Number()\n    output = gr.Number()\n    gr.Button(\"Add\").click(\n        lambda a, b: a + b, [num1, num2], output)\n    gr.Button(\"Multiply\").click(\n        lambda a, b: a * b, [num1, num2], output, queue=True)\ndemo2.launch()\n```\n\n## Iterative Outputs\n\nIn some cases, you may want to show a sequence of outputs rather than a single output. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image.\n\nIn such cases, you can supply a **generator** function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single `return` value, a function should `yield` a series of values instead. Usually the `yield` statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:\n\n```python\ndef my_generator(x):\n    for i in range(x):\n        yield i\n```\n\nYou supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image:\n\n```python\nimport gradio as gr\nimport numpy as np\nimport time\n\n# define core fn, which returns a generator {steps} times before returning the image\ndef fake_diffusion(steps):\n    for _ in range(steps):\n        time.sleep(1)\n        image = np.random.random((600, 600, 3))\n        yield image\n    image = \"https://gradio-builds.s3.amazonaws.com/diffusion_image/cute_dog.jpg\"\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion, inputs=gr.Slider(1, 10, 3), outputs=\"image\")\n\n# define queue - required for generators\ndemo.queue()\n\ndemo.launch()\n\n```\n<gradio-app space='gradio/fake_diffusion'></gradio-app>\n\nNote that we've added a `time.sleep(1)` in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).\n\nSupplying a generator into Gradio **requires** you to enable queuing in the underlying Interface or Blocks (see the queuing section above).\n\n## Progress Bars\n\nGradio supports the ability to create a custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a `gradio.Progress` instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the `tqdm()` method of the `Progress` instance to track progress over an iterable, as shown below. Queueing must be enabled for progress updates.\n\n```python\nimport gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\nif __name__ == \"__main__\":\n    demo.queue(concurrency_count=10).launch()\n\n```\n<gradio-app space='gradio/progress_simple'></gradio-app>\n\nIf you use the `tqdm` library, you can even report progress updates automatically from any `tqdm.tqdm` that already exists within your function by setting the default argument as  `gr.Progress(track_tqdm=True)`!\n\n## Batch Functions\n\nGradio supports the ability to pass *batch* functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.\n\nFor example, here is a batched function that takes in two lists of inputs (a list of \nwords and a list of ints), and returns a list of trimmed words as output:\n\n```py\nimport time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])        \n    return [trimmed_words]\n```\n\nThe advantage of using batched functions is that if you enable queuing, the Gradio\nserver can automatically *batch* incoming requests and process them in parallel, \npotentially speeding up your demo. Here's what the Gradio code looks like (notice\nthe `batch=True` and `max_batch_size=16` -- both of these parameters can be passed\ninto event triggers or into the `Interface` class) \n\nWith `Interface`:\n```python\ndemo = gr.Interface(trim_words, [\"textbox\", \"number\"], [\"output\"], \n                    batch=True, max_batch_size=16)\ndemo.queue()\ndemo.launch()\n```\n\n\nWith `Blocks`:\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.queue()\ndemo.launch()\n```\n\nIn the example above, 16 requests could be processed in parallel (for a total inference\ntime of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face `transformers` and `diffusers` models\nwork very naturally with Gradio's batch mode: here's [an example demo using diffusers to\ngenerate images in batches](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\n\nNote: using batch functions with Gradio **requires** you to enable queuing in the underlying Interface or Blocks (see the queuing section above).\n", "html": "<h1 id=\"key-features\">Key Features</h1>\n\n<p>Let's go through some of the most popular features of Gradio! Here are Gradio's key features: </p>\n\n<ol>\n<li><a href=\"#example-inputs\">Adding example inputs</a></li>\n<li><a href=\"#errors\">Passing custom error messages</a></li>\n<li><a href=\"#descriptive-content\">Adding descriptive content</a></li>\n<li><a href=\"#flagging\">Setting up flagging</a></li>\n<li><a href=\"#preprocessing-and-postprocessing\">Preprocessing and postprocessing</a></li>\n<li><a href=\"#styling\">Styling demos</a></li>\n<li><a href=\"#queuing\">Queuing users</a></li>\n<li><a href=\"#iterative-outputs\">Iterative outputs</a></li>\n<li><a href=\"#progress-bars\">Progress bars</a></li>\n<li><a href=\"#batch-functions\">Batch functions</a></li>\n</ol>\n\n<h2 id=\"example-inputs\">Example Inputs</h2>\n\n<p>You can provide example data that a user can easily load into <code>Interface</code>. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you can provide a <strong>nested list</strong> to the <code>examples=</code>  keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#components\">Docs</a>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        if num2 == 0:\n            raise gr.Error(\"Cannot divide by zero!\")\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\", \n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    examples=[\n        [5, \"add\", 3],\n        [4, \"divide\", 2],\n        [-4, \"multiply\", 2.5],\n        [0, \"subtract\", 1.2],\n    ],\n    title=\"Toy Calculator\",\n    description=\"Here's a sample toy calculator. Allows you to calculate things like $2+2=4$\",\n)\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/calculator'></gradio-app></p>\n\n<p>You can load a large dataset into the examples to browse and interact with the dataset through Gradio. The examples will be automatically paginated (you can configure this through the <code>examples_per_page</code> argument of <code>Interface</code>). </p>\n\n<p>Continue learning about examples in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/more-on-examples\">More On Examples</a> guide.</p>\n\n<h2 id=\"errors\">Errors</h2>\n\n<p>You wish to pass custom error messages to the user. To do so, raise a <code>gr.Error(\"custom message\")</code> to display an error message. If you try to divide by zero in the calculator demo above, a popup modal will display the custom error message. Learn more about Error in the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs#errors\">docs</a>.</p>\n\n<h2 id=\"descriptive-content\">Descriptive Content</h2>\n\n<p>In the previous example, you may have noticed the <code>title=</code> and <code>description=</code> keyword arguments in the <code>Interface</code> constructor that helps users understand your app.</p>\n\n<p>There are three arguments in the <code>Interface</code> constructor to specify where this content should go:</p>\n\n<ul>\n<li><code>title</code>: which accepts text and can display it at the very top of interface, and also becomes the page title.</li>\n<li><code>description</code>: which accepts text, markdown or HTML and places it right under the title.</li>\n<li><code>article</code>: which also accepts text, markdown or HTML and places it below the interface.</li>\n</ul>\n\n<p><img src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true\" alt=\"annotated\" /></p>\n\n<p>If you're using the <code>Blocks</code> API instead, you can insert text, markdown, or HTML anywhere using the <code>gr.Markdown(...)</code> or <code>gr.HTML(...)</code> components, with descriptive content inside the <code>Component</code> constructor.</p>\n\n<p>Another useful keyword argument is <code>label=</code>, which is present in every <code>Component</code>. This modifies the label text at the top of each <code>Component</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Number(label='Age')\n</code></pre></div>\n\n<h2 id=\"flagging\">Flagging</h2>\n\n<p>By default, an <code>Interface</code> will have \"Flag\" button. When a user testing your <code>Interface</code> sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the  <code>flagging_dir=</code>  argument to the <code>Interface</code> constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.</p>\n\n<p>For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:</p>\n\n<div class='codeblock'><pre><code class='lang-directory'>+-- calculator.py\n+-- flagged/\n|   +-- logs.csv\n</code></pre></div>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>num1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n</code></pre></div>\n\n<p>With the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:</p>\n\n<div class='codeblock'><pre><code class='lang-directory'>+-- sepia.py\n+-- flagged/\n|   +-- logs.csv\n|   +-- im/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n</code></pre></div>\n\n<p><em>flagged/logs.csv</em></p>\n\n<div class='codeblock'><pre><code class='lang-csv'>im,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n</code></pre></div>\n\n<p>If you wish for the user to provide a reason for flagging, you can pass a list of strings to the <code>flagging_options</code> argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.</p>\n\n<h2 id=\"preprocessing-and-postprocessing\">Preprocessing and Postprocessing</h2>\n\n<p><img src=\"https://github.com/gradio-app/gradio/blob/main/ui/packages/_website/src/assets/img/dataflow.svg?raw=true\" alt=\"\" /></p>\n\n<p>As you've seen, Gradio includes components that can handle a variety of different data types, such as images, audio, and video. Most components can be used both as inputs or outputs.</p>\n\n<p>When a component is used as an input, Gradio automatically handles the <em>preprocessing</em> needed to convert the data from a type sent by the user's browser (such as a base64 representation of a webcam snapshot) to a form that can be accepted by your function (such as a <code>numpy</code> array). </p>\n\n<p>Similarly, when a component is used as an output, Gradio automatically handles the <em>postprocessing</em> needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (such as a <code>Gallery</code> of images in base64 format).</p>\n\n<p>You can control the <em>preprocessing</em> using the parameters when constructing the image component. For example, here if you instantiate the <code>Image</code> component with the following parameters, it will convert the image to the <code>PIL</code> type and reshape it to be <code>(100, 100)</code> no matter the original size that it was submitted as:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>img = gradio.Image(shape=(100, 100), type=\"pil\")\n</code></pre></div>\n\n<p>In contrast, here we keep the original size of the image, but invert the colors before converting it to a numpy array:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>img = gradio.Image(invert_colors=True, type=\"numpy\")\n</code></pre></div>\n\n<p>Postprocessing is a lot easier! Gradio automatically recognizes the format of the returned data (e.g. is the <code>Image</code> a <code>numpy</code> array or a <code>str</code> filepath?) and postprocesses it into a format that can be displayed by the browser.</p>\n\n<p>Take a look at the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs\">Docs</a> to see all the preprocessing-related parameters for each Component.</p>\n\n<h2 id=\"styling\">Styling</h2>\n\n<p>Many components can be styled through the <code>style()</code> method. For example:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>img = gr.Image(\"lion.jpg\").style(height='24', rounded=False)\n</code></pre></div>\n\n<p>Take a look at the <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/docs\">Docs</a> to see all the styling options for each Component.</p>\n\n<p>For additional styling ability, you can pass any CSS to your app using the <code>css=</code> kwarg.\nThe base class for the Gradio app is <code>gradio-container</code>, so here's an example that changes the background color of the Gradio app:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Interface(css=\".gradio-container {background-color: red}\") as demo:\n    ...\n</code></pre></div>\n\n<h2 id=\"queuing\">Queuing</h2>\n\n<p>If your app expects heavy traffic, use the <code>queue()</code> method to control processing rate. This will queue up calls so only a certain number of requests are processed at a single time. Queueing uses websockets, which also prevent network timeouts, so you should use queueing if the inference time of your function is long (&gt; 1min). </p>\n\n<p>With <code>Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo = gr.Interface(...).queue()\ndemo.launch()\n</code></pre></div>\n\n<p>With <code>Blocks</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    #...\ndemo.queue()\ndemo.launch()\n</code></pre></div>\n\n<p>You can control the number of requests processsed at a single time as such:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo.queue(concurrency_count=3)\n</code></pre></div>\n\n<p>See the <a rel=\"noopener\" target=\"_blank\" href=\"/docs/#queue\">Docs on queueing</a> on configuring other queuing parameters.</p>\n\n<p>To specify only certain functions for queueing in Blocks:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo2:\n    num1 = gr.Number()\n    num2 = gr.Number()\n    output = gr.Number()\n    gr.Button(\"Add\").click(\n        lambda a, b: a + b, [num1, num2], output)\n    gr.Button(\"Multiply\").click(\n        lambda a, b: a * b, [num1, num2], output, queue=True)\ndemo2.launch()\n</code></pre></div>\n\n<h2 id=\"iterative-outputs\">Iterative Outputs</h2>\n\n<p>In some cases, you may want to show a sequence of outputs rather than a single output. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image.</p>\n\n<p>In such cases, you can supply a <strong>generator</strong> function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single <code>return</code> value, a function should <code>yield</code> a series of values instead. Usually the <code>yield</code> statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def my_generator(x):\n    for i in range(x):\n        yield i\n</code></pre></div>\n\n<p>You supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nimport numpy as np\nimport time\n\n# define core fn, which returns a generator {steps} times before returning the image\ndef fake_diffusion(steps):\n    for _ in range(steps):\n        time.sleep(1)\n        image = np.random.random((600, 600, 3))\n        yield image\n    image = \"https://gradio-builds.s3.amazonaws.com/diffusion_image/cute_dog.jpg\"\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion, inputs=gr.Slider(1, 10, 3), outputs=\"image\")\n\n# define queue - required for generators\ndemo.queue()\n\ndemo.launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/fake_diffusion'></gradio-app></p>\n\n<p>Note that we've added a <code>time.sleep(1)</code> in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).</p>\n\n<p>Supplying a generator into Gradio <strong>requires</strong> you to enable queuing in the underlying Interface or Blocks (see the queuing section above).</p>\n\n<h2 id=\"progress-bars\">Progress Bars</h2>\n\n<p>Gradio supports the ability to create a custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a <code>gradio.Progress</code> instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the <code>tqdm()</code> method of the <code>Progress</code> instance to track progress over an iterable, as shown below. Queueing must be enabled for progress updates.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\nif __name__ == \"__main__\":\n    demo.queue(concurrency_count=10).launch()\n\n</code></pre></div>\n\n<p><gradio-app space='gradio/progress_simple'></gradio-app></p>\n\n<p>If you use the <code>tqdm</code> library, you can even report progress updates automatically from any <code>tqdm.tqdm</code> that already exists within your function by setting the default argument as  <code>gr.Progress(track_tqdm=True)</code>!</p>\n\n<h2 id=\"batch-functions\">Batch Functions</h2>\n\n<p>Gradio supports the ability to pass <em>batch</em> functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.</p>\n\n<p>For example, here is a batched function that takes in two lists of inputs (a list of \nwords and a list of ints), and returns a list of trimmed words as output:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])        \n    return [trimmed_words]\n</code></pre></div>\n\n<p>The advantage of using batched functions is that if you enable queuing, the Gradio\nserver can automatically <em>batch</em> incoming requests and process them in parallel, \npotentially speeding up your demo. Here's what the Gradio code looks like (notice\nthe <code>batch=True</code> and <code>max_batch_size=16</code> -- both of these parameters can be passed\ninto event triggers or into the <code>Interface</code> class) </p>\n\n<p>With <code>Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>demo = gr.Interface(trim_words, [\"textbox\", \"number\"], [\"output\"], \n                    batch=True, max_batch_size=16)\ndemo.queue()\ndemo.launch()\n</code></pre></div>\n\n<p>With <code>Blocks</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.queue()\ndemo.launch()\n</code></pre></div>\n\n<p>In the example above, 16 requests could be processed in parallel (for a total inference\ntime of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face <code>transformers</code> and <code>diffusers</code> models\nwork very naturally with Gradio's batch mode: here's <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py\">an example demo using diffusers to\ngenerate images in batches</a></p>\n\n<p>Note: using batch functions with Gradio <strong>requires</strong> you to enable queuing in the underlying Interface or Blocks (see the queuing section above).</p>\n", "tags": [], "spaces": [], "url": "/guides/key-features/", "contributor": null}], "preprocessing": "this component does *not* accept input.", "postprocessing": "expects a valid <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> that can be rendered as Markdown.", "parent": "gradio", "prev_obj": "LinePlot", "next_obj": "Model3D"}, "model3d": {"class": null, "name": "Model3D", "description": "Component allows users to upload or view 3D Model files (.obj, .glb, or .gltf). <br>", "tags": {"preprocessing": "This component passes the uploaded file as a {str} filepath.", "postprocessing": "expects function to return a {str} path to a file of type (.obj, glb, or .gltf)", "demos": "model3D", "guides": "how-to-use-3D-model-component"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | Callable | None", "doc": "path to (.obj, glb, or .gltf) file to show in model3D viewer. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "clear_color", "annotation": "List[float] | None", "doc": "background color of scene", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Model3D"}, {"fn": null, "name": "edit", "description": "This event is triggered when the user edits the component (e.g. image) using the built-in editor. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Model3D"}, {"fn": null, "name": "clear", "description": "This event is triggered when the user clears the component (e.g. image or audio) using the X button for the component. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Model3D"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the Model3D component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}], "returns": {}, "example": null, "parent": "gradio.Model3D"}], "string_shortcuts": [["Model3D", "model3d", "Uses default values"]], "demos": [["model3D", "import gradio as gr\nimport os\n\n\ndef load_mesh(mesh_file_name):\n    return mesh_file_name\n\n\ndemo = gr.Interface(\n    fn=load_mesh,\n    inputs=gr.Model3D(),\n    outputs=gr.Model3D(\n            clear_color=[0.0, 0.0, 0.0, 0.0],  label=\"3D Model\"),\n    examples=[\n        [os.path.join(os.path.dirname(__file__), \"files/Bunny.obj\")],\n        [os.path.join(os.path.dirname(__file__), \"files/Duck.glb\")],\n        [os.path.join(os.path.dirname(__file__), \"files/Fox.gltf\")],\n        [os.path.join(os.path.dirname(__file__), \"files/face.obj\")],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()", "edit()", "clear()"], "events": "change(), edit(), clear()", "guides": [{"name": "how-to-use-3D-model-component", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 31, "pretty_name": "How To Use 3D Model Component", "content": "# How to Use the 3D Model Component\n\nRelated spaces: https://huggingface.co/spaces/dawood/Model3D, https://huggingface.co/spaces/radames/PIFu-Clothed-Human-Digitization, https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-obj\nTags: VISION, IMAGE\n\n## Introduction\n\n3D models are becoming more popular in machine learning and make for some of the most fun demos to experiment with. Using `gradio`, you can easily build a demo of your 3D image model and share it with anyone. The Gradio 3D Model component accepts 3 file types including: *.obj*, *.glb*, & *.gltf*.\n\nThis guide will show you how to build a demo for your 3D image model in a few lines of code; like the one below. Play around with 3D object by clicking around, dragging and zooming:\n\n<gradio-app space=\"dawood/Model3D\"> </gradio-app>\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](https://gradio.app/quickstart).\n\n\n## Taking a Look at the Code\n\nLet's take a look at how to create the minimal interface above. The prediction function in this case will just return the original 3D model mesh, but you can change this function to run inference on your machine learning model. We'll take a look at more complex examples below.\n\n```python\nimport gradio as gr\n\ndef load_mesh(mesh_file_name):\n    return mesh_file_name\n\ndemo = gr.Interface(\n    fn=load_mesh,\n    inputs=gr.Model3D(),\n    outputs=gr.Model3D(clear_color=[0.0, 0.0, 0.0, 0.0],  label=\"3D Model\"),\n    examples=[\n        [\"files/Bunny.obj\"],\n        [\"files/Duck.glb\"],\n        [\"files/Fox.gltf\"],\n        [\"files/face.obj\"],\n    ],\n    cache_examples=True,\n)\n\ndemo.launch()\n```\n\nLet's break down the code above:\n\n`load_mesh`: This is our 'prediction' function and for simplicity, this function will take in the 3D model mesh and return it.\n\nCreating the Interface:\n\n* `fn`: the prediction function that is used when the user clicks submit. In our case this is the `load_mesh` function.\n* `inputs`: create a model3D input component. The input expects an uploaded file as a {str} filepath.\n* `outputs`: create a model3D output component. The output component also expects a file as a {str} filepath.\n  * `clear_color`: this is the background color of the 3D model canvas. Expects RGBa values.\n  * `label`: the label that appears on the top left of the component.\n* `examples`: list of 3D model files. The 3D model component can accept *.obj*, *.glb*, & *.gltf* file types.\n* `cache_examples`: saves the predicted output for the examples, to save time on inference.\n\n\n## Exploring mode complex Model3D Demos:\n\nBelow is a demo that uses the DPT model to predict the depth of an image and then uses 3D Point Cloud to create a 3D object. Take a look at the [app.py](https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-obj/blob/main/app.py) file for a peek into the code and the model prediction function.\n<gradio-app space=\"radames/dpt-depth-estimation-3d-obj\"> </gradio-app>\n\nBelow is a demo that uses the PIFu model to convert an image of a clothed human into a 3D digitized model. Take a look at the [spaces.py](https://huggingface.co/spaces/radames/PIFu-Clothed-Human-Digitization/blob/main/PIFu/spaces.py) file for a peek into the code and the model prediction function.\n\n<gradio-app space=\"radames/PIFu-Clothed-Human-Digitization\"> </gradio-app>\n\n----------\n\nAnd you're done! That's all the code you need to build an interface for your Model3D model. Here are some references that you may find useful:\n\n* Gradio's [\"Getting Started\" guide](https://gradio.app/getting_started/)\n* The first [3D Model Demo](https://huggingface.co/spaces/dawood/Model3D) and [complete code](https://huggingface.co/spaces/dawood/Model3D/tree/main) (on Hugging Face Spaces)\n", "html": "<h1 id=\"how-to-use-the-3d-model-component\">How to Use the 3D Model Component</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>3D models are becoming more popular in machine learning and make for some of the most fun demos to experiment with. Using <code>gradio</code>, you can easily build a demo of your 3D image model and share it with anyone. The Gradio 3D Model component accepts 3 file types including: <em>.obj</em>, <em>.glb</em>, &amp; <em>.gltf</em>.</p>\n\n<p>This guide will show you how to build a demo for your 3D image model in a few lines of code; like the one below. Play around with 3D object by clicking around, dragging and zooming:</p>\n\n<p><gradio-app space=\"dawood/Model3D\"> </gradio-app></p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/quickstart\">installed</a>.</p>\n\n<h2 id=\"taking-a-look-at-the-code\">Taking a Look at the Code</h2>\n\n<p>Let's take a look at how to create the minimal interface above. The prediction function in this case will just return the original 3D model mesh, but you can change this function to run inference on your machine learning model. We'll take a look at more complex examples below.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef load_mesh(mesh_file_name):\n    return mesh_file_name\n\ndemo = gr.Interface(\n    fn=load_mesh,\n    inputs=gr.Model3D(),\n    outputs=gr.Model3D(clear_color=[0.0, 0.0, 0.0, 0.0],  label=\"3D Model\"),\n    examples=[\n        [\"files/Bunny.obj\"],\n        [\"files/Duck.glb\"],\n        [\"files/Fox.gltf\"],\n        [\"files/face.obj\"],\n    ],\n    cache_examples=True,\n)\n\ndemo.launch()\n</code></pre></div>\n\n<p>Let's break down the code above:</p>\n\n<p><code>load_mesh</code>: This is our 'prediction' function and for simplicity, this function will take in the 3D model mesh and return it.</p>\n\n<p>Creating the Interface:</p>\n\n<ul>\n<li><code>fn</code>: the prediction function that is used when the user clicks submit. In our case this is the <code>load_mesh</code> function.</li>\n<li><code>inputs</code>: create a model3D input component. The input expects an uploaded file as a {str} filepath.</li>\n<li><code>outputs</code>: create a model3D output component. The output component also expects a file as a {str} filepath.\n<ul>\n<li><code>clear_color</code>: this is the background color of the 3D model canvas. Expects RGBa values.</li>\n<li><code>label</code>: the label that appears on the top left of the component.</li>\n</ul></li>\n<li><code>examples</code>: list of 3D model files. The 3D model component can accept <em>.obj</em>, <em>.glb</em>, &amp; <em>.gltf</em> file types.</li>\n<li><code>cache_examples</code>: saves the predicted output for the examples, to save time on inference.</li>\n</ul>\n\n<h2 id=\"exploring-mode-complex-model3d-demos\">Exploring mode complex Model3D Demos:</h2>\n\n<p>Below is a demo that uses the DPT model to predict the depth of an image and then uses 3D Point Cloud to create a 3D object. Take a look at the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-obj/blob/main/app.py\">app.py</a> file for a peek into the code and the model prediction function.\n<gradio-app space=\"radames/dpt-depth-estimation-3d-obj\"> </gradio-app></p>\n\n<p>Below is a demo that uses the PIFu model to convert an image of a clothed human into a 3D digitized model. Take a look at the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/radames/PIFu-Clothed-Human-Digitization/blob/main/PIFu/spaces.py\">spaces.py</a> file for a peek into the code and the model prediction function.</p>\n\n<p><gradio-app space=\"radames/PIFu-Clothed-Human-Digitization\"> </gradio-app></p>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build an interface for your Model3D model. Here are some references that you may find useful:</p>\n\n<ul>\n<li>Gradio's <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/getting_started/\">\"Getting Started\" guide</a></li>\n<li>The first <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/Model3D\">3D Model Demo</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/Model3D/tree/main\">complete code</a> (on Hugging Face Spaces)</li>\n</ul>\n", "tags": ["VISION", "IMAGE"], "spaces": ["https://huggingface.co/spaces/dawood/Model3D", "https://huggingface.co/spaces/radames/PIFu-Clothed-Human-Digitization", "https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-obj"], "url": "/guides/how-to-use-3D-model-component/", "contributor": null}], "preprocessing": "This component passes the uploaded file as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath.", "postprocessing": "expects function to return a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> path to a file of type (.obj, glb, or .gltf)", "parent": "gradio", "prev_obj": "Markdown", "next_obj": "Number"}, "number": {"class": null, "name": "Number", "description": "Creates a numeric field for user to enter numbers as input or display numeric output. <br>", "tags": {"preprocessing": "passes field value as a {float} or {int} into the function, depending on `precision`.", "postprocessing": "expects an {int} or {float} returned from the function and sets field value to it.", "examples-format": "a {float} or {int} representing the number's value.", "demos": "tax_calculator, titanic_survival, blocks_simple_squares"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "float | Callable | None", "doc": "default value. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, will be editable; if False, editing will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}, {"name": "precision", "annotation": "int | None", "doc": "Precision to round input/output to. If set to 0, will round to nearest integer and convert type to int. If None, no rounding happens.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Number"}, {"fn": null, "name": "submit", "description": "This event is triggered when the user presses the Enter key while the component (e.g. a textbox) is focused. This method can be used when this component is in a Gradio Blocks. <br> <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Number"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place the component in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Number"}], "string_shortcuts": [["Number", "number", "Uses default values"]], "demos": [["tax_calculator", "import gradio as gr\n\ndef tax_calculator(income, marital_status, assets):\n    tax_brackets = [(10, 0), (25, 8), (60, 12), (120, 20), (250, 30)]\n    total_deductible = sum(assets[\"Cost\"])\n    taxable_income = income - total_deductible\n\n    total_tax = 0\n    for bracket, rate in tax_brackets:\n        if taxable_income > bracket:\n            total_tax += (taxable_income - bracket) * rate / 100\n\n    if marital_status == \"Married\":\n        total_tax *= 0.75\n    elif marital_status == \"Divorced\":\n        total_tax *= 0.8\n\n    return round(total_tax)\n\ndemo = gr.Interface(\n    tax_calculator,\n    [\n        \"number\",\n        gr.Radio([\"Single\", \"Married\", \"Divorced\"]),\n        gr.Dataframe(\n            headers=[\"Item\", \"Cost\"],\n            datatype=[\"str\", \"number\"],\n            label=\"Assets Purchased this Year\",\n        ),\n    ],\n    \"number\",\n    examples=[\n        [10000, \"Married\", [[\"Suit\", 5000], [\"Laptop\", 800], [\"Car\", 1800]]],\n        [80000, \"Single\", [[\"Suit\", 800], [\"Watch\", 1800], [\"Car\", 800]]],\n    ],\n)\n\ndemo.launch()\n"], ["titanic_survival", "import os\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nimport gradio as gr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata = pd.read_csv(os.path.join(current_dir, \"files/titanic.csv\"))\n\n\ndef encode_age(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    categories = pd.cut(df.Age, bins, labels=False)\n    df.Age = categories\n    return df\n\n\ndef encode_fare(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    categories = pd.cut(df.Fare, bins, labels=False)\n    df.Fare = categories\n    return df\n\n\ndef encode_df(df):\n    df = encode_age(df)\n    df = encode_fare(df)\n    sex_mapping = {\"male\": 0, \"female\": 1}\n    df = df.replace({\"Sex\": sex_mapping})\n    embark_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n    df = df.replace({\"Embarked\": embark_mapping})\n    df.Embarked = df.Embarked.fillna(0)\n    df[\"Company\"] = 0\n    df.loc[(df[\"SibSp\"] > 0), \"Company\"] = 1\n    df.loc[(df[\"Parch\"] > 0), \"Company\"] = 2\n    df.loc[(df[\"SibSp\"] > 0) & (df[\"Parch\"] > 0), \"Company\"] = 3\n    df = df[\n        [\n            \"PassengerId\",\n            \"Pclass\",\n            \"Sex\",\n            \"Age\",\n            \"Fare\",\n            \"Embarked\",\n            \"Company\",\n            \"Survived\",\n        ]\n    ]\n    return df\n\n\ntrain = encode_df(data)\n\nX_all = train.drop([\"Survived\", \"PassengerId\"], axis=1)\ny_all = train[\"Survived\"]\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=num_test, random_state=23\n)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\n\ndef predict_survival(passenger_class, is_male, age, company, fare, embark_point):\n    if passenger_class is None or embark_point is None:\n        return None\n    df = pd.DataFrame.from_dict(\n        {\n            \"Pclass\": [passenger_class + 1],\n            \"Sex\": [0 if is_male else 1],\n            \"Age\": [age],\n            \"Company\": [\n                (1 if \"Sibling\" in company else 0) + (2 if \"Child\" in company else 0)\n            ],\n            \"Fare\": [fare],\n            \"Embarked\": [embark_point + 1],\n        }\n    )\n    df = encode_age(df)\n    df = encode_fare(df)\n    pred = clf.predict_proba(df)[0]\n    return {\"Perishes\": float(pred[0]), \"Survives\": float(pred[1])}\n\n\ndemo = gr.Interface(\n    predict_survival,\n    [\n        gr.Dropdown([\"first\", \"second\", \"third\"], type=\"index\"),\n        \"checkbox\",\n        gr.Slider(0, 80, value=25),\n        gr.CheckboxGroup([\"Sibling\", \"Child\"], label=\"Travelling with (select all)\"),\n        gr.Number(value=20),\n        gr.Radio([\"S\", \"C\", \"Q\"], type=\"index\"),\n    ],\n    \"label\",\n    examples=[\n        [\"first\", True, 30, [], 50, \"S\"],\n        [\"second\", False, 40, [\"Sibling\", \"Child\"], 10, \"Q\"],\n        [\"third\", True, 30, [\"Child\"], 20, \"S\"],\n    ],\n    interpretation=\"default\",\n    live=True,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["blocks_simple_squares", "import gradio as gr\n\ndemo = gr.Blocks(css=\"#btn {color: red}\")\n\nwith demo:\n    default_json = {\"a\": \"a\"}\n\n    num = gr.State(value=0)\n    squared = gr.Number(value=0)\n    btn = gr.Button(\"Next Square\", elem_id=\"btn\").style(rounded=False)\n\n    stats = gr.State(value=default_json)\n    table = gr.JSON()\n\n    def increase(var, stats_history):\n        var += 1\n        stats_history[str(var)] = var**2\n        return var, var**2, stats_history, stats_history\n\n    btn.click(increase, [num, stats], [num, squared, stats, table])\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()", "submit()", "blur()"], "events": "change(), submit(), blur()", "preprocessing": "passes field value as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >float</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >int</span> into the function, depending on `precision`.", "postprocessing": "expects an <span class='text-orange-500' style='font-family: monospace; font-size: large;' >int</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >float</span> returned from the function and sets field value to it.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >float</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >int</span> representing the number's value.", "parent": "gradio", "prev_obj": "Model3D", "next_obj": "Plot"}, "plot": {"class": null, "name": "Plot", "description": "Used to display various kinds of plots (matplotlib, plotly, or bokeh are supported) <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects either a {matplotlib.figure.Figure}, a {plotly.graph_objects._figure.Figure}, or a {dict} corresponding to a bokeh plot (json_item format)", "demos": "altair_plot, outbreak_forecast, blocks_kinematics, stock_forecast, map_airbnb", "guides": "plot-component-for-maps"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "Callable | None | pd.DataFrame", "doc": "Optionally, supply a default plot object to display, must be a matplotlib, plotly, altair, or bokeh figure, or a callable. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Plot"}, {"fn": null, "name": "clear", "description": "This event is triggered when the user clears the component (e.g. image or audio) using the X button for the component. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Plot"}], "string_shortcuts": [["Plot", "plot", "Uses default values"]], "demos": [["altair_plot", "import altair as alt\nimport gradio as gr\nimport numpy as np\nimport pandas as pd\nfrom vega_datasets import data\n\n\ndef make_plot(plot_type):\n    if plot_type == \"scatter_plot\":\n        cars = data.cars()\n        return alt.Chart(cars).mark_point().encode(\n            x='Horsepower',\n            y='Miles_per_Gallon',\n            color='Origin',\n        )\n    elif plot_type == \"heatmap\":\n        # Compute x^2 + y^2 across a 2D grid\n        x, y = np.meshgrid(range(-5, 5), range(-5, 5))\n        z = x ** 2 + y ** 2\n\n        # Convert this grid to columnar data expected by Altair\n        source = pd.DataFrame({'x': x.ravel(),\n                            'y': y.ravel(),\n                            'z': z.ravel()})\n        return alt.Chart(source).mark_rect().encode(\n            x='x:O',\n            y='y:O',\n            color='z:Q'\n        )\n    elif plot_type == \"us_map\":\n        states = alt.topo_feature(data.us_10m.url, 'states')\n        source = data.income.url\n\n        return alt.Chart(source).mark_geoshape().encode(\n            shape='geo:G',\n            color='pct:Q',\n            tooltip=['name:N', 'pct:Q'],\n            facet=alt.Facet('group:N', columns=2),\n        ).transform_lookup(\n            lookup='id',\n            from_=alt.LookupData(data=states, key='id'),\n            as_='geo'\n        ).properties(\n            width=300,\n            height=175,\n        ).project(\n            type='albersUsa'\n        )\n    elif plot_type == \"interactive_barplot\":\n        source = data.movies.url\n\n        pts = alt.selection(type=\"single\", encodings=['x'])\n\n        rect = alt.Chart(data.movies.url).mark_rect().encode(\n            alt.X('IMDB_Rating:Q', bin=True),\n            alt.Y('Rotten_Tomatoes_Rating:Q', bin=True),\n            alt.Color('count()',\n                scale=alt.Scale(scheme='greenblue'),\n                legend=alt.Legend(title='Total Records')\n            )\n        )\n\n        circ = rect.mark_point().encode(\n            alt.ColorValue('grey'),\n            alt.Size('count()',\n                legend=alt.Legend(title='Records in Selection')\n            )\n        ).transform_filter(\n            pts\n        )\n\n        bar = alt.Chart(source).mark_bar().encode(\n            x='Major_Genre:N',\n            y='count()',\n            color=alt.condition(pts, alt.ColorValue(\"steelblue\"), alt.ColorValue(\"grey\"))\n        ).properties(\n            width=550,\n            height=200\n        ).add_selection(pts)\n\n        plot = alt.vconcat(\n            rect + circ,\n            bar\n        ).resolve_legend(\n            color=\"independent\",\n            size=\"independent\"\n        )\n        return plot\n    elif plot_type == \"radial\":\n        source = pd.DataFrame({\"values\": [12, 23, 47, 6, 52, 19]})\n\n        base = alt.Chart(source).encode(\n            theta=alt.Theta(\"values:Q\", stack=True),\n            radius=alt.Radius(\"values\", scale=alt.Scale(type=\"sqrt\", zero=True, rangeMin=20)),\n            color=\"values:N\",\n        )\n\n        c1 = base.mark_arc(innerRadius=20, stroke=\"#fff\")\n\n        c2 = base.mark_text(radiusOffset=10).encode(text=\"values:Q\")\n\n        return c1 + c2\n    elif plot_type == \"multiline\":\n        source = data.stocks()\n\n        highlight = alt.selection(type='single', on='mouseover',\n                                fields=['symbol'], nearest=True)\n\n        base = alt.Chart(source).encode(\n            x='date:T',\n            y='price:Q',\n            color='symbol:N'\n        )\n\n        points = base.mark_circle().encode(\n            opacity=alt.value(0)\n        ).add_selection(\n            highlight\n        ).properties(\n            width=600\n        )\n\n        lines = base.mark_line().encode(\n            size=alt.condition(~highlight, alt.value(1), alt.value(3))\n        )\n\n        return points + lines\n\n\nwith gr.Blocks() as demo:\n    button = gr.Radio(label=\"Plot type\",\n                      choices=['scatter_plot', 'heatmap', 'us_map',\n                               'interactive_barplot', \"radial\", \"multiline\"], value='scatter_plot')\n    plot = gr.Plot(label=\"Plot\")\n    button.change(make_plot, inputs=button, outputs=[plot])\n    demo.load(make_plot, inputs=[button], outputs=[plot])\n\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["outbreak_forecast", "import altair\n\nimport gradio as gr\nfrom math import sqrt\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.express as px\nimport pandas as pd\n\n\ndef outbreak(plot_type, r, month, countries, social_distancing):\n    months = [\"January\", \"February\", \"March\", \"April\", \"May\"]\n    m = months.index(month)\n    start_day = 30 * m\n    final_day = 30 * (m + 1)\n    x = np.arange(start_day, final_day + 1)\n    pop_count = {\"USA\": 350, \"Canada\": 40, \"Mexico\": 300, \"UK\": 120}\n    if social_distancing:\n        r = sqrt(r)\n    df = pd.DataFrame({\"day\": x})\n    for country in countries:\n        df[country] = x ** (r) * (pop_count[country] + 1)\n\n    if plot_type == \"Matplotlib\":\n        fig = plt.figure()\n        plt.plot(df[\"day\"], df[countries].to_numpy())\n        plt.title(\"Outbreak in \" + month)\n        plt.ylabel(\"Cases\")\n        plt.xlabel(\"Days since Day 0\")\n        plt.legend(countries)\n        return fig\n    elif plot_type == \"Plotly\":\n        fig = px.line(df, x=\"day\", y=countries)\n        fig.update_layout(\n            title=\"Outbreak in \" + month,\n            xaxis_title=\"Cases\",\n            yaxis_title=\"Days Since Day 0\",\n        )\n        return fig\n    elif plot_type == \"Altair\":\n        df = df.melt(id_vars=\"day\").rename(columns={\"variable\": \"country\"})\n        fig = altair.Chart(df).mark_line().encode(x=\"day\", y='value', color='country')\n        return fig\n    else:\n        raise ValueError(\"A plot type must be selected\")\n\n\ninputs = [\n    gr.Dropdown([\"Matplotlib\", \"Plotly\", \"Altair\"], label=\"Plot Type\"),\n    gr.Slider(1, 4, 3.2, label=\"R\"),\n    gr.Dropdown([\"January\", \"February\", \"March\", \"April\", \"May\"], label=\"Month\"),\n    gr.CheckboxGroup(\n        [\"USA\", \"Canada\", \"Mexico\", \"UK\"], label=\"Countries\", value=[\"USA\", \"Canada\"]\n    ),\n    gr.Checkbox(label=\"Social Distancing?\"),\n]\noutputs = gr.Plot()\n\ndemo = gr.Interface(\n    fn=outbreak,\n    inputs=inputs,\n    outputs=outputs,\n    examples=[\n        [\"Matplotlib\", 2, \"March\", [\"Mexico\", \"UK\"], True],\n        [\"Altair\", 2, \"March\", [\"Mexico\", \"Canada\"], True],\n        [\"Plotly\", 3.6, \"February\", [\"Canada\", \"Mexico\", \"UK\"], False],\n    ],\n    cache_examples=True,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["blocks_kinematics", "import pandas as pd\nimport numpy as np\n\nimport gradio as gr\n\n\ndef plot(v, a):\n    g = 9.81\n    theta = a / 180 * 3.14\n    tmax = ((2 * v) * np.sin(theta)) / g\n    timemat = tmax * np.linspace(0, 1, 40)\n\n    x = (v * timemat) * np.cos(theta)\n    y = ((v * timemat) * np.sin(theta)) - ((0.5 * g) * (timemat**2))\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n    return df\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n        r\"Let's do some kinematics! Choose the speed and angle to see the trajectory. Remember that the range $R = v_0^2 \\cdot \\frac{\\sin(2\\theta)}{g}$\"\n    )\n\n    with gr.Row():\n        speed = gr.Slider(1, 30, 25, label=\"Speed\")\n        angle = gr.Slider(0, 90, 45, label=\"Angle\")\n    output = gr.LinePlot(\n        x=\"x\",\n        y=\"y\",\n        overlay_point=True,\n        tooltip=[\"x\", \"y\"],\n        x_lim=[0, 100],\n        y_lim=[0, 60],\n        width=350,\n        height=300,\n    )\n    btn = gr.Button(value=\"Run\")\n    btn.click(plot, [speed, angle], output)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["stock_forecast", "import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport gradio as gr\n\n\ndef plot_forecast(final_year, companies, noise, show_legend, point_style):\n    start_year = 2020\n    x = np.arange(start_year, final_year + 1)\n    year_count = x.shape[0]\n    plt_format = ({\"cross\": \"X\", \"line\": \"-\", \"circle\": \"o--\"})[point_style]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    for i, company in enumerate(companies):\n        series = np.arange(0, year_count, dtype=float)\n        series = series**2 * (i + 1)\n        series += np.random.rand(year_count) * noise\n        ax.plot(x, series, plt_format)\n    if show_legend:\n        plt.legend(companies)\n    return fig\n\n\ndemo = gr.Interface(\n    plot_forecast,\n    [\n        gr.Radio([2025, 2030, 2035, 2040], label=\"Project to:\"),\n        gr.CheckboxGroup([\"Google\", \"Microsoft\", \"Gradio\"], label=\"Company Selection\"),\n        gr.Slider(1, 100, label=\"Noise Level\"),\n        gr.Checkbox(label=\"Show Legend\"),\n        gr.Dropdown([\"cross\", \"line\", \"circle\"], label=\"Style\"),\n    ],\n    gr.Plot(label=\"forecast\"),\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["map_airbnb", "import gradio as gr\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\ndf = dataset.to_pandas()\n\ndef filter_map(min_price, max_price, boroughs):\n\n    filtered_df = df[(df['neighbourhood_group'].isin(boroughs)) & \n          (df['price'] > min_price) & (df['price'] < max_price)]\n    names = filtered_df[\"name\"].tolist()\n    prices = filtered_df[\"price\"].tolist()\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\n    fig = go.Figure(go.Scattermapbox(\n            customdata=text_list,\n            lat=filtered_df['latitude'].tolist(),\n            lon=filtered_df['longitude'].tolist(),\n            mode='markers',\n            marker=go.scattermapbox.Marker(\n                size=6\n            ),\n            hoverinfo=\"text\",\n            hovertemplate='<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}'\n        ))\n\n    fig.update_layout(\n        mapbox_style=\"open-street-map\",\n        hovermode='closest',\n        mapbox=dict(\n            bearing=0,\n            center=go.layout.mapbox.Center(\n                lat=40.67,\n                lon=-73.90\n            ),\n            pitch=0,\n            zoom=9\n        ),\n    )\n\n    return fig\n\nwith gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\n        btn = gr.Button(value=\"Update Filter\")\n        map = gr.Plot().style()\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\n\nif __name__ == \"__main__\":\n    demo.launch()"]], "events_list": ["change()", "clear()"], "events": "change(), clear()", "guides": [{"name": "plot-component-for-maps", "category": "tabular-data-science-and-plots", "pretty_category": "Tabular Data Science And Plots", "guide_index": null, "absolute_index": 23, "pretty_name": "Plot Component For Maps", "content": "# How to Use the Plot Component for Maps\n\nRelated spaces: \nTags: PLOTS, MAPS\n\n## Introduction\n\nThis guide explains how you can use Gradio to plot geographical data on a map using the `gradio.Plot` component. The Gradio `Plot` component works with Matplotlib, Bokeh and Plotly. Plotly is what we will be working with in this guide. Plotly allows developers to easily create all sorts of maps with their geographical data. Take a look [here](https://plotly.com/python/maps/) for some examples.\n\n## Overview \n    \nWe will be using the New York City Airbnb dataset, which is hosted on kaggle [here](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data). I've uploaded it to the Hugging Face Hub as a dataset [here](https://huggingface.co/datasets/gradio/NYC-Airbnb-Open-Data) for easier use and download. Using this data we will plot Airbnb locations on a map output and allow filtering based on price and location. Below is the demo that we will be building. \u26a1\ufe0f\n\n<gradio-app space='gradio/map_airbnb'></gradio-app>\n\n## Step 1 - Loading CSV data \ud83d\udcbe\n\nLet's start by loading the Airbnb NYC data from the Hugging Face Hub.\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\ndf = dataset.to_pandas()\n\ndef filter_map(min_price, max_price, boroughs):\n    new_df = df[(df['neighbourhood_group'].isin(boroughs)) & \n            (df['price'] > min_price) & (df['price'] < max_price)]\n    names = new_df[\"name\"].tolist()\n    prices = new_df[\"price\"].tolist()\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\n```\n\nIn the code above, we first load the csv data into a pandas dataframe. Let's begin by defining a function that we will use as the prediction function for the gradio app. This function will accept the minimum price and maximum price range as well as the list of boroughs to filter the resulting map. We can use the passed in values (`min_price`, `max_price`, and list of `boroughs`) to filter the dataframe and create `new_df`. Next we will create `text_list` of the names and prices of each Airbnb to use as labels on the map.\n \n## Step 2 - Map Figure \ud83c\udf10\n\nPlotly makes it easy to work with maps. Let's take a look below how we can create a map figure.\n\n```python\nimport plotly.graph_objects as go\n\nfig = go.Figure(go.Scattermapbox(\n            customdata=text_list,\n            lat=new_df['latitude'].tolist(),\n            lon=new_df['longitude'].tolist(),\n            mode='markers',\n            marker=go.scattermapbox.Marker(\n                size=6\n            ),\n            hoverinfo=\"text\",\n            hovertemplate='<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}'\n        ))\n\nfig.update_layout(\n    mapbox_style=\"open-street-map\",\n    hovermode='closest',\n    mapbox=dict(\n        bearing=0,\n        center=go.layout.mapbox.Center(\n            lat=40.67,\n            lon=-73.90\n        ),\n        pitch=0,\n        zoom=9\n    ),\n)\n```\n\nAbove, we create a scatter plot on mapbox by passing it our list of latitudes and longitudes to plot markers.  We also pass in our custom data of names and prices for additional info to appear on every marker we hover over. Next we use `update_layout` to specify other map settings such as zoom, and centering.\n\nMore info [here](https://plotly.com/python/scattermapbox/) on scatter plots using Mapbox and Plotly.\n\n\n## Step 3 - Gradio App \u26a1\ufe0f\nWe will use two `gradio.Number` components and a `gradio.CheckboxGroup` to allow users of our app to specify price ranges and borough locations. We will then use the `gr.Plot` component as an output for our Plotly + Mapbox map we created earlier.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\n        btn = gr.Button(value=\"Update Filter\")\n        map = gr.Plot()\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\n```\n\nWe layout these components using the `gr.Column` and `gr.Row` and wel also add event triggers for when the demo first loads and when our \"Update Filter\" button is clicked in order to trigger the map to update with our new filters.\n\nThis is what the full demo code looks like:\n\n```python\nimport gradio as gr\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\ndf = dataset.to_pandas()\n\ndef filter_map(min_price, max_price, boroughs):\n\n    filtered_df = df[(df['neighbourhood_group'].isin(boroughs)) & \n          (df['price'] > min_price) & (df['price'] < max_price)]\n    names = filtered_df[\"name\"].tolist()\n    prices = filtered_df[\"price\"].tolist()\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\n    fig = go.Figure(go.Scattermapbox(\n            customdata=text_list,\n            lat=filtered_df['latitude'].tolist(),\n            lon=filtered_df['longitude'].tolist(),\n            mode='markers',\n            marker=go.scattermapbox.Marker(\n                size=6\n            ),\n            hoverinfo=\"text\",\n            hovertemplate='<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}'\n        ))\n\n    fig.update_layout(\n        mapbox_style=\"open-street-map\",\n        hovermode='closest',\n        mapbox=dict(\n            bearing=0,\n            center=go.layout.mapbox.Center(\n                lat=40.67,\n                lon=-73.90\n            ),\n            pitch=0,\n            zoom=9\n        ),\n    )\n\n    return fig\n\nwith gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\n        btn = gr.Button(value=\"Update Filter\")\n        map = gr.Plot().style()\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\n\ndemo.launch()\n```\n\n## Step 4 - Deployment \ud83e\udd17\nIf you run the code above, your app will start running locally.\nYou can even get a temporary shareable link by passing the `share=True` parameter to `launch`.\n\nBut what if you want to a permanent deployment solution?\nLet's deploy our Gradio app to the free HuggingFace Spaces platform.\n\nIf you haven't used Spaces before, follow the previous guide [here](/using_hugging_face_integrations).\n\n## Conclusion \ud83c\udf89\nAnd you're all done! That's all the code you need to build a map demo.\n\nHere's a link to the demo [Map demo](https://huggingface.co/spaces/gradio/map_airbnb) and [complete code](https://huggingface.co/spaces/gradio/map_airbnb/blob/main/run.py) (on Hugging Face Spaces)\n", "html": "<h1 id=\"how-to-use-the-plot-component-for-maps\">How to Use the Plot Component for Maps</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>This guide explains how you can use Gradio to plot geographical data on a map using the <code>gradio.Plot</code> component. The Gradio <code>Plot</code> component works with Matplotlib, Bokeh and Plotly. Plotly is what we will be working with in this guide. Plotly allows developers to easily create all sorts of maps with their geographical data. Take a look <a rel=\"noopener\" target=\"_blank\" href=\"https://plotly.com/python/maps/\">here</a> for some examples.</p>\n\n<h2 id=\"overview\">Overview</h2>\n\n<p>We will be using the New York City Airbnb dataset, which is hosted on kaggle <a rel=\"noopener\" target=\"_blank\" href=\"https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data\">here</a>. I've uploaded it to the Hugging Face Hub as a dataset <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/datasets/gradio/NYC-Airbnb-Open-Data\">here</a> for easier use and download. Using this data we will plot Airbnb locations on a map output and allow filtering based on price and location. Below is the demo that we will be building. \u26a1\ufe0f</p>\n\n<p><gradio-app space='gradio/map_airbnb'></gradio-app></p>\n\n<h2 id=\"step-1-loading-csv-data\">Step 1 - Loading CSV data \ud83d\udcbe</h2>\n\n<p>Let's start by loading the Airbnb NYC data from the Hugging Face Hub.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from datasets import load_dataset\n\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\ndf = dataset.to_pandas()\n\ndef filter_map(min_price, max_price, boroughs):\n    new_df = df[(df['neighbourhood_group'].isin(boroughs)) & \n            (df['price'] > min_price) & (df['price'] < max_price)]\n    names = new_df[\"name\"].tolist()\n    prices = new_df[\"price\"].tolist()\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\n</code></pre></div>\n\n<p>In the code above, we first load the csv data into a pandas dataframe. Let's begin by defining a function that we will use as the prediction function for the gradio app. This function will accept the minimum price and maximum price range as well as the list of boroughs to filter the resulting map. We can use the passed in values (<code>min_price</code>, <code>max_price</code>, and list of <code>boroughs</code>) to filter the dataframe and create <code>new_df</code>. Next we will create <code>text_list</code> of the names and prices of each Airbnb to use as labels on the map.</p>\n\n<h2 id=\"step-2-map-figure\">Step 2 - Map Figure \ud83c\udf10</h2>\n\n<p>Plotly makes it easy to work with maps. Let's take a look below how we can create a map figure.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import plotly.graph_objects as go\n\nfig = go.Figure(go.Scattermapbox(\n            customdata=text_list,\n            lat=new_df['latitude'].tolist(),\n            lon=new_df['longitude'].tolist(),\n            mode='markers',\n            marker=go.scattermapbox.Marker(\n                size=6\n            ),\n            hoverinfo=\"text\",\n            hovertemplate='<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}'\n        ))\n\nfig.update_layout(\n    mapbox_style=\"open-street-map\",\n    hovermode='closest',\n    mapbox=dict(\n        bearing=0,\n        center=go.layout.mapbox.Center(\n            lat=40.67,\n            lon=-73.90\n        ),\n        pitch=0,\n        zoom=9\n    ),\n)\n</code></pre></div>\n\n<p>Above, we create a scatter plot on mapbox by passing it our list of latitudes and longitudes to plot markers.  We also pass in our custom data of names and prices for additional info to appear on every marker we hover over. Next we use <code>update_layout</code> to specify other map settings such as zoom, and centering.</p>\n\n<p>More info <a rel=\"noopener\" target=\"_blank\" href=\"https://plotly.com/python/scattermapbox/\">here</a> on scatter plots using Mapbox and Plotly.</p>\n\n<h2 id=\"step-3-gradio-app\">Step 3 - Gradio App \u26a1\ufe0f</h2>\n\n<p>We will use two <code>gradio.Number</code> components and a <code>gradio.CheckboxGroup</code> to allow users of our app to specify price ranges and borough locations. We will then use the <code>gr.Plot</code> component as an output for our Plotly + Mapbox map we created earlier.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\n        btn = gr.Button(value=\"Update Filter\")\n        map = gr.Plot()\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\n</code></pre></div>\n\n<p>We layout these components using the <code>gr.Column</code> and <code>gr.Row</code> and wel also add event triggers for when the demo first loads and when our \"Update Filter\" button is clicked in order to trigger the map to update with our new filters.</p>\n\n<p>This is what the full demo code looks like:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\ndf = dataset.to_pandas()\n\ndef filter_map(min_price, max_price, boroughs):\n\n    filtered_df = df[(df['neighbourhood_group'].isin(boroughs)) & \n          (df['price'] > min_price) & (df['price'] < max_price)]\n    names = filtered_df[\"name\"].tolist()\n    prices = filtered_df[\"price\"].tolist()\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\n    fig = go.Figure(go.Scattermapbox(\n            customdata=text_list,\n            lat=filtered_df['latitude'].tolist(),\n            lon=filtered_df['longitude'].tolist(),\n            mode='markers',\n            marker=go.scattermapbox.Marker(\n                size=6\n            ),\n            hoverinfo=\"text\",\n            hovertemplate='<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}'\n        ))\n\n    fig.update_layout(\n        mapbox_style=\"open-street-map\",\n        hovermode='closest',\n        mapbox=dict(\n            bearing=0,\n            center=go.layout.mapbox.Center(\n                lat=40.67,\n                lon=-73.90\n            ),\n            pitch=0,\n            zoom=9\n        ),\n    )\n\n    return fig\n\nwith gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\n        btn = gr.Button(value=\"Update Filter\")\n        map = gr.Plot().style()\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\n\ndemo.launch()\n</code></pre></div>\n\n<h2 id=\"step-4-deployment\">Step 4 - Deployment \ud83e\udd17</h2>\n\n<p>If you run the code above, your app will start running locally.\nYou can even get a temporary shareable link by passing the <code>share=True</code> parameter to <code>launch</code>.</p>\n\n<p>But what if you want to a permanent deployment solution?\nLet's deploy our Gradio app to the free HuggingFace Spaces platform.</p>\n\n<p>If you haven't used Spaces before, follow the previous guide <a rel=\"noopener\" target=\"_blank\" href=\"/using_hugging_face_integrations\">here</a>.</p>\n\n<h2 id=\"conclusion\">Conclusion \ud83c\udf89</h2>\n\n<p>And you're all done! That's all the code you need to build a map demo.</p>\n\n<p>Here's a link to the demo <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/gradio/map_airbnb\">Map demo</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/gradio/map_airbnb/blob/main/run.py\">complete code</a> (on Hugging Face Spaces)</p>\n", "tags": ["PLOTS", "MAPS"], "spaces": [""], "url": "/guides/plot-component-for-maps/", "contributor": null}], "preprocessing": "this component does *not* accept input.", "postprocessing": "expects either a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >matplotlib.figure.Figure</span>, a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >plotly.graph_objects._figure.Figure</span>, or a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >dict</span> corresponding to a bokeh plot (json_item format)", "parent": "gradio", "prev_obj": "Number", "next_obj": "Radio"}, "radio": {"class": null, "name": "Radio", "description": "Creates a set of radio buttons of which only one can be selected. <br>", "tags": {"preprocessing": "passes the value of the selected radio button as a {str} or its index as an {int} into the function, depending on `type`.", "postprocessing": "expects a {str} corresponding to the value of the radio button to be selected.", "examples-format": "a {str} representing the radio option to select.", "demos": "sentence_builder, titanic_survival, blocks_essay"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "choices", "annotation": "List[str] | None", "doc": "list of options to select from.", "default": "None"}, {"name": "value", "annotation": "str | Callable | None", "doc": "the button selected by default. If None, no button is selected by default. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "type", "annotation": "str", "doc": "Type of value to be returned by component. \"value\" returns the string of the choice selected, \"index\" returns the index of the choice selected.", "default": "\"value\""}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, choices in this radio group will be selectable; if False, selection will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Radio"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the radio component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "item_container", "annotation": "bool | None", "doc": "If True, will place items in a container.", "default": "None"}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place the component in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Radio"}], "string_shortcuts": [["Radio", "radio", "Uses default values"]], "demos": [["sentence_builder", "import gradio as gr\n\n\ndef sentence_builder(quantity, animal, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4),\n        gr.Dropdown([\"cat\", \"dog\", \"bird\"]),\n        gr.Radio([\"park\", \"zoo\", \"road\"]),\n        gr.Dropdown([\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True),\n        gr.Checkbox(label=\"Is it the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", \"park\", [\"ran\", \"swam\"], True],\n        [4, \"dog\", \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", \"road\", [\"ran\"], False],\n        [8, \"cat\", \"zoo\", [\"ate\"], True],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["titanic_survival", "import os\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nimport gradio as gr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata = pd.read_csv(os.path.join(current_dir, \"files/titanic.csv\"))\n\n\ndef encode_age(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    categories = pd.cut(df.Age, bins, labels=False)\n    df.Age = categories\n    return df\n\n\ndef encode_fare(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    categories = pd.cut(df.Fare, bins, labels=False)\n    df.Fare = categories\n    return df\n\n\ndef encode_df(df):\n    df = encode_age(df)\n    df = encode_fare(df)\n    sex_mapping = {\"male\": 0, \"female\": 1}\n    df = df.replace({\"Sex\": sex_mapping})\n    embark_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n    df = df.replace({\"Embarked\": embark_mapping})\n    df.Embarked = df.Embarked.fillna(0)\n    df[\"Company\"] = 0\n    df.loc[(df[\"SibSp\"] > 0), \"Company\"] = 1\n    df.loc[(df[\"Parch\"] > 0), \"Company\"] = 2\n    df.loc[(df[\"SibSp\"] > 0) & (df[\"Parch\"] > 0), \"Company\"] = 3\n    df = df[\n        [\n            \"PassengerId\",\n            \"Pclass\",\n            \"Sex\",\n            \"Age\",\n            \"Fare\",\n            \"Embarked\",\n            \"Company\",\n            \"Survived\",\n        ]\n    ]\n    return df\n\n\ntrain = encode_df(data)\n\nX_all = train.drop([\"Survived\", \"PassengerId\"], axis=1)\ny_all = train[\"Survived\"]\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=num_test, random_state=23\n)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\n\ndef predict_survival(passenger_class, is_male, age, company, fare, embark_point):\n    if passenger_class is None or embark_point is None:\n        return None\n    df = pd.DataFrame.from_dict(\n        {\n            \"Pclass\": [passenger_class + 1],\n            \"Sex\": [0 if is_male else 1],\n            \"Age\": [age],\n            \"Company\": [\n                (1 if \"Sibling\" in company else 0) + (2 if \"Child\" in company else 0)\n            ],\n            \"Fare\": [fare],\n            \"Embarked\": [embark_point + 1],\n        }\n    )\n    df = encode_age(df)\n    df = encode_fare(df)\n    pred = clf.predict_proba(df)[0]\n    return {\"Perishes\": float(pred[0]), \"Survives\": float(pred[1])}\n\n\ndemo = gr.Interface(\n    predict_survival,\n    [\n        gr.Dropdown([\"first\", \"second\", \"third\"], type=\"index\"),\n        \"checkbox\",\n        gr.Slider(0, 80, value=25),\n        gr.CheckboxGroup([\"Sibling\", \"Child\"], label=\"Travelling with (select all)\"),\n        gr.Number(value=20),\n        gr.Radio([\"S\", \"C\", \"Q\"], type=\"index\"),\n    ],\n    \"label\",\n    examples=[\n        [\"first\", True, 30, [], 50, \"S\"],\n        [\"second\", False, 40, [\"Sibling\", \"Child\"], 10, \"Q\"],\n        [\"third\", True, 30, [\"Child\"], 20, \"S\"],\n    ],\n    interpretation=\"default\",\n    live=True,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["blocks_essay", "import gradio as gr\n\n\ndef change_textbox(choice):\n    if choice == \"short\":\n        return gr.Textbox.update(lines=2, visible=True)\n    elif choice == \"long\":\n        return gr.Textbox.update(lines=8, visible=True)\n    else:\n        return gr.Textbox.update(visible=False)\n\n\nwith gr.Blocks() as demo:\n    radio = gr.Radio(\n        [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\n    )\n    text = gr.Textbox(lines=2, interactive=True)\n\n    radio.change(fn=change_textbox, inputs=radio, outputs=text)\n\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()"], "events": "change()", "preprocessing": "passes the value of the selected radio button as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> or its index as an <span class='text-orange-500' style='font-family: monospace; font-size: large;' >int</span> into the function, depending on `type`.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> corresponding to the value of the radio button to be selected.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> representing the radio option to select.", "parent": "gradio", "prev_obj": "Plot", "next_obj": "ScatterPlot"}, "scatterplot": {"class": null, "name": "ScatterPlot", "description": "Create a scatter plot. <br> <br>", "tags": {"preprocessing": "this component does *not* accept input.", "postprocessing": "expects a pandas dataframe with the data to plot.", "demos": "native_plots", "guides": "creating-a-dashboard-from-bigquery-data"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "pd.DataFrame | Callable | None", "doc": "The pandas dataframe containing the data to display in a scatter plot, or a callable. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "x", "annotation": "str | None", "doc": "Column corresponding to the x axis.", "default": "None"}, {"name": "y", "annotation": "str | None", "doc": "Column corresponding to the y axis.", "default": "None"}, {"name": "color", "annotation": "str | None", "doc": "The column to determine the point color. If the column contains numeric data, gradio will interpolate the column data so that small values correspond to light colors and large values correspond to dark values.", "default": "None"}, {"name": "size", "annotation": "str | None", "doc": "The column used to determine the point size. Should contain numeric data so that gradio can map the data to the point size.", "default": "None"}, {"name": "shape", "annotation": "str | None", "doc": "The column used to determine the point shape. Should contain categorical data. Gradio will map each unique value to a different shape.", "default": "None"}, {"name": "title", "annotation": "str | None", "doc": "The title to display on top of the chart.", "default": "None"}, {"name": "tooltip", "annotation": "List[str] | str | None", "doc": "The column (or list of columns) to display on the tooltip when a user hovers a point on the plot.", "default": "None"}, {"name": "x_title", "annotation": "str | None", "doc": "The title given to the x axis. By default, uses the value of the x parameter.", "default": "None"}, {"name": "y_title", "annotation": "str | None", "doc": "The title given to the y axis. By default, uses the value of the y parameter.", "default": "None"}, {"name": "color_legend_title", "annotation": "str | None", "doc": "The title given to the color legend. By default, uses the value of color parameter.", "default": "None"}, {"name": "size_legend_title", "annotation": "str | None", "doc": "The title given to the size legend. By default, uses the value of the size parameter.", "default": "None"}, {"name": "shape_legend_title", "annotation": "str | None", "doc": "The title given to the shape legend. By default, uses the value of the shape parameter.", "default": "None"}, {"name": "color_legend_position", "annotation": "str | None", "doc": "The position of the color legend. If the string value 'none' is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.", "default": "None"}, {"name": "size_legend_position", "annotation": "str | None", "doc": "The position of the size legend. If the string value 'none' is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.", "default": "None"}, {"name": "shape_legend_position", "annotation": "str | None", "doc": "The position of the shape legend. If the string value 'none' is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.", "default": "None"}, {"name": "height", "annotation": "int | None", "doc": "The height of the plot in pixels.", "default": "None"}, {"name": "width", "annotation": "int | None", "doc": "The width of the plot in pixels.", "default": "None"}, {"name": "x_lim", "annotation": "List[int | float] | None", "doc": "A tuple or list containing the limits for the x-axis, specified as [x_min, x_max].", "default": "None"}, {"name": "y_lim", "annotation": "List[int | float] | None", "doc": "A tuple of list containing the limits for the y-axis, specified as [y_min, y_max].", "default": "None"}, {"name": "caption", "annotation": "str | None", "doc": "The (optional) caption to display below the plot.", "default": "None"}, {"name": "interactive", "annotation": "bool | None", "doc": "Whether users should be able to interact with the plot by panning or zooming with their mouse or trackpad.", "default": "True"}, {"name": "label", "annotation": "str | None", "doc": "The (optional) label to display on the top left corner of the plot.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": " If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "Whether the label should be displayed.", "default": "True"}, {"name": "visible", "annotation": "bool", "doc": "Whether the plot should be visible.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "Unique id used for custom css targetting.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.ScatterPlot"}, {"fn": null, "name": "clear", "description": "This event is triggered when the user clears the component (e.g. image or audio) using the X button for the component. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.ScatterPlot"}], "string_shortcuts": [["ScatterPlot", "scatterplot", "Uses default values"]], "demos": [["native_plots", "import gradio as gr\n\nfrom scatter_plot_demo import scatter_plot\nfrom line_plot_demo import line_plot\nfrom bar_plot_demo import bar_plot\n\n\nwith gr.Blocks() as demo:\n    with gr.Tabs():\n        with gr.TabItem(\"Scatter Plot\"):\n            scatter_plot.render()\n        with gr.TabItem(\"Line Plot\"):\n            line_plot.render()\n        with gr.TabItem(\"Bar Plot\"):\n            bar_plot.render()\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()", "clear()"], "events": "change(), clear()", "guides": [{"name": "creating-a-dashboard-from-bigquery-data", "category": "tabular-data-science-and-plots", "pretty_category": "Tabular Data Science And Plots", "guide_index": null, "absolute_index": 21, "pretty_name": "Creating A Dashboard From Bigquery Data", "content": "# Creating a Real-Time Dashboard from BigQuery Data\n\nTags: TABULAR, DASHBOARD, PLOTS \n\n\n[Google BigQuery](https://cloud.google.com/bigquery) is a cloud-based service for processing very large data sets. It is a serverless and highly scalable data warehousing solution that enables users to analyze data [using SQL-like queries](https://www.oreilly.com/library/view/google-bigquery-the/9781492044451/ch01.html).\n\nIn this tutorial, we will show you how to query a BigQuery dataset in Python and display the data in a dashboard that updates in real time using `gradio`. The dashboard will look like this:\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/bigquery-dashboard.gif\">\n\nWe'll cover the following steps in this Guide:\n\n1. Setting up your BigQuery credentials\n2. Using the BigQuery client\n3. Building the real-time dashboard (in just *7 lines of Python*)\n\nWe'll be working with the [New York Times' COVID dataset](https://www.nytimes.com/interactive/2021/us/covid-cases.html) that is available as a public dataset on BigQuery. The dataset, named `covid19_nyt.us_counties` contains the latest information about the number of confirmed cases and deaths from COVID across US counties. \n\n**Prerequisites**: This Guide uses [Gradio Blocks](../quickstart/#blocks-more-flexibility-and-control), so make your are familiar with the Blocks class. \n\n## Setting up your BigQuery Credentials\n\nTo use Gradio with BigQuery, you will need to obtain your BigQuery credentials and use them with the [BigQuery Python client](https://pypi.org/project/google-cloud-bigquery/). If you already have BigQuery credentials (as a `.json` file), you can skip this section. If not, you can do this for free in just a couple of minutes.\n\n1. First, log in to your Google Cloud account and go to the Google Cloud Console (https://console.cloud.google.com/)\n\n2. In the Cloud Console, click on the hamburger menu in the top-left corner and select \"APIs & Services\" from the menu. If you do not have an existing project, you will need to create one.\n\n3. Then, click the \"+ Enabled APIs & services\" button, which allows you to enable specific services for your project. Search for \"BigQuery API\", click on it, and click the \"Enable\" button. If you see the \"Manage\" button, then the BigQuery is already enabled, and you're all set. \n\n4. In the APIs & Services menu, click on the \"Credentials\" tab and then click on the \"Create credentials\" button.\n\n5. In the \"Create credentials\" dialog, select \"Service account key\" as the type of credentials to create, and give it a name. Also grant the service account permissions by giving it a role such as \"BigQuery User\", which will allow you to run queries.\n\n6. After selecting the service account, select the \"JSON\" key type and then click on the \"Create\" button. This will download the JSON key file containing your credentials to your computer. It will look something like this:\n\n```json\n{\n \"type\": \"service_account\",\n \"project_id\": \"your project\",\n \"private_key_id\": \"your private key id\",\n \"private_key\": \"private key\",\n \"client_email\": \"email\",\n \"client_id\": \"client id\",\n \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n \"client_x509_cert_url\":  \"https://www.googleapis.com/robot/v1/metadata/x509/email_id\"\n}\n```\n\n## Using the BigQuery Client\n\nOnce you have the credentials, you will need to use the BigQuery Python client to authenticate using your credentials. To do this, you will need to install the BigQuery Python client by running the following command in the terminal:\n\n```bash\npip install google-cloud-bigquery[pandas]\n```\n\nYou'll notice that we've installed the pandas add-on, which will be helpful for processing the BigQuery dataset as a pandas dataframe. Once the client is installed, you can authenticate using your credentials by running the following code:\n\n```py\nfrom google.cloud import bigquery\n\nclient = bigquery.Client.from_service_account_json(\"path/to/key.json\")\n```\n\nWith your credentials authenticated, you can now use the BigQuery Python client to interact with your BigQuery datasets. \n\nHere is an example of a function which queries the `covid19_nyt.us_counties` dataset in BigQuery to show the top 20 counties with the most confirmed cases as of the current day:\n\n```py\nimport numpy as np\n\nQUERY = (\n    'SELECT * FROM `bigquery-public-data.covid19_nyt.us_counties` ' \n    'ORDER BY date DESC,confirmed_cases DESC '\n    'LIMIT 20')\n\ndef run_query():\n    query_job = client.query(QUERY)  \n    query_result = query_job.result()  \n    df = query_result.to_dataframe()\n    # Select a subset of columns \n    df = df[[\"confirmed_cases\", \"deaths\", \"county\", \"state_name\"]]\n    # Convert numeric columns to standard numpy types\n    df = df.astype({\"deaths\": np.int64, \"confirmed_cases\": np.int64})\n    return df\n```\n\n## Building the Real-Time Dashboard\n\nOnce you have a function to query the data, you can use the `gr.DataFrame` component from the Gradio library to display the results in a tabular format. This is a useful way to inspect the data and make sure that it has been queried correctly.\n\nHere is an example of how to use the `gr.DataFrame` component to display the results. By passing in the `run_query` function to `gr.DataFrame`, we instruct Gradio to run the function as soon as the page loads and show the results. In addition, you also pass in the keyword `every` to tell the dashboard to refresh every hour (60*60 seconds).\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.DataFrame(run_query, every=60*60)\n\ndemo.queue().launch()  # Run the demo using queuing\n```\n\nPerhaps you'd like to add a visualization to our dashboard. You can use the `gr.ScatterPlot()` component to visualize the data in a scatter plot. This allows you to see the relationship between different variables such as case count and case deaths in the dataset and can be useful for exploring the data and gaining insights. Again, we can do this in real-time\nby passing in the `every` parameter. \n\nHere is a complete example showing how to use the `gr.ScatterPlot` to visualize in addition to displaying data with the `gr.DataFrame`\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# \ud83d\udc89 Covid Dashboard (Updated Hourly)\")\n    with gr.Row():\n        gr.DataFrame(run_query, every=60*60)\n        gr.ScatterPlot(run_query, every=60*60, x=\"confirmed_cases\", \n                        y=\"deaths\", tooltip=\"county\", width=500, height=500)\n\ndemo.queue().launch()  # Run the demo with queuing enabled\n```", "html": "<h1 id=\"creating-a-real-time-dashboard-from-bigquery-data\">Creating a Real-Time Dashboard from BigQuery Data</h1>\n\n<p><a rel=\"noopener\" target=\"_blank\" href=\"https://cloud.google.com/bigquery\">Google BigQuery</a> is a cloud-based service for processing very large data sets. It is a serverless and highly scalable data warehousing solution that enables users to analyze data <a rel=\"noopener\" target=\"_blank\" href=\"https://www.oreilly.com/library/view/google-bigquery-the/9781492044451/ch01.html\">using SQL-like queries</a>.</p>\n\n<p>In this tutorial, we will show you how to query a BigQuery dataset in Python and display the data in a dashboard that updates in real time using <code>gradio</code>. The dashboard will look like this:</p>\n\n<p><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/bigquery-dashboard.gif\"></p>\n\n<p>We'll cover the following steps in this Guide:</p>\n\n<ol>\n<li>Setting up your BigQuery credentials</li>\n<li>Using the BigQuery client</li>\n<li>Building the real-time dashboard (in just <em>7 lines of Python</em>)</li>\n</ol>\n\n<p>We'll be working with the <a rel=\"noopener\" target=\"_blank\" href=\"https://www.nytimes.com/interactive/2021/us/covid-cases.html\">New York Times' COVID dataset</a> that is available as a public dataset on BigQuery. The dataset, named <code>covid19_nyt.us_counties</code> contains the latest information about the number of confirmed cases and deaths from COVID across US counties. </p>\n\n<p><strong>Prerequisites</strong>: This Guide uses <a rel=\"noopener\" target=\"_blank\" href=\"../quickstart/#blocks-more-flexibility-and-control\">Gradio Blocks</a>, so make your are familiar with the Blocks class. </p>\n\n<h2 id=\"setting-up-your-bigquery-credentials\">Setting up your BigQuery Credentials</h2>\n\n<p>To use Gradio with BigQuery, you will need to obtain your BigQuery credentials and use them with the <a rel=\"noopener\" target=\"_blank\" href=\"https://pypi.org/project/google-cloud-bigquery/\">BigQuery Python client</a>. If you already have BigQuery credentials (as a <code>.json</code> file), you can skip this section. If not, you can do this for free in just a couple of minutes.</p>\n\n<ol>\n<li><p>First, log in to your Google Cloud account and go to the Google Cloud Console (https://console.cloud.google.com/)</p></li>\n<li><p>In the Cloud Console, click on the hamburger menu in the top-left corner and select \"APIs &amp; Services\" from the menu. If you do not have an existing project, you will need to create one.</p></li>\n<li><p>Then, click the \"+ Enabled APIs &amp; services\" button, which allows you to enable specific services for your project. Search for \"BigQuery API\", click on it, and click the \"Enable\" button. If you see the \"Manage\" button, then the BigQuery is already enabled, and you're all set. </p></li>\n<li><p>In the APIs &amp; Services menu, click on the \"Credentials\" tab and then click on the \"Create credentials\" button.</p></li>\n<li><p>In the \"Create credentials\" dialog, select \"Service account key\" as the type of credentials to create, and give it a name. Also grant the service account permissions by giving it a role such as \"BigQuery User\", which will allow you to run queries.</p></li>\n<li><p>After selecting the service account, select the \"JSON\" key type and then click on the \"Create\" button. This will download the JSON key file containing your credentials to your computer. It will look something like this:</p></li>\n</ol>\n\n<div class='codeblock'><pre><code class='lang-json'>{\n \"type\": \"service_account\",\n \"project_id\": \"your project\",\n \"private_key_id\": \"your private key id\",\n \"private_key\": \"private key\",\n \"client_email\": \"email\",\n \"client_id\": \"client id\",\n \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n \"client_x509_cert_url\":  \"https://www.googleapis.com/robot/v1/metadata/x509/email_id\"\n}\n</code></pre></div>\n\n<h2 id=\"using-the-bigquery-client\">Using the BigQuery Client</h2>\n\n<p>Once you have the credentials, you will need to use the BigQuery Python client to authenticate using your credentials. To do this, you will need to install the BigQuery Python client by running the following command in the terminal:</p>\n\n<div class='codeblock'><pre><code class='lang-bash'>pip install google-cloud-bigquery[pandas]\n</code></pre></div>\n\n<p>You'll notice that we've installed the pandas add-on, which will be helpful for processing the BigQuery dataset as a pandas dataframe. Once the client is installed, you can authenticate using your credentials by running the following code:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>from google.cloud import bigquery\n\nclient = bigquery.Client.from_service_account_json(\"path/to/key.json\")\n</code></pre></div>\n\n<p>With your credentials authenticated, you can now use the BigQuery Python client to interact with your BigQuery datasets. </p>\n\n<p>Here is an example of a function which queries the <code>covid19_nyt.us_counties</code> dataset in BigQuery to show the top 20 counties with the most confirmed cases as of the current day:</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import numpy as np\n\nQUERY = (\n    'SELECT * FROM `bigquery-public-data.covid19_nyt.us_counties` ' \n    'ORDER BY date DESC,confirmed_cases DESC '\n    'LIMIT 20')\n\ndef run_query():\n    query_job = client.query(QUERY)  \n    query_result = query_job.result()  \n    df = query_result.to_dataframe()\n    # Select a subset of columns \n    df = df[[\"confirmed_cases\", \"deaths\", \"county\", \"state_name\"]]\n    # Convert numeric columns to standard numpy types\n    df = df.astype({\"deaths\": np.int64, \"confirmed_cases\": np.int64})\n    return df\n</code></pre></div>\n\n<h2 id=\"building-the-real-time-dashboard\">Building the Real-Time Dashboard</h2>\n\n<p>Once you have a function to query the data, you can use the <code>gr.DataFrame</code> component from the Gradio library to display the results in a tabular format. This is a useful way to inspect the data and make sure that it has been queried correctly.</p>\n\n<p>Here is an example of how to use the <code>gr.DataFrame</code> component to display the results. By passing in the <code>run_query</code> function to <code>gr.DataFrame</code>, we instruct Gradio to run the function as soon as the page loads and show the results. In addition, you also pass in the keyword <code>every</code> to tell the dashboard to refresh every hour (60*60 seconds).</p>\n\n<div class='codeblock'><pre><code class='lang-py'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.DataFrame(run_query, every=60*60)\n\ndemo.queue().launch()  # Run the demo using queuing\n</code></pre></div>\n\n<p>Perhaps you'd like to add a visualization to our dashboard. You can use the <code>gr.ScatterPlot()</code> component to visualize the data in a scatter plot. This allows you to see the relationship between different variables such as case count and case deaths in the dataset and can be useful for exploring the data and gaining insights. Again, we can do this in real-time\nby passing in the <code>every</code> parameter. </p>\n\n<p>Here is a complete example showing how to use the <code>gr.ScatterPlot</code> to visualize in addition to displaying data with the <code>gr.DataFrame</code></p>\n\n<div class='codeblock'><pre><code class='lang-py'>import gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# \ud83d\udc89 Covid Dashboard (Updated Hourly)\")\n    with gr.Row():\n        gr.DataFrame(run_query, every=60*60)\n        gr.ScatterPlot(run_query, every=60*60, x=\"confirmed_cases\", \n                        y=\"deaths\", tooltip=\"county\", width=500, height=500)\n\ndemo.queue().launch()  # Run the demo with queuing enabled\n</code></pre></div>\n", "tags": ["TABULAR", "DASHBOARD", "PLOTS "], "spaces": [], "url": "/guides/creating-a-dashboard-from-bigquery-data/", "contributor": null}], "preprocessing": "this component does *not* accept input.", "postprocessing": "expects a pandas dataframe with the data to plot.", "parent": "gradio", "prev_obj": "Radio", "next_obj": "Slider"}, "slider": {"class": null, "name": "Slider", "description": "Creates a slider that ranges from `minimum` to `maximum` with a step size of `step`. <br>", "tags": {"preprocessing": "passes slider value as a {float} into the function.", "postprocessing": "expects an {int} or {float} returned from function and sets slider value to it as long as it is within range.", "examples-format": "A {float} or {int} representing the slider's value.", "demos": "sentence_builder, generate_tone, titanic_survival, interface_random_slider, blocks_random_slider", "guides": "create-your-own-friends-with-a-gan"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "minimum", "annotation": "float", "doc": "minimum value for slider.", "default": "0"}, {"name": "maximum", "annotation": "float", "doc": "maximum value for slider.", "default": "100"}, {"name": "value", "annotation": "float | Callable | None", "doc": "default value. If callable, the function will be called whenever the app loads to set the initial value of the component. Ignored if randomized=True.", "default": "None"}, {"name": "step", "annotation": "float | None", "doc": "increment between slider values.", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, slider will be adjustable; if False, adjusting will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}, {"name": "randomize", "annotation": "bool", "doc": "If True, the value of the slider when the app loads is taken uniformly at random from the range given by the minimum and maximum.", "default": "False"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Slider"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the slider.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place the component in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Slider"}], "string_shortcuts": [["Slider", "slider", "Uses default values"]], "demos": [["sentence_builder", "import gradio as gr\n\n\ndef sentence_builder(quantity, animal, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4),\n        gr.Dropdown([\"cat\", \"dog\", \"bird\"]),\n        gr.Radio([\"park\", \"zoo\", \"road\"]),\n        gr.Dropdown([\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True),\n        gr.Checkbox(label=\"Is it the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", \"park\", [\"ran\", \"swam\"], True],\n        [4, \"dog\", \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", \"road\", [\"ran\"], False],\n        [8, \"cat\", \"zoo\", [\"ate\"], True],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["generate_tone", "import numpy as np\nimport gradio as gr\n\nnotes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n\ndef generate_tone(note, octave, duration):\n    sr = 48000\n    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)\n    frequency = a4_freq * 2 ** (tones_from_a4 / 12)\n    duration = int(duration)\n    audio = np.linspace(0, duration, duration * sr)\n    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)\n    return sr, audio\n\ndemo = gr.Interface(\n    generate_tone,\n    [\n        gr.Dropdown(notes, type=\"index\"),\n        gr.Slider(4, 6, step=1),\n        gr.Textbox(value=1, label=\"Duration in seconds\"),\n    ],\n    \"audio\",\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["titanic_survival", "import os\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nimport gradio as gr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata = pd.read_csv(os.path.join(current_dir, \"files/titanic.csv\"))\n\n\ndef encode_age(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    categories = pd.cut(df.Age, bins, labels=False)\n    df.Age = categories\n    return df\n\n\ndef encode_fare(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    categories = pd.cut(df.Fare, bins, labels=False)\n    df.Fare = categories\n    return df\n\n\ndef encode_df(df):\n    df = encode_age(df)\n    df = encode_fare(df)\n    sex_mapping = {\"male\": 0, \"female\": 1}\n    df = df.replace({\"Sex\": sex_mapping})\n    embark_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n    df = df.replace({\"Embarked\": embark_mapping})\n    df.Embarked = df.Embarked.fillna(0)\n    df[\"Company\"] = 0\n    df.loc[(df[\"SibSp\"] > 0), \"Company\"] = 1\n    df.loc[(df[\"Parch\"] > 0), \"Company\"] = 2\n    df.loc[(df[\"SibSp\"] > 0) & (df[\"Parch\"] > 0), \"Company\"] = 3\n    df = df[\n        [\n            \"PassengerId\",\n            \"Pclass\",\n            \"Sex\",\n            \"Age\",\n            \"Fare\",\n            \"Embarked\",\n            \"Company\",\n            \"Survived\",\n        ]\n    ]\n    return df\n\n\ntrain = encode_df(data)\n\nX_all = train.drop([\"Survived\", \"PassengerId\"], axis=1)\ny_all = train[\"Survived\"]\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=num_test, random_state=23\n)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\n\ndef predict_survival(passenger_class, is_male, age, company, fare, embark_point):\n    if passenger_class is None or embark_point is None:\n        return None\n    df = pd.DataFrame.from_dict(\n        {\n            \"Pclass\": [passenger_class + 1],\n            \"Sex\": [0 if is_male else 1],\n            \"Age\": [age],\n            \"Company\": [\n                (1 if \"Sibling\" in company else 0) + (2 if \"Child\" in company else 0)\n            ],\n            \"Fare\": [fare],\n            \"Embarked\": [embark_point + 1],\n        }\n    )\n    df = encode_age(df)\n    df = encode_fare(df)\n    pred = clf.predict_proba(df)[0]\n    return {\"Perishes\": float(pred[0]), \"Survives\": float(pred[1])}\n\n\ndemo = gr.Interface(\n    predict_survival,\n    [\n        gr.Dropdown([\"first\", \"second\", \"third\"], type=\"index\"),\n        \"checkbox\",\n        gr.Slider(0, 80, value=25),\n        gr.CheckboxGroup([\"Sibling\", \"Child\"], label=\"Travelling with (select all)\"),\n        gr.Number(value=20),\n        gr.Radio([\"S\", \"C\", \"Q\"], type=\"index\"),\n    ],\n    \"label\",\n    examples=[\n        [\"first\", True, 30, [], 50, \"S\"],\n        [\"second\", False, 40, [\"Sibling\", \"Child\"], 10, \"Q\"],\n        [\"third\", True, 30, [\"Child\"], 20, \"S\"],\n    ],\n    interpretation=\"default\",\n    live=True,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["interface_random_slider", "import gradio as gr\n\n\ndef func(slider_1, slider_2, *args):\n    return slider_1 + slider_2 * 5\n\n\ndemo = gr.Interface(\n    func,\n    [\n        gr.Slider(minimum=1.5, maximum=250000.89, randomize=True, label=\"Random Big Range\"),\n        gr.Slider(minimum=-1, maximum=1, randomize=True, step=0.05, label=\"Random only multiple of 0.05 allowed\"),\n        gr.Slider(minimum=0, maximum=1, randomize=True, step=0.25, label=\"Random only multiples of 0.25 allowed\"),\n        gr.Slider(minimum=-100, maximum=100, randomize=True, step=3, label=\"Random between -100 and 100 step 3\"),\n        gr.Slider(minimum=-100, maximum=100, randomize=True, label=\"Random between -100 and 100\"),\n        gr.Slider(value=0.25, minimum=5, maximum=30, step=-1),\n    ],\n    \"number\",\n    interpretation=\"default\"\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["blocks_random_slider", "\nimport gradio as gr\n\n\ndef func(slider_1, slider_2):\n    return slider_1 * 5 + slider_2\n\n\nwith gr.Blocks() as demo:\n    slider = gr.Slider(minimum=-10.2, maximum=15, label=\"Random Slider (Static)\", randomize=True)\n    slider_1 = gr.Slider(minimum=100, maximum=200, label=\"Random Slider (Input 1)\", randomize=True)\n    slider_2 = gr.Slider(minimum=10, maximum=23.2, label=\"Random Slider (Input 2)\", randomize=True)\n    slider_3 = gr.Slider(value=3, label=\"Non random slider\")\n    btn = gr.Button(\"Run\")\n    btn.click(func, inputs=[slider_1, slider_2], outputs=gr.Number())\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()"], "events": "change()", "guides": [{"name": "create-your-own-friends-with-a-gan", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 26, "pretty_name": "Create Your Own Friends With A Gan", "content": "# Create Your Own Friends with a GAN\n\nRelated spaces: https://huggingface.co/spaces/NimaBoscarino/cryptopunks, https://huggingface.co/spaces/nateraw/cryptopunks-generator\nTags: GAN, IMAGE, HUB\n\nContributed by <a href=\"https://huggingface.co/NimaBoscarino\">Nima Boscarino</a> and <a href=\"https://huggingface.co/nateraw\">Nate Raw</a>\n\n\n## Introduction\n\nIt seems that cryptocurrencies, [NFTs](https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html), and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets [may be taxable, such as in Canada](https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html), today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated [CryptoPunks](https://www.larvalabs.com/cryptopunks).\n\nGenerative Adversarial Networks, often known just as *GANs*, are a specific class of deep-learning models that are designed to learn from an input dataset to create (*generate!*) new material that is convincingly similar to elements of the original training set. Famously, the website [thispersondoesnotexist.com](https://thispersondoesnotexist.com/) went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even [music](https://salu133445.github.io/musegan/)!\n\nToday we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a peek at what we're going to be putting together:\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). To use the pretrained model, also install `torch` and `torchvision`.\n\n## GANs: a very brief introduction\n\nOriginally proposed in [Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661), GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the *generator*, is responsible for generating images. The other network, the *discriminator*, receives an image at a time from the generator along with a **real** image from the training data set. The discriminator then has to guess: which image is the fake?\n\nThe generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (*adversarial!*) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!\n\nFor a more in-depth look at GANs, you can take a look at [this excellent post on Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/) or this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). For now, though, we'll dive into a demo!\n\n## Step 1 \u2014 Create the Generator model\n\nTo generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:\n\n```python\nfrom torch import nn\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n```\n\nWe're taking the generator from [this repo by @teddykoker](https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90), where you can also see the original discriminator model structure.\n\nAfter instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at [nateraw/cryptopunks-gan](https://huggingface.co/nateraw/cryptopunks-gan):\n\n```python\nfrom huggingface_hub import hf_hub_download\nimport torch\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n```\n\n## Step 2 \u2014 Defining a `predict` function\n\nThe `predict` function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our `predict` function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use `torchvision`'s `save_image` function to save the output of the model as a `png` file, and return the file name:\n\n```python\nfrom torchvision.utils import save_image\n\ndef predict(seed):\n    num_punks = 4\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWe're giving our `predict` function a `seed` parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.\n\n*Note!* Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.\n\n## Step 3 \u2014 Creating a Gradio interface\n\nAt this point you can even run the code you have with `predict(<SOME_NUMBER>)`, and you'll find your freshly generated punks in your file system at `./punks.png`. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:\n\n* Set a slider input so users can choose the \"seed\" value\n* Use an image component for our output to showcase the generated punks\n* Use our `predict()` to take the seed and generate the images\n\nWith `gr.Interface()`, we can define all of that with a single function call:\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nLaunching the interface should present you with something like this:\n\n<iframe src=\"https://nimaboscarino-cryptopunks-1.hf.space\" frameBorder=\"0\" height=\"365\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n## Step 4 \u2014 Even more punks!\n\nGenerating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the `inputs` list that we pass to `gr.Interface`:\n\n```python\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nThe new input will be passed to our `predict()` function, so we have to make some changes to that function to accept a new parameter:\n\n```python\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWhen you relaunch your interface, you should see a second slider that'll let you control the number of punks!\n\n## Step 5 - Polishing it up\n\nYour Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight \u2728\n\nWe can add some examples that users can easily try out by adding this to the `gr.Interface`:\n\n```python\ngr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n```\n\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the `inputs`. So in our case, `[seed, num_punks]`. Give it a try!\n\nYou can also try adding a `title`, `description`, and `article` to the `gr.Interface`. Each of those parameters accepts a string, so try it out and see what happens \ud83d\udc40 `article` will also accept HTML, as [explored in a previous guide](./key_features/#descriptive-content)!\n\nWhen you're all done, you may end up with something like this:\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nFor reference, here is our full code:\n\n```python\nimport torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),\n    ],\n    outputs=\"image\",\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True)\n```\n----------\n\nCongratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can [scour the Hub for more GANs](https://huggingface.co/models?other=gan) (or train your own) and continue making even more awesome demos \ud83e\udd17", "html": "<h1 id=\"create-your-own-friends-with-a-gan\">Create Your Own Friends with a GAN</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>It seems that cryptocurrencies, <a rel=\"noopener\" target=\"_blank\" href=\"https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html\">NFTs</a>, and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets <a rel=\"noopener\" target=\"_blank\" href=\"https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html\">may be taxable, such as in Canada</a>, today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated <a rel=\"noopener\" target=\"_blank\" href=\"https://www.larvalabs.com/cryptopunks\">CryptoPunks</a>.</p>\n\n<p>Generative Adversarial Networks, often known just as <em>GANs</em>, are a specific class of deep-learning models that are designed to learn from an input dataset to create (<em>generate!</em>) new material that is convincingly similar to elements of the original training set. Famously, the website <a rel=\"noopener\" target=\"_blank\" href=\"https://thispersondoesnotexist.com/\">thispersondoesnotexist.com</a> went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even <a rel=\"noopener\" target=\"_blank\" href=\"https://salu133445.github.io/musegan/\">music</a>!</p>\n\n<p>Today we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a peek at what we're going to be putting together:</p>\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. To use the pretrained model, also install <code>torch</code> and <code>torchvision</code>.</p>\n\n<h2 id=\"gans-a-very-brief-introduction\">GANs: a very brief introduction</h2>\n\n<p>Originally proposed in <a rel=\"noopener\" target=\"_blank\" href=\"https://arxiv.org/abs/1406.2661\">Goodfellow et al. 2014</a>, GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the <em>generator</em>, is responsible for generating images. The other network, the <em>discriminator</em>, receives an image at a time from the generator along with a <strong>real</strong> image from the training data set. The discriminator then has to guess: which image is the fake?</p>\n\n<p>The generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (<em>adversarial!</em>) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!</p>\n\n<p>For a more in-depth look at GANs, you can take a look at <a rel=\"noopener\" target=\"_blank\" href=\"https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/\">this excellent post on Analytics Vidhya</a> or this <a rel=\"noopener\" target=\"_blank\" href=\"https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\">PyTorch tutorial</a>. For now, though, we'll dive into a demo!</p>\n\n<h2 id=\"step-1-create-the-generator-model\">Step 1 \u2014 Create the Generator model</h2>\n\n<p>To generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from torch import nn\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n</code></pre></div>\n\n<p>We're taking the generator from <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90\">this repo by @teddykoker</a>, where you can also see the original discriminator model structure.</p>\n\n<p>After instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/nateraw/cryptopunks-gan\">nateraw/cryptopunks-gan</a>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from huggingface_hub import hf_hub_download\nimport torch\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n</code></pre></div>\n\n<h2 id=\"step-2-defining-a-predict-function\">Step 2 \u2014 Defining a <code>predict</code> function</h2>\n\n<p>The <code>predict</code> function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our <code>predict</code> function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use <code>torchvision</code>'s <code>save_image</code> function to save the output of the model as a <code>png</code> file, and return the file name:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from torchvision.utils import save_image\n\ndef predict(seed):\n    num_punks = 4\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n</code></pre></div>\n\n<p>We're giving our <code>predict</code> function a <code>seed</code> parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.</p>\n\n<p><em>Note!</em> Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.</p>\n\n<h2 id=\"step-3-creating-a-gradio-interface\">Step 3 \u2014 Creating a Gradio interface</h2>\n\n<p>At this point you can even run the code you have with <code>predict(&lt;SOME_NUMBER&gt;)</code>, and you'll find your freshly generated punks in your file system at <code>./punks.png</code>. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:</p>\n\n<ul>\n<li>Set a slider input so users can choose the \"seed\" value</li>\n<li>Use an image component for our output to showcase the generated punks</li>\n<li>Use our <code>predict()</code> to take the seed and generate the images</li>\n</ul>\n\n<p>With <code>gr.Interface()</code>, we can define all of that with a single function call:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n</code></pre></div>\n\n<p>Launching the interface should present you with something like this:</p>\n\n<iframe src=\"https://nimaboscarino-cryptopunks-1.hf.space\" frameBorder=\"0\" height=\"365\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"step-4-even-more-punks\">Step 4 \u2014 Even more punks!</h2>\n\n<p>Generating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the <code>inputs</code> list that we pass to <code>gr.Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n</code></pre></div>\n\n<p>The new input will be passed to our <code>predict()</code> function, so we have to make some changes to that function to accept a new parameter:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n</code></pre></div>\n\n<p>When you relaunch your interface, you should see a second slider that'll let you control the number of punks!</p>\n\n<h2 id=\"step-5-polishing-it-up\">Step 5 - Polishing it up</h2>\n\n<p>Your Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight \u2728</p>\n\n<p>We can add some examples that users can easily try out by adding this to the <code>gr.Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n</code></pre></div>\n\n<p>The <code>examples</code> parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the <code>inputs</code>. So in our case, <code>[seed, num_punks]</code>. Give it a try!</p>\n\n<p>You can also try adding a <code>title</code>, <code>description</code>, and <code>article</code> to the <code>gr.Interface</code>. Each of those parameters accepts a string, so try it out and see what happens \ud83d\udc40 <code>article</code> will also accept HTML, as <a rel=\"noopener\" target=\"_blank\" href=\"./key_features/#descriptive-content\">explored in a previous guide</a>!</p>\n\n<p>When you're all done, you may end up with something like this:</p>\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>For reference, here is our full code:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),\n    ],\n    outputs=\"image\",\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True)\n</code></pre></div>\n\n<hr />\n\n<p>Congratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/models?other=gan\">scour the Hub for more GANs</a> (or train your own) and continue making even more awesome demos \ud83e\udd17</p>\n", "tags": ["GAN", "IMAGE", "HUB"], "spaces": ["https://huggingface.co/spaces/NimaBoscarino/cryptopunks", "https://huggingface.co/spaces/nateraw/cryptopunks-generator"], "url": "/guides/create-your-own-friends-with-a-gan/", "contributor": "<a href=\"https://huggingface.co/NimaBoscarino\">Nima Boscarino</a> and <a href=\"https://huggingface.co/nateraw\">Nate Raw</a>"}], "preprocessing": "passes slider value as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >float</span> into the function.", "postprocessing": "expects an <span class='text-orange-500' style='font-family: monospace; font-size: large;' >int</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >float</span> returned from function and sets slider value to it as long as it is within range.", "examples-format": "A <span class='text-orange-500' style='font-family: monospace; font-size: large;' >float</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >int</span> representing the slider's value.", "parent": "gradio", "prev_obj": "ScatterPlot", "next_obj": "State"}, "state": {"class": null, "name": "State", "description": "Special hidden component that stores session state across runs of the demo by the same user. The value of the State variable is cleared when the user refreshes the page. <br>", "tags": {"preprocessing": "No preprocessing is performed", "postprocessing": "No postprocessing is performed", "demos": "chatbot_demo, blocks_simple_squares", "guides": "creating-a-chatbot, real-time-speech-recognition"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "Any", "doc": "the initial value of the state. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [], "demos": [["chatbot_demo", "import gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n\ndef predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response \n    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n    return response, history\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    state = gr.State([])\n\n    with gr.Row():\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n\n    txt.submit(predict, [txt, state], [chatbot, state])\n            \nif __name__ == \"__main__\":\n    demo.launch()\n"], ["blocks_simple_squares", "import gradio as gr\n\ndemo = gr.Blocks(css=\"#btn {color: red}\")\n\nwith demo:\n    default_json = {\"a\": \"a\"}\n\n    num = gr.State(value=0)\n    squared = gr.Number(value=0)\n    btn = gr.Button(\"Next Square\", elem_id=\"btn\").style(rounded=False)\n\n    stats = gr.State(value=default_json)\n    table = gr.JSON()\n\n    def increase(var, stats_history):\n        var += 1\n        stats_history[str(var)] = var**2\n        return var, var**2, stats_history, stats_history\n\n    btn.click(increase, [num, stats], [num, squared, stats, table])\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": [], "guides": [{"name": "creating-a-chatbot", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 27, "pretty_name": "Creating A Chatbot", "content": "# How to Create a Chatbot\n\nRelated spaces: https://huggingface.co/spaces/dawood/chatbot-guide, https://huggingface.co/spaces/dawood/chatbot-guide-multimodal, https://huggingface.co/spaces/ThomasSimonini/Chat-with-Gandalf-GPT-J6B, https://huggingface.co/spaces/gorkemgoknar/moviechatbot, https://huggingface.co/spaces/Kirili4ik/chat-with-Kirill\nTags: NLP, TEXT, HTML\n\n## Introduction\n\nChatbots are widely studied in natural language processing (NLP) research and are a common use case of NLP in industry. Because chatbots are designed to be used directly by customers and end users, it is important to validate that chatbots are behaving as expected when confronted with a wide variety of input prompts. \n\nUsing `gradio`, you can easily build a demo of your chatbot model and share that with a testing team, or test it yourself using an intuitive chatbot GUI.\n\nThis tutorial will show how to take a pretrained chatbot model and deploy it with a Gradio interface in 4 steps. The live chatbot interface that we create will look something like this (try it!):\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nChatbots are *stateful*, meaning that the model's prediction can change depending on how the user has previously interacted with the model. So, in this tutorial, we will also cover how to use **state** with Gradio demos. \n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/quickstart). To use a pretrained chatbot model, also install `transformers` and `torch`. \n\nLet's get started! Here's how to build your own chatbot: \n\n  [1. Set up the Chatbot Model](#1-set-up-the-chatbot-model)\n  [2. Define a `predict` function](#2-define-a-predict-function)\n  [3. Create a Gradio Demo using Blocks](#3-create-a-gradio-demo-using-blocks)\n  [4. Chatbot Markdown Support](#4-chatbot-markdown-support)\n\n## 1. Set up the Chatbot Model\n\nFirst, you will need to have a chatbot model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will use a pretrained chatbot model, `DialoGPT`, and its tokenizer from the [Hugging Face Hub](https://huggingface.co/microsoft/DialoGPT-medium), but you can replace this with your own model. \n\nHere is the code to load `DialoGPT` from Hugging Face `transformers`.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n```\n\n## 2. Define a `predict` function\n\nNext, you will need to define a function that takes in the *user input* as well as the previous *chat history* to generate a response.\n\nIn the case of our pretrained model, it will look like this:\n\n```python\ndef predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response \n    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n    return response, history\n```\n\nLet's break this down. The function takes two parameters:\n\n* `input`: which is what the user enters (through the Gradio GUI) in a particular step of the conversation. \n* `history`: which represents the **state**, consisting of the list of user and bot responses. To create a stateful Gradio demo, we *must* pass in a parameter to represent the state, and we set the default value of this parameter to be the initial value of the state (in this case, the empty list since this is what we would like the chat history to be at the start).\n\nThen, the function tokenizes the input and concatenates it with the tokens corresponding to the previous user and bot responses. Then, this is fed into the pretrained model to get a prediction. Finally, we do some cleaning up so that we can return two values from our function:\n\n* `response`: which is a list of tuples of strings corresponding to all of the user and bot responses. This will be rendered as the output in the Gradio demo.\n* `history` variable, which is the token representation of all of the user and bot responses. In stateful Gradio demos, we *must* return the updated state at the end of the function. \n\n## 3. Create a Gradio Demo using Blocks\n\nNow that we have our predictive function set up, we can create a Gradio demo around it. \n\nIn this case, our function takes in two values, a text input and a state input. The corresponding input components in `gradio` are `\"text\"` and `\"state\"`. \n\nThe function also returns two values. We will display the list of responses using the dedicated `\"chatbot\"` component and use the `\"state\"` output component type for the second return value.\n\nNote that the `\"state\"` input and output components are not displayed. \n\n```python\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    state = gr.State([])\n    \n    with gr.Row():\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n            \n    txt.submit(predict, [txt, state], [chatbot, state])\n            \ndemo.launch()\n```\n\nThis produces the following demo, which you can try right here in your browser (try typing in some simple greetings like \"Hi!\" to get started):\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\n----------\n\n## 4. Chatbot Markdown Support\n\n\nThe `gr.Chatbot` also supports a subset of markdown including bold, italics, code, and images. Let's take a look at how we can use the markdown support to allow a user to submit images to the chatbot component.\n\n```python\ndef add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n```\n\n\nNotice the `add_image` function takes in both the `state` and `image` and appends the user submitted image to `state` by using markdown. \n\n\n```python\nimport gradio as gr\n\ndef add_text(state, text):\n    state = state + [(text, text + \"?\")]\n    return state, state\n\ndef add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n\n\nwith gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n    state = gr.State([])\n    \n    with gr.Row():\n        with gr.Column(scale=0.85):\n            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\n        with gr.Column(scale=0.15, min_width=0):\n            btn = gr.UploadButton(\"\ud83d\uddbc\ufe0f\", file_types=[\"image\"])\n            \n    txt.submit(add_text, [state, txt], [state, chatbot])\n    txt.submit(lambda :\"\", None, txt)\n    btn.upload(add_image, [state, btn], [state, chatbot])\n            \ndemo.launch()\n```\n\nThis is the code for a chatbot with a textbox for a user to submit text and an image upload button to submit images. The rest of the demo code is creating an interface using blocks; basically adding a couple more components compared to section 3.\n\nThis code will produce a demo like the one below:\n\n<iframe src=\"https://dawood-chatbot-guide-multimodal.hf.space\" frameBorder=\"0\" height=\"650\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nAnd you're done! That's all the code you need to build an interface for your chatbot model. Here are some references that you may find useful:\n\n* Gradio's [Quickstart guide](https://gradio.app/quickstart/)\n* The first chatbot demo [chatbot demo](https://huggingface.co/spaces/dawood/chatbot-guide) and [complete code](https://huggingface.co/spaces/dawood/chatbot-guide/blob/main/app.py) (on Hugging Face Spaces)\n* The final chatbot with markdown support [chatbot demo](https://huggingface.co/spaces/dawood/chatbot-guide-multimodal) and [complete code](https://huggingface.co/spaces/dawood/chatbot-guide-multimodal/blob/main/app.py) (on Hugging Face Spaces)\n", "html": "<h1 id=\"how-to-create-a-chatbot\">How to Create a Chatbot</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Chatbots are widely studied in natural language processing (NLP) research and are a common use case of NLP in industry. Because chatbots are designed to be used directly by customers and end users, it is important to validate that chatbots are behaving as expected when confronted with a wide variety of input prompts. </p>\n\n<p>Using <code>gradio</code>, you can easily build a demo of your chatbot model and share that with a testing team, or test it yourself using an intuitive chatbot GUI.</p>\n\n<p>This tutorial will show how to take a pretrained chatbot model and deploy it with a Gradio interface in 4 steps. The live chatbot interface that we create will look something like this (try it!):</p>\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Chatbots are <em>stateful</em>, meaning that the model's prediction can change depending on how the user has previously interacted with the model. So, in this tutorial, we will also cover how to use <strong>state</strong> with Gradio demos. </p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/quickstart\">installed</a>. To use a pretrained chatbot model, also install <code>transformers</code> and <code>torch</code>. </p>\n\n<p>Let's get started! Here's how to build your own chatbot: </p>\n\n<p><a href=\"#1-set-up-the-chatbot-model\">1. Set up the Chatbot Model</a>\n  <a href=\"#2-define-a-predict-function\">2. Define a <code>predict</code> function</a>\n  <a href=\"#3-create-a-gradio-demo-using-blocks\">3. Create a Gradio Demo using Blocks</a>\n  <a href=\"#4-chatbot-markdown-support\">4. Chatbot Markdown Support</a></p>\n\n<h2 id=\"1-set-up-the-chatbot-model\">1. Set up the Chatbot Model</h2>\n\n<p>First, you will need to have a chatbot model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will use a pretrained chatbot model, <code>DialoGPT</code>, and its tokenizer from the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/microsoft/DialoGPT-medium\">Hugging Face Hub</a>, but you can replace this with your own model. </p>\n\n<p>Here is the code to load <code>DialoGPT</code> from Hugging Face <code>transformers</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n</code></pre></div>\n\n<h2 id=\"2-define-a-predict-function\">2. Define a <code>predict</code> function</h2>\n\n<p>Next, you will need to define a function that takes in the <em>user input</em> as well as the previous <em>chat history</em> to generate a response.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response \n    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n    return response, history\n</code></pre></div>\n\n<p>Let's break this down. The function takes two parameters:</p>\n\n<ul>\n<li><code>input</code>: which is what the user enters (through the Gradio GUI) in a particular step of the conversation. </li>\n<li><code>history</code>: which represents the <strong>state</strong>, consisting of the list of user and bot responses. To create a stateful Gradio demo, we <em>must</em> pass in a parameter to represent the state, and we set the default value of this parameter to be the initial value of the state (in this case, the empty list since this is what we would like the chat history to be at the start).</li>\n</ul>\n\n<p>Then, the function tokenizes the input and concatenates it with the tokens corresponding to the previous user and bot responses. Then, this is fed into the pretrained model to get a prediction. Finally, we do some cleaning up so that we can return two values from our function:</p>\n\n<ul>\n<li><code>response</code>: which is a list of tuples of strings corresponding to all of the user and bot responses. This will be rendered as the output in the Gradio demo.</li>\n<li><code>history</code> variable, which is the token representation of all of the user and bot responses. In stateful Gradio demos, we <em>must</em> return the updated state at the end of the function. </li>\n</ul>\n\n<h2 id=\"3-create-a-gradio-demo-using-blocks\">3. Create a Gradio Demo using Blocks</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio demo around it. </p>\n\n<p>In this case, our function takes in two values, a text input and a state input. The corresponding input components in <code>gradio</code> are <code>\"text\"</code> and <code>\"state\"</code>. </p>\n\n<p>The function also returns two values. We will display the list of responses using the dedicated <code>\"chatbot\"</code> component and use the <code>\"state\"</code> output component type for the second return value.</p>\n\n<p>Note that the <code>\"state\"</code> input and output components are not displayed. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    state = gr.State([])\n\n    with gr.Row():\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n\n    txt.submit(predict, [txt, state], [chatbot, state])\n\ndemo.launch()\n</code></pre></div>\n\n<p>This produces the following demo, which you can try right here in your browser (try typing in some simple greetings like \"Hi!\" to get started):</p>\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<h2 id=\"4-chatbot-markdown-support\">4. Chatbot Markdown Support</h2>\n\n<p>The <code>gr.Chatbot</code> also supports a subset of markdown including bold, italics, code, and images. Let's take a look at how we can use the markdown support to allow a user to submit images to the chatbot component.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n</code></pre></div>\n\n<p>Notice the <code>add_image</code> function takes in both the <code>state</code> and <code>image</code> and appends the user submitted image to <code>state</code> by using markdown. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef add_text(state, text):\n    state = state + [(text, text + \"?\")]\n    return state, state\n\ndef add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n\n\nwith gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n    state = gr.State([])\n\n    with gr.Row():\n        with gr.Column(scale=0.85):\n            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\n        with gr.Column(scale=0.15, min_width=0):\n            btn = gr.UploadButton(\"\ud83d\uddbc\ufe0f\", file_types=[\"image\"])\n\n    txt.submit(add_text, [state, txt], [state, chatbot])\n    txt.submit(lambda :\"\", None, txt)\n    btn.upload(add_image, [state, btn], [state, chatbot])\n\ndemo.launch()\n</code></pre></div>\n\n<p>This is the code for a chatbot with a textbox for a user to submit text and an image upload button to submit images. The rest of the demo code is creating an interface using blocks; basically adding a couple more components compared to section 3.</p>\n\n<p>This code will produce a demo like the one below:</p>\n\n<iframe src=\"https://dawood-chatbot-guide-multimodal.hf.space\" frameBorder=\"0\" height=\"650\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>And you're done! That's all the code you need to build an interface for your chatbot model. Here are some references that you may find useful:</p>\n\n<ul>\n<li>Gradio's <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/quickstart/\">Quickstart guide</a></li>\n<li>The first chatbot demo <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide\">chatbot demo</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide/blob/main/app.py\">complete code</a> (on Hugging Face Spaces)</li>\n<li>The final chatbot with markdown support <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide-multimodal\">chatbot demo</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide-multimodal/blob/main/app.py\">complete code</a> (on Hugging Face Spaces)</li>\n</ul>\n", "tags": ["NLP", "TEXT", "HTML"], "spaces": ["https://huggingface.co/spaces/dawood/chatbot-guide", "https://huggingface.co/spaces/dawood/chatbot-guide-multimodal", "https://huggingface.co/spaces/ThomasSimonini/Chat-with-Gandalf-GPT-J6B", "https://huggingface.co/spaces/gorkemgoknar/moviechatbot", "https://huggingface.co/spaces/Kirili4ik/chat-with-Kirill"], "url": "/guides/creating-a-chatbot/", "contributor": null}, {"name": "real-time-speech-recognition", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 33, "pretty_name": "Real Time Speech Recognition", "content": "# Real Time Speech Recognition \n\nRelated spaces: https://huggingface.co/spaces/abidlabs/streaming-asr-paused, https://huggingface.co/spaces/abidlabs/full-context-asr\nTags: ASR, SPEECH, STREAMING\n\n## Introduction\n\nAutomatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).\n\nUsing `gradio`, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.\n\nThis tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a ***full-context*** model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it ***streaming***, meaning that the audio model will convert speech as you speak. The streaming demo that we create will look something like this (try it below or [in a new tab](https://huggingface.co/spaces/abidlabs/streaming-asr-paused)!):\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nReal-time ASR is inherently *stateful*, meaning that the model's predictions change depending on what words the user previously spoke. So, in this tutorial, we will also cover how to use **state** with Gradio demos. \n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:\n\n* Transformers (for this, `pip install transformers` and `pip install torch`) \n* DeepSpeech (`pip install deepspeech==0.8.2`)\n\nMake sure you have at least one of these installed so that you can follow along the tutorial. You will also need `ffmpeg` [installed on your system](https://www.ffmpeg.org/download.html), if you do not already have it, to process files from the microphone.\n\nHere's how to build a real time speech recognition (ASR) app: \n\n1. [Set up the Transformers ASR Model](#1-set-up-the-transformers-asr-model)\n2. [Create a Full-Context ASR Demo with Transformers](#2-create-a-full-context-asr-demo-with-transformers) \n3. [Create a Streaming ASR Demo  with Transformers](#3-create-a-streaming-asr-demo-with-transformers)\n4. [Create a Streaming ASR Demo with DeepSpeech](#4-create-a-streaming-asr-demo-with-deepspeech)\n\n\n## 1. Set up the Transformers ASR Model\n\nFirst, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the Hugging Face model, `Wav2Vec2`. \n\nHere is the code to load `Wav2Vec2` from Hugging Face `transformers`.\n\n```python\nfrom transformers import pipeline\n\np = pipeline(\"automatic-speech-recognition\")\n```\n\nThat's it! By default, the automatic speech recognition model pipeline loads Facebook's `facebook/wav2vec2-base-960h` model.\n\n## 2. Create a Full-Context ASR Demo with Transformers \n\nWe will start by creating a *full-context* ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the `pipeline` object above.\n\nWe will use `gradio`'s built in `Audio` component, configured to take input from the user's microphone and return a filepath for the recorded audio. The output component will be a plain `Textbox`.\n\n```python\nimport gradio as gr\n\ndef transcribe(audio):\n    text = p(audio)[\"text\"]\n    return text\n\ngr.Interface(\n    fn=transcribe, \n    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n    outputs=\"text\").launch()\n```\n\nSo what's happening here? The `transcribe` function takes a single parameter, `audio`, which is a filepath to the audio file that the user has recorded. The `pipeline` object expects a filepath and converts it to text, which is returned to the frontend and displayed in a textbox. \n\nLet's see it in action! (Record a short audio clip and then click submit, or [open in a new tab](https://huggingface.co/spaces/abidlabs/full-context-asr)):\n\n<iframe src=\"https://abidlabs-full-context-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n## 3. Create a Streaming ASR Demo  with Transformers\n\nOk great! We've built an ASR model that works well for short audio clips. However, if you are recording longer audio clips, you probably want a *streaming* interface, one that transcribes audio as the user speaks instead of just all-at-once at the end.\n\nThe good news is that it's not too difficult to adapt the demo we just made to make it streaming, using the same `Wav2Vec2` model. \n\nThe biggest change is that we must now introduce a `state` parameter, which holds the audio that has been *transcribed so far*. This allows us to only the latest chunk of audio and simply append it to the audio we previously transcribed. \n\nWhen adding state to a Gradio demo, you need to do a total of 3 things:\n\n* Add a `state` parameter to the function\n* Return the updated `state` at the end of the function\n* Add the `\"state\"` components to the `inputs` and `outputs` in `Interface` \n\nHere's what the code looks like:\n\n```python\ndef transcribe(audio, state=\"\"):\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\n# Set the starting state to an empty string\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\" \n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n```\n\nNotice that we've also made one other change, which is that we've set `live=True`. This keeps the Gradio interface running constantly, so it automatically transcribes audio without the user having to repeatedly hit the submit button.\n\nLet's see how it does (try below or [in a new tab](https://huggingface.co/spaces/abidlabs/streaming-asr))!\n\n<iframe src=\"https://abidlabs-streaming-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nOne thing that you may notice is that the transcription quality has dropped since the chunks of audio are so small, they lack the context to properly be transcribed. A \"hacky\" fix to this is to simply increase the runtime of the `transcribe()` function so that longer audio chunks are processed. We can do this by adding a `time.sleep()` inside the function, as shown below (we'll see a proper fix next) \n\n```python\nfrom transformers import pipeline\nimport gradio as gr\nimport time\n\np = pipeline(\"automatic-speech-recognition\")\n\ndef transcribe(audio, state=\"\"):\n    time.sleep(2)\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\"\n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n```\n\nTry the demo below to see the difference (or [open in a new tab](https://huggingface.co/spaces/abidlabs/streaming-asr-paused))!\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\n## 4. Create a Streaming ASR Demo with DeepSpeech\n\nYou're not restricted to ASR models from the `transformers` library -- you can use your own models or models from other libraries. The `DeepSpeech` library contains models that are specifically designed to handle streaming audio data. These models perform really well with  streaming data as they are able to account for previous chunks of audio data when making predictions.\n\nGoing through the DeepSpeech library is beyond the scope of this Guide (check out their [excellent documentation here](https://deepspeech.readthedocs.io/en/r0.9/)), but you can use Gradio very similarly with a DeepSpeech ASR model as with a Transformers ASR model. \n\nHere's a complete example (on Linux):\n\nFirst install the DeepSpeech library and download the pretrained models from the terminal:\n\n```bash\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.pbmm\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.scorer\napt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\npip install deepspeech==0.8.2\n```\n\nThen, create a similar `transcribe()` function as before:\n\n```python\nfrom deepspeech import Model\nimport numpy as np\n\nmodel_file_path = \"deepspeech-0.8.2-models.pbmm\"\nlm_file_path = \"deepspeech-0.8.2-models.scorer\"\nbeam_width = 100\nlm_alpha = 0.93\nlm_beta = 1.18\n\nmodel = Model(model_file_path)\nmodel.enableExternalScorer(lm_file_path)\nmodel.setScorerAlphaBeta(lm_alpha, lm_beta)\nmodel.setBeamWidth(beam_width)\n\n\ndef reformat_freq(sr, y):\n    if sr not in (\n        48000,\n        16000,\n    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)\n        raise ValueError(\"Unsupported rate\", sr)\n    if sr == 48000:\n        y = (\n            ((y / max(np.max(y), 1)) * 32767)\n            .reshape((-1, 3))\n            .mean(axis=1)\n            .astype(\"int16\")\n        )\n        sr = 16000\n    return sr, y\n\n\ndef transcribe(speech, stream):\n    _, y = reformat_freq(*speech)\n    if stream is None:\n        stream = model.createStream()\n    stream.feedAudioContent(y)\n    text = stream.intermediateDecode()\n    return text, stream\n\n```\n\nThen, create a Gradio Interface as before (the only difference being that the return type should be `numpy` instead of a `filepath` to be compatible with the DeepSpeech models)\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"numpy\"), \n        \"state\" \n    ], \n    outputs= [\n        \"text\", \n        \"state\"\n    ], \n    live=True).launch()\n```\n\nRunning all of this should allow you to deploy your realtime ASR model with a nice GUI. Try it out and see how well it works for you.\n\n--------------------------------------------\n\n\nAnd you're done! That's all the code you need to build a web-based GUI for your ASR model. \n\nFun tip: you can share your ASR model instantly with others simply by setting `share=True` in `launch()`. \n\n\n", "html": "<h1 id=\"real-time-speech-recognition\">Real Time Speech Recognition</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Automatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).</p>\n\n<p>Using <code>gradio</code>, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.</p>\n\n<p>This tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a <strong><em>full-context</em></strong> model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it <strong><em>streaming</em></strong>, meaning that the audio model will convert speech as you speak. The streaming demo that we create will look something like this (try it below or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/streaming-asr-paused\">in a new tab</a>!):</p>\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Real-time ASR is inherently <em>stateful</em>, meaning that the model's predictions change depending on what words the user previously spoke. So, in this tutorial, we will also cover how to use <strong>state</strong> with Gradio demos. </p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:</p>\n\n<ul>\n<li>Transformers (for this, <code>pip install transformers</code> and <code>pip install torch</code>) </li>\n<li>DeepSpeech (<code>pip install deepspeech==0.8.2</code>)</li>\n</ul>\n\n<p>Make sure you have at least one of these installed so that you can follow along the tutorial. You will also need <code>ffmpeg</code> <a rel=\"noopener\" target=\"_blank\" href=\"https://www.ffmpeg.org/download.html\">installed on your system</a>, if you do not already have it, to process files from the microphone.</p>\n\n<p>Here's how to build a real time speech recognition (ASR) app: </p>\n\n<ol>\n<li><a href=\"#1-set-up-the-transformers-asr-model\">Set up the Transformers ASR Model</a></li>\n<li><a href=\"#2-create-a-full-context-asr-demo-with-transformers\">Create a Full-Context ASR Demo with Transformers</a> </li>\n<li><a href=\"#3-create-a-streaming-asr-demo-with-transformers\">Create a Streaming ASR Demo  with Transformers</a></li>\n<li><a href=\"#4-create-a-streaming-asr-demo-with-deepspeech\">Create a Streaming ASR Demo with DeepSpeech</a></li>\n</ol>\n\n<h2 id=\"1-set-up-the-transformers-asr-model\">1. Set up the Transformers ASR Model</h2>\n\n<p>First, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the Hugging Face model, <code>Wav2Vec2</code>. </p>\n\n<p>Here is the code to load <code>Wav2Vec2</code> from Hugging Face <code>transformers</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import pipeline\n\np = pipeline(\"automatic-speech-recognition\")\n</code></pre></div>\n\n<p>That's it! By default, the automatic speech recognition model pipeline loads Facebook's <code>facebook/wav2vec2-base-960h</code> model.</p>\n\n<h2 id=\"2-create-a-full-context-asr-demo-with-transformers\">2. Create a Full-Context ASR Demo with Transformers</h2>\n\n<p>We will start by creating a <em>full-context</em> ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the <code>pipeline</code> object above.</p>\n\n<p>We will use <code>gradio</code>'s built in <code>Audio</code> component, configured to take input from the user's microphone and return a filepath for the recorded audio. The output component will be a plain <code>Textbox</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef transcribe(audio):\n    text = p(audio)[\"text\"]\n    return text\n\ngr.Interface(\n    fn=transcribe, \n    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n    outputs=\"text\").launch()\n</code></pre></div>\n\n<p>So what's happening here? The <code>transcribe</code> function takes a single parameter, <code>audio</code>, which is a filepath to the audio file that the user has recorded. The <code>pipeline</code> object expects a filepath and converts it to text, which is returned to the frontend and displayed in a textbox. </p>\n\n<p>Let's see it in action! (Record a short audio clip and then click submit, or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/full-context-asr\">open in a new tab</a>):</p>\n\n<iframe src=\"https://abidlabs-full-context-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"3-create-a-streaming-asr-demo-with-transformers\">3. Create a Streaming ASR Demo  with Transformers</h2>\n\n<p>Ok great! We've built an ASR model that works well for short audio clips. However, if you are recording longer audio clips, you probably want a <em>streaming</em> interface, one that transcribes audio as the user speaks instead of just all-at-once at the end.</p>\n\n<p>The good news is that it's not too difficult to adapt the demo we just made to make it streaming, using the same <code>Wav2Vec2</code> model. </p>\n\n<p>The biggest change is that we must now introduce a <code>state</code> parameter, which holds the audio that has been <em>transcribed so far</em>. This allows us to only the latest chunk of audio and simply append it to the audio we previously transcribed. </p>\n\n<p>When adding state to a Gradio demo, you need to do a total of 3 things:</p>\n\n<ul>\n<li>Add a <code>state</code> parameter to the function</li>\n<li>Return the updated <code>state</code> at the end of the function</li>\n<li>Add the <code>\"state\"</code> components to the <code>inputs</code> and <code>outputs</code> in <code>Interface</code> </li>\n</ul>\n\n<p>Here's what the code looks like:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def transcribe(audio, state=\"\"):\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\n# Set the starting state to an empty string\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\" \n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n</code></pre></div>\n\n<p>Notice that we've also made one other change, which is that we've set <code>live=True</code>. This keeps the Gradio interface running constantly, so it automatically transcribes audio without the user having to repeatedly hit the submit button.</p>\n\n<p>Let's see how it does (try below or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/streaming-asr\">in a new tab</a>)!</p>\n\n<iframe src=\"https://abidlabs-streaming-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>One thing that you may notice is that the transcription quality has dropped since the chunks of audio are so small, they lack the context to properly be transcribed. A \"hacky\" fix to this is to simply increase the runtime of the <code>transcribe()</code> function so that longer audio chunks are processed. We can do this by adding a <code>time.sleep()</code> inside the function, as shown below (we'll see a proper fix next) </p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import pipeline\nimport gradio as gr\nimport time\n\np = pipeline(\"automatic-speech-recognition\")\n\ndef transcribe(audio, state=\"\"):\n    time.sleep(2)\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\"\n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n</code></pre></div>\n\n<p>Try the demo below to see the difference (or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/streaming-asr-paused\">open in a new tab</a>)!</p>\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"4-create-a-streaming-asr-demo-with-deepspeech\">4. Create a Streaming ASR Demo with DeepSpeech</h2>\n\n<p>You're not restricted to ASR models from the <code>transformers</code> library -- you can use your own models or models from other libraries. The <code>DeepSpeech</code> library contains models that are specifically designed to handle streaming audio data. These models perform really well with  streaming data as they are able to account for previous chunks of audio data when making predictions.</p>\n\n<p>Going through the DeepSpeech library is beyond the scope of this Guide (check out their <a rel=\"noopener\" target=\"_blank\" href=\"https://deepspeech.readthedocs.io/en/r0.9/\">excellent documentation here</a>), but you can use Gradio very similarly with a DeepSpeech ASR model as with a Transformers ASR model. </p>\n\n<p>Here's a complete example (on Linux):</p>\n\n<p>First install the DeepSpeech library and download the pretrained models from the terminal:</p>\n\n<div class='codeblock'><pre><code class='lang-bash'>wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.pbmm\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.scorer\napt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\npip install deepspeech==0.8.2\n</code></pre></div>\n\n<p>Then, create a similar <code>transcribe()</code> function as before:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from deepspeech import Model\nimport numpy as np\n\nmodel_file_path = \"deepspeech-0.8.2-models.pbmm\"\nlm_file_path = \"deepspeech-0.8.2-models.scorer\"\nbeam_width = 100\nlm_alpha = 0.93\nlm_beta = 1.18\n\nmodel = Model(model_file_path)\nmodel.enableExternalScorer(lm_file_path)\nmodel.setScorerAlphaBeta(lm_alpha, lm_beta)\nmodel.setBeamWidth(beam_width)\n\n\ndef reformat_freq(sr, y):\n    if sr not in (\n        48000,\n        16000,\n    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)\n        raise ValueError(\"Unsupported rate\", sr)\n    if sr == 48000:\n        y = (\n            ((y / max(np.max(y), 1)) * 32767)\n            .reshape((-1, 3))\n            .mean(axis=1)\n            .astype(\"int16\")\n        )\n        sr = 16000\n    return sr, y\n\n\ndef transcribe(speech, stream):\n    _, y = reformat_freq(*speech)\n    if stream is None:\n        stream = model.createStream()\n    stream.feedAudioContent(y)\n    text = stream.intermediateDecode()\n    return text, stream\n\n</code></pre></div>\n\n<p>Then, create a Gradio Interface as before (the only difference being that the return type should be <code>numpy</code> instead of a <code>filepath</code> to be compatible with the DeepSpeech models)</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"numpy\"), \n        \"state\" \n    ], \n    outputs= [\n        \"text\", \n        \"state\"\n    ], \n    live=True).launch()\n</code></pre></div>\n\n<p>Running all of this should allow you to deploy your realtime ASR model with a nice GUI. Try it out and see how well it works for you.</p>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a web-based GUI for your ASR model. </p>\n\n<p>Fun tip: you can share your ASR model instantly with others simply by setting <code>share=True</code> in <code>launch()</code>. </p>\n", "tags": ["ASR", "SPEECH", "STREAMING"], "spaces": ["https://huggingface.co/spaces/abidlabs/streaming-asr-paused", "https://huggingface.co/spaces/abidlabs/full-context-asr"], "url": "/guides/real-time-speech-recognition/", "contributor": null}], "preprocessing": "No preprocessing is performed", "postprocessing": "No postprocessing is performed", "parent": "gradio", "prev_obj": "Slider", "next_obj": "Textbox"}, "textbox": {"class": null, "name": "Textbox", "description": "Creates a textarea for user to enter string input or display string output. <br>", "tags": {"preprocessing": "passes textarea value as a {str} into the function.", "postprocessing": "expects a {str} returned from function and sets textarea value to it.", "examples-format": "a {str} representing the textbox input.", "demos": "hello_world, diff_texts, sentence_builder", "guides": "creating-a-chatbot, real-time-speech-recognition"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | Callable | None", "doc": "default text to provide in textarea. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "\"\""}, {"name": "lines", "annotation": "int", "doc": "minimum number of line rows to provide in textarea.", "default": "1"}, {"name": "max_lines", "annotation": "int", "doc": "maximum number of line rows to provide in textarea.", "default": "20"}, {"name": "placeholder", "annotation": "str | None", "doc": "placeholder hint to provide behind textarea.", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, will be rendered as an editable textbox; if False, editing will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}, {"name": "type", "annotation": "str", "doc": "The type of textbox. One of: 'text', 'password', 'email', Default is 'text'.", "default": "\"text\""}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Textbox"}, {"fn": null, "name": "submit", "description": "This event is triggered when the user presses the Enter key while the component (e.g. a textbox) is focused. This method can be used when this component is in a Gradio Blocks. <br> <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Textbox"}, {"fn": null, "name": "blur", "description": "This event is triggered when the component's is unfocused/blurred (e.g. when the user clicks outside of a textbox). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "Callable function"}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Textbox"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "container", "annotation": "bool | None", "doc": "If True, will place the component in a container - providing some extra padding around the border.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Textbox"}], "string_shortcuts": [["Textbox", "textbox", "Uses default values"], ["TextArea", "textarea", "Uses lines=7"]], "demos": [["hello_world", "import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n    \nif __name__ == \"__main__\":\n    demo.launch()   "], ["diff_texts", "from difflib import Differ\n\nimport gradio as gr\n\n\ndef diff_texts(text1, text2):\n    d = Differ()\n    return [\n        (token[2:], token[0] if token[0] != \" \" else None)\n        for token in d.compare(text1, text2)\n    ]\n\n\ndemo = gr.Interface(\n    diff_texts,\n    [\n        gr.Textbox(\n            label=\"Initial text\",\n            lines=3,\n            value=\"The quick brown fox jumped over the lazy dogs.\",\n        ),\n        gr.Textbox(\n            label=\"Text to compare\",\n            lines=3,\n            value=\"The fast brown fox jumps over lazy dogs.\",\n        ),\n    ],\n    gr.HighlightedText(\n        label=\"Diff\",\n        combine_adjacent=True,\n    ).style(color_map={\"+\": \"red\", \"-\": \"green\"}),\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["sentence_builder", "import gradio as gr\n\n\ndef sentence_builder(quantity, animal, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4),\n        gr.Dropdown([\"cat\", \"dog\", \"bird\"]),\n        gr.Radio([\"park\", \"zoo\", \"road\"]),\n        gr.Dropdown([\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True),\n        gr.Checkbox(label=\"Is it the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", \"park\", [\"ran\", \"swam\"], True],\n        [4, \"dog\", \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", \"road\", [\"ran\"], False],\n        [8, \"cat\", \"zoo\", [\"ate\"], True],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()", "submit()", "blur()"], "events": "change(), submit(), blur()", "guides": [{"name": "creating-a-chatbot", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 27, "pretty_name": "Creating A Chatbot", "content": "# How to Create a Chatbot\n\nRelated spaces: https://huggingface.co/spaces/dawood/chatbot-guide, https://huggingface.co/spaces/dawood/chatbot-guide-multimodal, https://huggingface.co/spaces/ThomasSimonini/Chat-with-Gandalf-GPT-J6B, https://huggingface.co/spaces/gorkemgoknar/moviechatbot, https://huggingface.co/spaces/Kirili4ik/chat-with-Kirill\nTags: NLP, TEXT, HTML\n\n## Introduction\n\nChatbots are widely studied in natural language processing (NLP) research and are a common use case of NLP in industry. Because chatbots are designed to be used directly by customers and end users, it is important to validate that chatbots are behaving as expected when confronted with a wide variety of input prompts. \n\nUsing `gradio`, you can easily build a demo of your chatbot model and share that with a testing team, or test it yourself using an intuitive chatbot GUI.\n\nThis tutorial will show how to take a pretrained chatbot model and deploy it with a Gradio interface in 4 steps. The live chatbot interface that we create will look something like this (try it!):\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nChatbots are *stateful*, meaning that the model's prediction can change depending on how the user has previously interacted with the model. So, in this tutorial, we will also cover how to use **state** with Gradio demos. \n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/quickstart). To use a pretrained chatbot model, also install `transformers` and `torch`. \n\nLet's get started! Here's how to build your own chatbot: \n\n  [1. Set up the Chatbot Model](#1-set-up-the-chatbot-model)\n  [2. Define a `predict` function](#2-define-a-predict-function)\n  [3. Create a Gradio Demo using Blocks](#3-create-a-gradio-demo-using-blocks)\n  [4. Chatbot Markdown Support](#4-chatbot-markdown-support)\n\n## 1. Set up the Chatbot Model\n\nFirst, you will need to have a chatbot model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will use a pretrained chatbot model, `DialoGPT`, and its tokenizer from the [Hugging Face Hub](https://huggingface.co/microsoft/DialoGPT-medium), but you can replace this with your own model. \n\nHere is the code to load `DialoGPT` from Hugging Face `transformers`.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n```\n\n## 2. Define a `predict` function\n\nNext, you will need to define a function that takes in the *user input* as well as the previous *chat history* to generate a response.\n\nIn the case of our pretrained model, it will look like this:\n\n```python\ndef predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response \n    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n    return response, history\n```\n\nLet's break this down. The function takes two parameters:\n\n* `input`: which is what the user enters (through the Gradio GUI) in a particular step of the conversation. \n* `history`: which represents the **state**, consisting of the list of user and bot responses. To create a stateful Gradio demo, we *must* pass in a parameter to represent the state, and we set the default value of this parameter to be the initial value of the state (in this case, the empty list since this is what we would like the chat history to be at the start).\n\nThen, the function tokenizes the input and concatenates it with the tokens corresponding to the previous user and bot responses. Then, this is fed into the pretrained model to get a prediction. Finally, we do some cleaning up so that we can return two values from our function:\n\n* `response`: which is a list of tuples of strings corresponding to all of the user and bot responses. This will be rendered as the output in the Gradio demo.\n* `history` variable, which is the token representation of all of the user and bot responses. In stateful Gradio demos, we *must* return the updated state at the end of the function. \n\n## 3. Create a Gradio Demo using Blocks\n\nNow that we have our predictive function set up, we can create a Gradio demo around it. \n\nIn this case, our function takes in two values, a text input and a state input. The corresponding input components in `gradio` are `\"text\"` and `\"state\"`. \n\nThe function also returns two values. We will display the list of responses using the dedicated `\"chatbot\"` component and use the `\"state\"` output component type for the second return value.\n\nNote that the `\"state\"` input and output components are not displayed. \n\n```python\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    state = gr.State([])\n    \n    with gr.Row():\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n            \n    txt.submit(predict, [txt, state], [chatbot, state])\n            \ndemo.launch()\n```\n\nThis produces the following demo, which you can try right here in your browser (try typing in some simple greetings like \"Hi!\" to get started):\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\n----------\n\n## 4. Chatbot Markdown Support\n\n\nThe `gr.Chatbot` also supports a subset of markdown including bold, italics, code, and images. Let's take a look at how we can use the markdown support to allow a user to submit images to the chatbot component.\n\n```python\ndef add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n```\n\n\nNotice the `add_image` function takes in both the `state` and `image` and appends the user submitted image to `state` by using markdown. \n\n\n```python\nimport gradio as gr\n\ndef add_text(state, text):\n    state = state + [(text, text + \"?\")]\n    return state, state\n\ndef add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n\n\nwith gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n    state = gr.State([])\n    \n    with gr.Row():\n        with gr.Column(scale=0.85):\n            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\n        with gr.Column(scale=0.15, min_width=0):\n            btn = gr.UploadButton(\"\ud83d\uddbc\ufe0f\", file_types=[\"image\"])\n            \n    txt.submit(add_text, [state, txt], [state, chatbot])\n    txt.submit(lambda :\"\", None, txt)\n    btn.upload(add_image, [state, btn], [state, chatbot])\n            \ndemo.launch()\n```\n\nThis is the code for a chatbot with a textbox for a user to submit text and an image upload button to submit images. The rest of the demo code is creating an interface using blocks; basically adding a couple more components compared to section 3.\n\nThis code will produce a demo like the one below:\n\n<iframe src=\"https://dawood-chatbot-guide-multimodal.hf.space\" frameBorder=\"0\" height=\"650\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nAnd you're done! That's all the code you need to build an interface for your chatbot model. Here are some references that you may find useful:\n\n* Gradio's [Quickstart guide](https://gradio.app/quickstart/)\n* The first chatbot demo [chatbot demo](https://huggingface.co/spaces/dawood/chatbot-guide) and [complete code](https://huggingface.co/spaces/dawood/chatbot-guide/blob/main/app.py) (on Hugging Face Spaces)\n* The final chatbot with markdown support [chatbot demo](https://huggingface.co/spaces/dawood/chatbot-guide-multimodal) and [complete code](https://huggingface.co/spaces/dawood/chatbot-guide-multimodal/blob/main/app.py) (on Hugging Face Spaces)\n", "html": "<h1 id=\"how-to-create-a-chatbot\">How to Create a Chatbot</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Chatbots are widely studied in natural language processing (NLP) research and are a common use case of NLP in industry. Because chatbots are designed to be used directly by customers and end users, it is important to validate that chatbots are behaving as expected when confronted with a wide variety of input prompts. </p>\n\n<p>Using <code>gradio</code>, you can easily build a demo of your chatbot model and share that with a testing team, or test it yourself using an intuitive chatbot GUI.</p>\n\n<p>This tutorial will show how to take a pretrained chatbot model and deploy it with a Gradio interface in 4 steps. The live chatbot interface that we create will look something like this (try it!):</p>\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Chatbots are <em>stateful</em>, meaning that the model's prediction can change depending on how the user has previously interacted with the model. So, in this tutorial, we will also cover how to use <strong>state</strong> with Gradio demos. </p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/quickstart\">installed</a>. To use a pretrained chatbot model, also install <code>transformers</code> and <code>torch</code>. </p>\n\n<p>Let's get started! Here's how to build your own chatbot: </p>\n\n<p><a href=\"#1-set-up-the-chatbot-model\">1. Set up the Chatbot Model</a>\n  <a href=\"#2-define-a-predict-function\">2. Define a <code>predict</code> function</a>\n  <a href=\"#3-create-a-gradio-demo-using-blocks\">3. Create a Gradio Demo using Blocks</a>\n  <a href=\"#4-chatbot-markdown-support\">4. Chatbot Markdown Support</a></p>\n\n<h2 id=\"1-set-up-the-chatbot-model\">1. Set up the Chatbot Model</h2>\n\n<p>First, you will need to have a chatbot model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will use a pretrained chatbot model, <code>DialoGPT</code>, and its tokenizer from the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/microsoft/DialoGPT-medium\">Hugging Face Hub</a>, but you can replace this with your own model. </p>\n\n<p>Here is the code to load <code>DialoGPT</code> from Hugging Face <code>transformers</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n</code></pre></div>\n\n<h2 id=\"2-define-a-predict-function\">2. Define a <code>predict</code> function</h2>\n\n<p>Next, you will need to define a function that takes in the <em>user input</em> as well as the previous <em>chat history</em> to generate a response.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response \n    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n    return response, history\n</code></pre></div>\n\n<p>Let's break this down. The function takes two parameters:</p>\n\n<ul>\n<li><code>input</code>: which is what the user enters (through the Gradio GUI) in a particular step of the conversation. </li>\n<li><code>history</code>: which represents the <strong>state</strong>, consisting of the list of user and bot responses. To create a stateful Gradio demo, we <em>must</em> pass in a parameter to represent the state, and we set the default value of this parameter to be the initial value of the state (in this case, the empty list since this is what we would like the chat history to be at the start).</li>\n</ul>\n\n<p>Then, the function tokenizes the input and concatenates it with the tokens corresponding to the previous user and bot responses. Then, this is fed into the pretrained model to get a prediction. Finally, we do some cleaning up so that we can return two values from our function:</p>\n\n<ul>\n<li><code>response</code>: which is a list of tuples of strings corresponding to all of the user and bot responses. This will be rendered as the output in the Gradio demo.</li>\n<li><code>history</code> variable, which is the token representation of all of the user and bot responses. In stateful Gradio demos, we <em>must</em> return the updated state at the end of the function. </li>\n</ul>\n\n<h2 id=\"3-create-a-gradio-demo-using-blocks\">3. Create a Gradio Demo using Blocks</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio demo around it. </p>\n\n<p>In this case, our function takes in two values, a text input and a state input. The corresponding input components in <code>gradio</code> are <code>\"text\"</code> and <code>\"state\"</code>. </p>\n\n<p>The function also returns two values. We will display the list of responses using the dedicated <code>\"chatbot\"</code> component and use the <code>\"state\"</code> output component type for the second return value.</p>\n\n<p>Note that the <code>\"state\"</code> input and output components are not displayed. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    state = gr.State([])\n\n    with gr.Row():\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n\n    txt.submit(predict, [txt, state], [chatbot, state])\n\ndemo.launch()\n</code></pre></div>\n\n<p>This produces the following demo, which you can try right here in your browser (try typing in some simple greetings like \"Hi!\" to get started):</p>\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<h2 id=\"4-chatbot-markdown-support\">4. Chatbot Markdown Support</h2>\n\n<p>The <code>gr.Chatbot</code> also supports a subset of markdown including bold, italics, code, and images. Let's take a look at how we can use the markdown support to allow a user to submit images to the chatbot component.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n</code></pre></div>\n\n<p>Notice the <code>add_image</code> function takes in both the <code>state</code> and <code>image</code> and appends the user submitted image to <code>state</code> by using markdown. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef add_text(state, text):\n    state = state + [(text, text + \"?\")]\n    return state, state\n\ndef add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n\n\nwith gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n    state = gr.State([])\n\n    with gr.Row():\n        with gr.Column(scale=0.85):\n            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\n        with gr.Column(scale=0.15, min_width=0):\n            btn = gr.UploadButton(\"\ud83d\uddbc\ufe0f\", file_types=[\"image\"])\n\n    txt.submit(add_text, [state, txt], [state, chatbot])\n    txt.submit(lambda :\"\", None, txt)\n    btn.upload(add_image, [state, btn], [state, chatbot])\n\ndemo.launch()\n</code></pre></div>\n\n<p>This is the code for a chatbot with a textbox for a user to submit text and an image upload button to submit images. The rest of the demo code is creating an interface using blocks; basically adding a couple more components compared to section 3.</p>\n\n<p>This code will produce a demo like the one below:</p>\n\n<iframe src=\"https://dawood-chatbot-guide-multimodal.hf.space\" frameBorder=\"0\" height=\"650\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>And you're done! That's all the code you need to build an interface for your chatbot model. Here are some references that you may find useful:</p>\n\n<ul>\n<li>Gradio's <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/quickstart/\">Quickstart guide</a></li>\n<li>The first chatbot demo <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide\">chatbot demo</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide/blob/main/app.py\">complete code</a> (on Hugging Face Spaces)</li>\n<li>The final chatbot with markdown support <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide-multimodal\">chatbot demo</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide-multimodal/blob/main/app.py\">complete code</a> (on Hugging Face Spaces)</li>\n</ul>\n", "tags": ["NLP", "TEXT", "HTML"], "spaces": ["https://huggingface.co/spaces/dawood/chatbot-guide", "https://huggingface.co/spaces/dawood/chatbot-guide-multimodal", "https://huggingface.co/spaces/ThomasSimonini/Chat-with-Gandalf-GPT-J6B", "https://huggingface.co/spaces/gorkemgoknar/moviechatbot", "https://huggingface.co/spaces/Kirili4ik/chat-with-Kirill"], "url": "/guides/creating-a-chatbot/", "contributor": null}, {"name": "real-time-speech-recognition", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 33, "pretty_name": "Real Time Speech Recognition", "content": "# Real Time Speech Recognition \n\nRelated spaces: https://huggingface.co/spaces/abidlabs/streaming-asr-paused, https://huggingface.co/spaces/abidlabs/full-context-asr\nTags: ASR, SPEECH, STREAMING\n\n## Introduction\n\nAutomatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).\n\nUsing `gradio`, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.\n\nThis tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a ***full-context*** model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it ***streaming***, meaning that the audio model will convert speech as you speak. The streaming demo that we create will look something like this (try it below or [in a new tab](https://huggingface.co/spaces/abidlabs/streaming-asr-paused)!):\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nReal-time ASR is inherently *stateful*, meaning that the model's predictions change depending on what words the user previously spoke. So, in this tutorial, we will also cover how to use **state** with Gradio demos. \n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:\n\n* Transformers (for this, `pip install transformers` and `pip install torch`) \n* DeepSpeech (`pip install deepspeech==0.8.2`)\n\nMake sure you have at least one of these installed so that you can follow along the tutorial. You will also need `ffmpeg` [installed on your system](https://www.ffmpeg.org/download.html), if you do not already have it, to process files from the microphone.\n\nHere's how to build a real time speech recognition (ASR) app: \n\n1. [Set up the Transformers ASR Model](#1-set-up-the-transformers-asr-model)\n2. [Create a Full-Context ASR Demo with Transformers](#2-create-a-full-context-asr-demo-with-transformers) \n3. [Create a Streaming ASR Demo  with Transformers](#3-create-a-streaming-asr-demo-with-transformers)\n4. [Create a Streaming ASR Demo with DeepSpeech](#4-create-a-streaming-asr-demo-with-deepspeech)\n\n\n## 1. Set up the Transformers ASR Model\n\nFirst, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the Hugging Face model, `Wav2Vec2`. \n\nHere is the code to load `Wav2Vec2` from Hugging Face `transformers`.\n\n```python\nfrom transformers import pipeline\n\np = pipeline(\"automatic-speech-recognition\")\n```\n\nThat's it! By default, the automatic speech recognition model pipeline loads Facebook's `facebook/wav2vec2-base-960h` model.\n\n## 2. Create a Full-Context ASR Demo with Transformers \n\nWe will start by creating a *full-context* ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the `pipeline` object above.\n\nWe will use `gradio`'s built in `Audio` component, configured to take input from the user's microphone and return a filepath for the recorded audio. The output component will be a plain `Textbox`.\n\n```python\nimport gradio as gr\n\ndef transcribe(audio):\n    text = p(audio)[\"text\"]\n    return text\n\ngr.Interface(\n    fn=transcribe, \n    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n    outputs=\"text\").launch()\n```\n\nSo what's happening here? The `transcribe` function takes a single parameter, `audio`, which is a filepath to the audio file that the user has recorded. The `pipeline` object expects a filepath and converts it to text, which is returned to the frontend and displayed in a textbox. \n\nLet's see it in action! (Record a short audio clip and then click submit, or [open in a new tab](https://huggingface.co/spaces/abidlabs/full-context-asr)):\n\n<iframe src=\"https://abidlabs-full-context-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n## 3. Create a Streaming ASR Demo  with Transformers\n\nOk great! We've built an ASR model that works well for short audio clips. However, if you are recording longer audio clips, you probably want a *streaming* interface, one that transcribes audio as the user speaks instead of just all-at-once at the end.\n\nThe good news is that it's not too difficult to adapt the demo we just made to make it streaming, using the same `Wav2Vec2` model. \n\nThe biggest change is that we must now introduce a `state` parameter, which holds the audio that has been *transcribed so far*. This allows us to only the latest chunk of audio and simply append it to the audio we previously transcribed. \n\nWhen adding state to a Gradio demo, you need to do a total of 3 things:\n\n* Add a `state` parameter to the function\n* Return the updated `state` at the end of the function\n* Add the `\"state\"` components to the `inputs` and `outputs` in `Interface` \n\nHere's what the code looks like:\n\n```python\ndef transcribe(audio, state=\"\"):\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\n# Set the starting state to an empty string\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\" \n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n```\n\nNotice that we've also made one other change, which is that we've set `live=True`. This keeps the Gradio interface running constantly, so it automatically transcribes audio without the user having to repeatedly hit the submit button.\n\nLet's see how it does (try below or [in a new tab](https://huggingface.co/spaces/abidlabs/streaming-asr))!\n\n<iframe src=\"https://abidlabs-streaming-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nOne thing that you may notice is that the transcription quality has dropped since the chunks of audio are so small, they lack the context to properly be transcribed. A \"hacky\" fix to this is to simply increase the runtime of the `transcribe()` function so that longer audio chunks are processed. We can do this by adding a `time.sleep()` inside the function, as shown below (we'll see a proper fix next) \n\n```python\nfrom transformers import pipeline\nimport gradio as gr\nimport time\n\np = pipeline(\"automatic-speech-recognition\")\n\ndef transcribe(audio, state=\"\"):\n    time.sleep(2)\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\"\n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n```\n\nTry the demo below to see the difference (or [open in a new tab](https://huggingface.co/spaces/abidlabs/streaming-asr-paused))!\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\n## 4. Create a Streaming ASR Demo with DeepSpeech\n\nYou're not restricted to ASR models from the `transformers` library -- you can use your own models or models from other libraries. The `DeepSpeech` library contains models that are specifically designed to handle streaming audio data. These models perform really well with  streaming data as they are able to account for previous chunks of audio data when making predictions.\n\nGoing through the DeepSpeech library is beyond the scope of this Guide (check out their [excellent documentation here](https://deepspeech.readthedocs.io/en/r0.9/)), but you can use Gradio very similarly with a DeepSpeech ASR model as with a Transformers ASR model. \n\nHere's a complete example (on Linux):\n\nFirst install the DeepSpeech library and download the pretrained models from the terminal:\n\n```bash\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.pbmm\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.scorer\napt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\npip install deepspeech==0.8.2\n```\n\nThen, create a similar `transcribe()` function as before:\n\n```python\nfrom deepspeech import Model\nimport numpy as np\n\nmodel_file_path = \"deepspeech-0.8.2-models.pbmm\"\nlm_file_path = \"deepspeech-0.8.2-models.scorer\"\nbeam_width = 100\nlm_alpha = 0.93\nlm_beta = 1.18\n\nmodel = Model(model_file_path)\nmodel.enableExternalScorer(lm_file_path)\nmodel.setScorerAlphaBeta(lm_alpha, lm_beta)\nmodel.setBeamWidth(beam_width)\n\n\ndef reformat_freq(sr, y):\n    if sr not in (\n        48000,\n        16000,\n    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)\n        raise ValueError(\"Unsupported rate\", sr)\n    if sr == 48000:\n        y = (\n            ((y / max(np.max(y), 1)) * 32767)\n            .reshape((-1, 3))\n            .mean(axis=1)\n            .astype(\"int16\")\n        )\n        sr = 16000\n    return sr, y\n\n\ndef transcribe(speech, stream):\n    _, y = reformat_freq(*speech)\n    if stream is None:\n        stream = model.createStream()\n    stream.feedAudioContent(y)\n    text = stream.intermediateDecode()\n    return text, stream\n\n```\n\nThen, create a Gradio Interface as before (the only difference being that the return type should be `numpy` instead of a `filepath` to be compatible with the DeepSpeech models)\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"numpy\"), \n        \"state\" \n    ], \n    outputs= [\n        \"text\", \n        \"state\"\n    ], \n    live=True).launch()\n```\n\nRunning all of this should allow you to deploy your realtime ASR model with a nice GUI. Try it out and see how well it works for you.\n\n--------------------------------------------\n\n\nAnd you're done! That's all the code you need to build a web-based GUI for your ASR model. \n\nFun tip: you can share your ASR model instantly with others simply by setting `share=True` in `launch()`. \n\n\n", "html": "<h1 id=\"real-time-speech-recognition\">Real Time Speech Recognition</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Automatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).</p>\n\n<p>Using <code>gradio</code>, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.</p>\n\n<p>This tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a <strong><em>full-context</em></strong> model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it <strong><em>streaming</em></strong>, meaning that the audio model will convert speech as you speak. The streaming demo that we create will look something like this (try it below or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/streaming-asr-paused\">in a new tab</a>!):</p>\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Real-time ASR is inherently <em>stateful</em>, meaning that the model's predictions change depending on what words the user previously spoke. So, in this tutorial, we will also cover how to use <strong>state</strong> with Gradio demos. </p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:</p>\n\n<ul>\n<li>Transformers (for this, <code>pip install transformers</code> and <code>pip install torch</code>) </li>\n<li>DeepSpeech (<code>pip install deepspeech==0.8.2</code>)</li>\n</ul>\n\n<p>Make sure you have at least one of these installed so that you can follow along the tutorial. You will also need <code>ffmpeg</code> <a rel=\"noopener\" target=\"_blank\" href=\"https://www.ffmpeg.org/download.html\">installed on your system</a>, if you do not already have it, to process files from the microphone.</p>\n\n<p>Here's how to build a real time speech recognition (ASR) app: </p>\n\n<ol>\n<li><a href=\"#1-set-up-the-transformers-asr-model\">Set up the Transformers ASR Model</a></li>\n<li><a href=\"#2-create-a-full-context-asr-demo-with-transformers\">Create a Full-Context ASR Demo with Transformers</a> </li>\n<li><a href=\"#3-create-a-streaming-asr-demo-with-transformers\">Create a Streaming ASR Demo  with Transformers</a></li>\n<li><a href=\"#4-create-a-streaming-asr-demo-with-deepspeech\">Create a Streaming ASR Demo with DeepSpeech</a></li>\n</ol>\n\n<h2 id=\"1-set-up-the-transformers-asr-model\">1. Set up the Transformers ASR Model</h2>\n\n<p>First, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the Hugging Face model, <code>Wav2Vec2</code>. </p>\n\n<p>Here is the code to load <code>Wav2Vec2</code> from Hugging Face <code>transformers</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import pipeline\n\np = pipeline(\"automatic-speech-recognition\")\n</code></pre></div>\n\n<p>That's it! By default, the automatic speech recognition model pipeline loads Facebook's <code>facebook/wav2vec2-base-960h</code> model.</p>\n\n<h2 id=\"2-create-a-full-context-asr-demo-with-transformers\">2. Create a Full-Context ASR Demo with Transformers</h2>\n\n<p>We will start by creating a <em>full-context</em> ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the <code>pipeline</code> object above.</p>\n\n<p>We will use <code>gradio</code>'s built in <code>Audio</code> component, configured to take input from the user's microphone and return a filepath for the recorded audio. The output component will be a plain <code>Textbox</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef transcribe(audio):\n    text = p(audio)[\"text\"]\n    return text\n\ngr.Interface(\n    fn=transcribe, \n    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n    outputs=\"text\").launch()\n</code></pre></div>\n\n<p>So what's happening here? The <code>transcribe</code> function takes a single parameter, <code>audio</code>, which is a filepath to the audio file that the user has recorded. The <code>pipeline</code> object expects a filepath and converts it to text, which is returned to the frontend and displayed in a textbox. </p>\n\n<p>Let's see it in action! (Record a short audio clip and then click submit, or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/full-context-asr\">open in a new tab</a>):</p>\n\n<iframe src=\"https://abidlabs-full-context-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"3-create-a-streaming-asr-demo-with-transformers\">3. Create a Streaming ASR Demo  with Transformers</h2>\n\n<p>Ok great! We've built an ASR model that works well for short audio clips. However, if you are recording longer audio clips, you probably want a <em>streaming</em> interface, one that transcribes audio as the user speaks instead of just all-at-once at the end.</p>\n\n<p>The good news is that it's not too difficult to adapt the demo we just made to make it streaming, using the same <code>Wav2Vec2</code> model. </p>\n\n<p>The biggest change is that we must now introduce a <code>state</code> parameter, which holds the audio that has been <em>transcribed so far</em>. This allows us to only the latest chunk of audio and simply append it to the audio we previously transcribed. </p>\n\n<p>When adding state to a Gradio demo, you need to do a total of 3 things:</p>\n\n<ul>\n<li>Add a <code>state</code> parameter to the function</li>\n<li>Return the updated <code>state</code> at the end of the function</li>\n<li>Add the <code>\"state\"</code> components to the <code>inputs</code> and <code>outputs</code> in <code>Interface</code> </li>\n</ul>\n\n<p>Here's what the code looks like:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def transcribe(audio, state=\"\"):\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\n# Set the starting state to an empty string\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\" \n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n</code></pre></div>\n\n<p>Notice that we've also made one other change, which is that we've set <code>live=True</code>. This keeps the Gradio interface running constantly, so it automatically transcribes audio without the user having to repeatedly hit the submit button.</p>\n\n<p>Let's see how it does (try below or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/streaming-asr\">in a new tab</a>)!</p>\n\n<iframe src=\"https://abidlabs-streaming-asr.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>One thing that you may notice is that the transcription quality has dropped since the chunks of audio are so small, they lack the context to properly be transcribed. A \"hacky\" fix to this is to simply increase the runtime of the <code>transcribe()</code> function so that longer audio chunks are processed. We can do this by adding a <code>time.sleep()</code> inside the function, as shown below (we'll see a proper fix next) </p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import pipeline\nimport gradio as gr\nimport time\n\np = pipeline(\"automatic-speech-recognition\")\n\ndef transcribe(audio, state=\"\"):\n    time.sleep(2)\n    text = p(audio)[\"text\"]\n    state += text + \" \"\n    return state, state\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"filepath\", streaming=True), \n        \"state\"\n    ],\n    outputs=[\n        \"textbox\",\n        \"state\"\n    ],\n    live=True).launch()\n</code></pre></div>\n\n<p>Try the demo below to see the difference (or <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/abidlabs/streaming-asr-paused\">open in a new tab</a>)!</p>\n\n<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"4-create-a-streaming-asr-demo-with-deepspeech\">4. Create a Streaming ASR Demo with DeepSpeech</h2>\n\n<p>You're not restricted to ASR models from the <code>transformers</code> library -- you can use your own models or models from other libraries. The <code>DeepSpeech</code> library contains models that are specifically designed to handle streaming audio data. These models perform really well with  streaming data as they are able to account for previous chunks of audio data when making predictions.</p>\n\n<p>Going through the DeepSpeech library is beyond the scope of this Guide (check out their <a rel=\"noopener\" target=\"_blank\" href=\"https://deepspeech.readthedocs.io/en/r0.9/\">excellent documentation here</a>), but you can use Gradio very similarly with a DeepSpeech ASR model as with a Transformers ASR model. </p>\n\n<p>Here's a complete example (on Linux):</p>\n\n<p>First install the DeepSpeech library and download the pretrained models from the terminal:</p>\n\n<div class='codeblock'><pre><code class='lang-bash'>wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.pbmm\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.scorer\napt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\npip install deepspeech==0.8.2\n</code></pre></div>\n\n<p>Then, create a similar <code>transcribe()</code> function as before:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from deepspeech import Model\nimport numpy as np\n\nmodel_file_path = \"deepspeech-0.8.2-models.pbmm\"\nlm_file_path = \"deepspeech-0.8.2-models.scorer\"\nbeam_width = 100\nlm_alpha = 0.93\nlm_beta = 1.18\n\nmodel = Model(model_file_path)\nmodel.enableExternalScorer(lm_file_path)\nmodel.setScorerAlphaBeta(lm_alpha, lm_beta)\nmodel.setBeamWidth(beam_width)\n\n\ndef reformat_freq(sr, y):\n    if sr not in (\n        48000,\n        16000,\n    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)\n        raise ValueError(\"Unsupported rate\", sr)\n    if sr == 48000:\n        y = (\n            ((y / max(np.max(y), 1)) * 32767)\n            .reshape((-1, 3))\n            .mean(axis=1)\n            .astype(\"int16\")\n        )\n        sr = 16000\n    return sr, y\n\n\ndef transcribe(speech, stream):\n    _, y = reformat_freq(*speech)\n    if stream is None:\n        stream = model.createStream()\n    stream.feedAudioContent(y)\n    text = stream.intermediateDecode()\n    return text, stream\n\n</code></pre></div>\n\n<p>Then, create a Gradio Interface as before (the only difference being that the return type should be <code>numpy</code> instead of a <code>filepath</code> to be compatible with the DeepSpeech models)</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(\n    fn=transcribe, \n    inputs=[\n        gr.Audio(source=\"microphone\", type=\"numpy\"), \n        \"state\" \n    ], \n    outputs= [\n        \"text\", \n        \"state\"\n    ], \n    live=True).launch()\n</code></pre></div>\n\n<p>Running all of this should allow you to deploy your realtime ASR model with a nice GUI. Try it out and see how well it works for you.</p>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a web-based GUI for your ASR model. </p>\n\n<p>Fun tip: you can share your ASR model instantly with others simply by setting <code>share=True</code> in <code>launch()</code>. </p>\n", "tags": ["ASR", "SPEECH", "STREAMING"], "spaces": ["https://huggingface.co/spaces/abidlabs/streaming-asr-paused", "https://huggingface.co/spaces/abidlabs/full-context-asr"], "url": "/guides/real-time-speech-recognition/", "contributor": null}], "preprocessing": "passes textarea value as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> into the function.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> returned from function and sets textarea value to it.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> representing the textbox input.", "parent": "gradio", "prev_obj": "State", "next_obj": "Timeseries"}, "timeseries": {"class": null, "name": "Timeseries", "description": "Creates a component that can be used to upload/preview timeseries csv files or display a dataframe consisting of a time series graphically.", "tags": {"preprocessing": "passes the uploaded timeseries data as a {pandas.DataFrame} into the function", "postprocessing": "expects a {pandas.DataFrame} or {str} path to a csv to be returned, which is then displayed as a timeseries graph", "examples-format": "a {str} filepath of csv data with time series data.", "demos": "fraud_detector"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | Callable | None", "doc": "File path for the timeseries csv file. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "x", "annotation": "str | None", "doc": "Column name of x (time) series. None if csv has no headers, in which case first column is x series.", "default": "None"}, {"name": "y", "annotation": "str | List[str] | None", "doc": "Column name of y series, or list of column names if multiple series. None if csv has no headers, in which case every column after first is a y series.", "default": "None"}, {"name": "colors", "annotation": "List[str] | None", "doc": "an ordered list of colors to use for each line plot", "default": "None"}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, will allow users to upload a timeseries csv; if False, can only be used to display timeseries data. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Timeseries"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the TimeSeries component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}], "returns": {}, "example": null, "parent": "gradio.Timeseries"}], "string_shortcuts": [["Timeseries", "timeseries", "Uses default values"]], "demos": [["fraud_detector", "import random\nimport os\nimport gradio as gr\n\n\ndef fraud_detector(card_activity, categories, sensitivity):\n    activity_range = random.randint(0, 100)\n    drop_columns = [\n        column for column in [\"retail\", \"food\", \"other\"] if column not in categories\n    ]\n    if len(drop_columns):\n        card_activity.drop(columns=drop_columns, inplace=True)\n    return (\n        card_activity,\n        card_activity,\n        {\"fraud\": activity_range / 100.0, \"not fraud\": 1 - activity_range / 100.0},\n    )\n\n\ndemo = gr.Interface(\n    fraud_detector,\n    [\n        gr.Timeseries(x=\"time\", y=[\"retail\", \"food\", \"other\"]),\n        gr.CheckboxGroup(\n            [\"retail\", \"food\", \"other\"], value=[\"retail\", \"food\", \"other\"]\n        ),\n        gr.Slider(1, 3),\n    ],\n    [\n        \"dataframe\",\n        gr.Timeseries(x=\"time\", y=[\"retail\", \"food\", \"other\"]),\n        gr.Label(label=\"Fraud Level\"),\n    ],\n    examples=[\n        [os.path.join(os.path.dirname(__file__), \"fraud.csv\"), [\"retail\", \"food\", \"other\"], 1.0],\n    ],\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()"], "events": "change()", "preprocessing": "passes the uploaded timeseries data as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >pandas.DataFrame</span> into the function", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >pandas.DataFrame</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> path to a csv to be returned, which is then displayed as a timeseries graph", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath of csv data with time series data.", "parent": "gradio", "prev_obj": "Textbox", "next_obj": "UploadButton"}, "uploadbutton": {"class": null, "name": "UploadButton", "description": "Used to create an upload button, when cicked allows a user to upload files that satisfy the specified file type or generic files (if file_type not set).", "tags": {"preprocessing": "passes the uploaded file as a {file-object} or {List[file-object]} depending on `file_count` (or a {bytes}/{List{bytes}} depending on `type`)", "postprocessing": "expects function to return a {str} path to a file, or {List[str]} consisting of paths to files.", "examples-format": "a {str} path to a local file that populates the component.", "demos": "upload_button"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "label", "annotation": "str", "doc": "Text to display on the button. Defaults to \"Upload a File\".", "default": "\"Upload a File\""}, {"name": "value", "annotation": "str | List[str] | Callable | None", "doc": "Default text for the button to display.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}, {"name": "type", "annotation": "str", "doc": "Type of value to be returned by component. \"file\" returns a temporary file object whose path can be retrieved by file_obj.name and original filename can be retrieved with file_obj.orig_name, \"binary\" returns an bytes object.", "default": "\"file\""}, {"name": "file_count", "annotation": "str", "doc": "if single, allows user to upload one file. If \"multiple\", user uploads multiple files. If \"directory\", user uploads all files in selected directory. Return type will be list for each file in case of \"multiple\" or \"directory\".", "default": "\"single\""}, {"name": "file_types", "annotation": "List[str] | None", "doc": "List of type of files to be uploaded. \"file\" allows any file to be uploaded, \"image\" allows only image files to be uploaded, \"audio\" allows only audio files to be uploaded, \"video\" allows only video files to be uploaded, \"text\" allows only text files to be uploaded.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "click", "description": "This event is triggered when the component (e.g. a button) is clicked. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "<class 'inspect._empty'>", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.UploadButton"}, {"fn": null, "name": "upload", "description": "This event is triggered when the user uploads a file into the component (e.g. when the user uploads a video into a video component). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "Callable function"}, {"name": "inputs", "annotation": "List[Component]", "doc": "List of inputs"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of outputs", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.UploadButton"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the button component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "full_width", "annotation": "bool | None", "doc": "If True, will expand to fill parent container.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.UploadButton"}], "string_shortcuts": [["UploadButton", "uploadbutton", "Uses default values"]], "demos": [["upload_button", "import gradio as gr\n\ndef upload_file(files):\n    file_paths = [file.name for file in files]\n    return file_paths\n\nwith gr.Blocks() as demo:\n    file_output = gr.File()\n    upload_button = gr.UploadButton(\"Click to Upload a File\", file_types=[\"image\", \"video\"], file_count=\"multiple\")\n    upload_button.upload(upload_file, upload_button, file_output)\n\ndemo.launch()\n"]], "events_list": ["click()", "upload()"], "events": "click(), upload()", "preprocessing": "passes the uploaded file as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >file-object</span> or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[file-object]</span> depending on `file_count` (or a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >bytes</span>/<span class='text-orange-500' style='font-family: monospace; font-size: large;' >List<span class='text-orange-500' style='font-family: monospace; font-size: large;' >bytes</span></span> depending on `type`)", "postprocessing": "expects function to return a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> path to a file, or <span class='text-orange-500' style='font-family: monospace; font-size: large;' >List[str]</span> consisting of paths to files.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> path to a local file that populates the component.", "parent": "gradio", "prev_obj": "Timeseries", "next_obj": "Video"}, "video": {"class": null, "name": "Video", "description": "Creates a video component that can be used to upload/record videos (as an input) or display videos (as an output). For the video to be playable in the browser it must have a compatible container and codec combination. Allowed combinations are .mp4 with h264 codec, .ogg with theora codec, and .webm with vp9 codec. If the component detects that the output video would not be playable in the browser it will attempt to convert it to a playable mp4 video. If the conversion fails, the original video is returned.", "tags": {"preprocessing": "passes the uploaded video as a {str} filepath or URL whose extension can be modified by `format`.", "postprocessing": "expects a {str} filepath to a video which is displayed.", "examples-format": "a {str} filepath to a local file that contains the video.", "demos": "video_identity"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "value", "annotation": "str | Callable | None", "doc": "A path or URL for the default value that Video component is going to take. If callable, the function will be called whenever the app loads to set the initial value of the component.", "default": "None"}, {"name": "format", "annotation": "str | None", "doc": "Format of video format to be returned by component, such as 'avi' or 'mp4'. Use 'mp4' to ensure browser playability. If set to None, video will keep uploaded format.", "default": "None"}, {"name": "source", "annotation": "str", "doc": "Source of video. \"upload\" creates a box where user can drop an video file, \"webcam\" allows user to record a video from their webcam.", "default": "\"upload\""}, {"name": "label", "annotation": "str | None", "doc": "component name in interface.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.", "default": "None"}, {"name": "show_label", "annotation": "bool", "doc": "if True, will display label.", "default": "True"}, {"name": "interactive", "annotation": "bool | None", "doc": "if True, will allow users to upload a video; if False, can only be used to display videos. If not provided, this is inferred based on whether the component is used as an input or output.", "default": "None"}, {"name": "visible", "annotation": "bool", "doc": "If False, component will be hidden.", "default": "True"}, {"name": "elem_id", "annotation": "str | None", "doc": "An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.", "default": "None"}, {"name": "mirror_webcam", "annotation": "bool", "doc": "If True webcam will be mirrored. Default is True.", "default": "True"}, {"name": "include_audio", "annotation": "bool | None", "doc": "Whether the component should record/retain the audio track for a video. By default, audio is excluded for webcam videos and included for uploaded videos.", "default": "None"}], "returns": {"annotation": null}, "example": null, "fns": [{"fn": null, "name": "change", "description": "This event is triggered when the component's input value changes (e.g. when the user types in a textbox or uploads an image). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Video"}, {"fn": null, "name": "clear", "description": "This event is triggered when the user clears the component (e.g. image or audio) using the X button for the component. This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Video"}, {"fn": null, "name": "play", "description": "This event is triggered when the user plays the component (e.g. audio or video). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Video"}, {"fn": null, "name": "pause", "description": "This event is triggered when the user pauses the component (e.g. audio or video). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Video"}, {"fn": null, "name": "stop", "description": "This event is triggered when the user stops the component (e.g. audio or video). This method can be used when this component is in a Gradio Blocks. <br>", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "fn", "annotation": "Callable | None", "doc": "the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."}, {"name": "inputs", "annotation": "Component | List[Component] | Set[Component] | None", "doc": "List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.", "default": "None"}, {"name": "outputs", "annotation": "Component | List[Component] | None", "doc": "List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.", "default": "None"}, {"name": "api_name", "annotation": "str | None", "doc": "Defining this parameter exposes the endpoint in the api docs", "default": "None"}, {"name": "status_tracker", "annotation": "StatusTracker | None", "doc": null, "default": "None"}, {"name": "scroll_to_output", "annotation": "bool", "doc": "If True, will scroll to output component on completion", "default": "False"}, {"name": "show_progress", "annotation": "bool", "doc": "If True, will show progress animation while pending", "default": "True"}, {"name": "queue", "annotation": "bool | None", "doc": "If True, will place the request on the queue, if the queue exists", "default": "None"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.", "default": "False"}, {"name": "max_batch_size", "annotation": "int", "doc": "Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)", "default": "4"}, {"name": "preprocess", "annotation": "bool", "doc": "If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "If False, will not run postprocessing of component data before returning 'fn' output to the browser.", "default": "True"}, {"name": "cancels", "annotation": "Dict[str, Any] | List[Dict[str, Any]] | None", "doc": "A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.", "default": "None"}, {"name": "every", "annotation": "float | None", "doc": "Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Video"}, {"fn": null, "name": "style", "description": "This method can be used to change the appearance of the video component.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "height", "annotation": "int | None", "doc": "Height of the video.", "default": "None"}, {"name": "width", "annotation": "int | None", "doc": "Width of the video.", "default": "None"}], "returns": {}, "example": null, "parent": "gradio.Video"}], "string_shortcuts": [["Video", "video", "Uses default values"], ["PlayableVideo", "playablevideo", "Uses format=\"mp4\""]], "demos": [["video_identity", "import gradio as gr\nimport os\n\n\ndef video_identity(video):\n    return video\n\n\ndemo = gr.Interface(video_identity, \n                    gr.Video(), \n                    \"playable_video\", \n                    examples=[\n                        os.path.join(os.path.dirname(__file__), \n                                     \"video/video_sample.mp4\")], \n                    cache_examples=True)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "events_list": ["change()", "clear()", "pause()", "play()", "stop()", "upload()"], "events": "change(), clear(), pause(), play(), stop(), upload()", "preprocessing": "passes the uploaded video as a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath or URL whose extension can be modified by `format`.", "postprocessing": "expects a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath to a video which is displayed.", "examples-format": "a <span class='text-orange-500' style='font-family: monospace; font-size: large;' >str</span> filepath to a local file that contains the video.", "parent": "gradio", "prev_obj": "UploadButton", "next_obj": "Examples"}}, "helpers": {"examples": {"class": null, "name": "Examples", "description": "This class is a wrapper over the Dataset component and can be used to create Examples for Blocks / Interfaces. Populates the Dataset component with examples and assigns event listener so that clicking on an example populates the input/output components. Optionally handles example caching for fast inference. <br>", "tags": {"demos": "blocks_inputs, fake_gan", "guides": "more-on-examples-and-flagging, using-hugging-face-integrations, image-classification-in-pytorch, image-classification-in-tensorflow, image-classification-with_vision_transformers, create_your_own_friends_with_a_gan"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "examples", "annotation": "List[Any] | List[List[Any]] | str", "doc": "example inputs that can be clicked to populate specific components. Should be nested list, in which the outer list consists of samples and each inner list consists of an input corresponding to each input component. A string path to a directory of examples can also be provided but it should be within the directory with the python file running the gradio app. If there are multiple input components and a directory is provided, a log.csv file must be present in the directory to link corresponding inputs."}, {"name": "inputs", "annotation": "IOComponent | List[IOComponent]", "doc": "the component or list of components corresponding to the examples"}, {"name": "outputs", "annotation": "IOComponent | List[IOComponent] | None", "doc": "optionally, provide the component or list of components corresponding to the output of the examples. Required if `cache` is True.", "default": "None"}, {"name": "fn", "annotation": "Callable | None", "doc": "optionally, provide the function to run to generate the outputs corresponding to the examples. Required if `cache` is True.", "default": "None"}, {"name": "cache_examples", "annotation": "bool", "doc": "if True, caches examples for fast runtime. If True, then `fn` and `outputs` need to be provided", "default": "False"}, {"name": "examples_per_page", "annotation": "int", "doc": "how many examples to show per page.", "default": "10"}, {"name": "label", "annotation": "str | None", "doc": "the label to use for the examples component (by default, \"Examples\")", "default": "\"Examples\""}, {"name": "elem_id", "annotation": "str | None", "doc": "an optional string that is assigned as the id of this component in the HTML DOM.", "default": "None"}, {"name": "run_on_click", "annotation": "bool", "doc": "if cache_examples is False, clicking on an example does not run the function when an example is clicked. Set this to True to run the function when an example is clicked. Has no effect if cache_examples is True.", "default": "False"}, {"name": "preprocess", "annotation": "bool", "doc": "if True, preprocesses the example input before running the prediction function and caching the output. Only applies if cache_examples is True.", "default": "True"}, {"name": "postprocess", "annotation": "bool", "doc": "if True, postprocesses the example output after running the prediction function and before caching. Only applies if cache_examples is True.", "default": "True"}, {"name": "batch", "annotation": "bool", "doc": "If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. Used only if cache_examples is True.", "default": "False"}], "returns": {"annotation": null}, "example": null, "fns": [], "demos": [["blocks_inputs", "import gradio as gr\nimport os\n\n\ndef combine(a, b):\n    return a + \" \" + b\n\n\ndef mirror(x):\n    return x\n\n\nwith gr.Blocks() as demo:\n\n    txt = gr.Textbox(label=\"Input\", lines=2)\n    txt_2 = gr.Textbox(label=\"Input 2\")\n    txt_3 = gr.Textbox(value=\"\", label=\"Output\")\n    btn = gr.Button(value=\"Submit\")\n    btn.click(combine, inputs=[txt, txt_2], outputs=[txt_3])\n\n    with gr.Row():\n        im = gr.Image()\n        im_2 = gr.Image()\n\n    btn = gr.Button(value=\"Mirror Image\")\n    btn.click(mirror, inputs=[im], outputs=[im_2])\n\n    gr.Markdown(\"## Text Examples\")\n    gr.Examples(\n        [[\"hi\", \"Adam\"], [\"hello\", \"Eve\"]],\n        [txt, txt_2],\n        txt_3,\n        combine,\n        cache_examples=True,\n    )\n    gr.Markdown(\"## Image Examples\")\n    gr.Examples(\n        examples=[os.path.join(os.path.dirname(__file__), \"lion.jpg\")],\n        inputs=im,\n        outputs=im_2,\n        fn=mirror,\n        cache_examples=True,\n    )\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["fake_gan", "# This demo needs to be run from the repo folder.\n# python demo/fake_gan/run.py\nimport os\nimport random\n\nimport gradio as gr\n\n\ndef fake_gan():\n    images = [\n        (random.choice(\n            [\n                \"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=387&q=80\",\n                \"https://images.unsplash.com/photo-1554151228-14d9def656e4?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=386&q=80\",\n                \"https://images.unsplash.com/photo-1542909168-82c3e7fdca5c?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8aHVtYW4lMjBmYWNlfGVufDB8fDB8fA%3D%3D&w=1000&q=80\",\n                \"https://images.unsplash.com/photo-1546456073-92b9f0a8d413?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=387&q=80\",\n                \"https://images.unsplash.com/photo-1601412436009-d964bd02edbc?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=464&q=80\",\n            ]\n        ), f\"label {i}\" if i != 0 else \"label\" * 50)\n        for i in range(3)\n    ]\n    return images\n\n\nwith gr.Blocks() as demo:\n    with gr.Column(variant=\"panel\"):\n        with gr.Row(variant=\"compact\"):\n            text = gr.Textbox(\n                label=\"Enter your prompt\",\n                show_label=False,\n                max_lines=1,\n                placeholder=\"Enter your prompt\",\n            ).style(\n                container=False,\n            )\n            btn = gr.Button(\"Generate image\").style(full_width=False)\n\n        gallery = gr.Gallery(\n            label=\"Generated images\", show_label=False, elem_id=\"gallery\"\n        ).style(grid=[2], height=\"auto\")\n\n    btn.click(fake_gan, None, gallery)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]], "guides": [{"name": "using-hugging-face-integrations", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": 1, "absolute_index": 13, "pretty_name": "Using Hugging Face Integrations", "content": "# Using Hugging Face Integrations\n\nRelated spaces: https://huggingface.co/spaces/farukozderim/Model-Comparator-Space-Builder, https://huggingface.co/spaces/osanseviero/helsinki_translation_en_es, https://huggingface.co/spaces/osanseviero/remove-bg-webcam, https://huggingface.co/spaces/mrm8488/GPT-J-6B, https://huggingface.co/spaces/akhaliq/T0pp, https://huggingface.co/spaces/osanseviero/mix_match_gradio\nTags: HUB, SPACES, EMBED\n\nContributed by <a href=\"https://huggingface.co/osanseviero\">Omar Sanseviero</a> \ud83e\udd99 and <a href=\"https://huggingface.co/farukozderim\">\u00d6mer Faruk \u00d6zdemir</a>\n\n## Introduction\n\nThe Hugging Face Hub is a central platform that has over 90,000 [models](https://huggingface.co/models), 14,000 [datasets](https://huggingface.co/datasets) and 14,000 [demos](https://huggingface.co/spaces), also known as Spaces. From Natural Language Processing to Computer Vision and Speech, the Hub supports multiple domains. Although Hugging Face is famous for its \ud83e\udd17 transformers and diffusers libraries, the Hub also supports dozens of ML libraries, such as PyTorch, TensorFlow, spaCy, and many others.\n\nGradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.\n\n## Using regular inference with `pipeline`\n\nFirst, let's build a simple interface that translates text from English to Spanish. Between the over a thousand models shared by the University of Helsinki, there is an [existing model](https://huggingface.co/Helsinki-NLP/opus-mt-en-es), `opus-mt-en-es`, that does precisely this!\n\nThe \ud83e\udd17 transformers library has a very easy-to-use abstraction, [`pipeline()`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline) that handles most of the complex code to offer a simple API for common tasks. By specifying the task and an (optional) model, you can use an existing model with few lines:\n\n```python\nimport gradio as gr\n\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n\ndef predict(text):\n  return pipe(text)[0][\"translation_text\"]\n  \niface = gr.Interface(\n  fn=predict, \n  inputs='text',\n  outputs='text',\n  examples=[[\"Hello! My name is Omar\"]]\n)\n\niface.launch()\n```\n\nThe previous code produces the following interface, which you can try right here in your browser: \n\n<iframe src=\"https://osanseviero-helsinki-translation-en-es.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nThis demo requires installing four libraries: gradio, torch, transformers, and sentencepiece. Apart from that, this is a Gradio with the structure you're used to! The demo is a usual Gradio `Interface` with a prediction function, a specified input, and a specified output. The prediction function executes the `pipeline` function with the given input, retrieves the first (and only) translation result, and returns the `translation_text` field, which you're interested in.\n\n## Using Hugging Face Inference API\n\nHugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production.\n\nLet's try the same demo as above but using the Inference API instead of loading the model yourself. Given a Hugging Face model supported in the Inference API, Gradio can automatically infer the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function. Here is what the code would look like!\n\n```python\nimport gradio as gr\n\niface = gr.Interface.load(\"huggingface/Helsinki-NLP/opus-mt-en-es\",\n  examples=[[\"Hello! My name is Omar\"]]\n)\n\niface.launch()\n```\n\nLet's go over some of the key differences:\n\n* `Interface.load()` is used instead of the usual `Interface()`.\n* `Interface.load()` receives a string with the prefix `huggingface/`, and then the model repository ID.\n* Since the input, output and prediction functions are not needed, you only need to modify the UI parts (such as `title`, `description`, and `examples`).\n* There is no need to install any dependencies (except Gradio) since you are not loading the model on your computer.\n\nYou might notice that the first inference takes about 20 seconds. This happens since the Inference API is loading the model in the server. You get some benefits afterward:\n\n* The inference will be much faster.\n* The server caches your requests.\n* You get built-in automatic scaling.\n\n## Hosting your Gradio demos\n\n\n[Hugging Face Spaces](https://hf.co/spaces) allows anyone to host their Gradio demos freely. The community shares oven 2,000 Spaces. Uploading your Gradio demos take a couple of minutes. You can head to [hf.co/new-space](https://huggingface.co/new-space), select the Gradio SDK, create an `app.py` file, and voila! You have a demo you can share with anyone else.\n\n## Building demos based on other demos\n\nYou can use the existing Spaces to tweak the UI or combine multiple demos. Let's find how to do this! First, let's take a look at an existing demo that does background removal. \n\nThis is a Gradio demo [already shared](https://huggingface.co/spaces/eugenesiow/remove-bg) by a community member. You can load an existing demo using `Interface` in a syntax similar to how it's done for the Inference API. It just takes two lines of code and with the prefix `spaces`.\n\n```python\nimport gradio as gr\n\ngr.Interface.load(\"spaces/eugenesiow/remove-bg\").launch()\n```\n\nThe code snippet above will load the same interface as the corresponding Space demo.\n\n<iframe src=\"https://eugenesiow-remove-bg.hf.space\" frameBorder=\"0\" height=\"900\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nYou can change UI elements, such as the title or theme, but also change the expected type. The previous Space expected users to upload images. What if you would like users to have their webcam and remove the background from there? You can load the Space but change the source of input as follows:\n\n```python\nimport gradio as gr\n\ngr.Interface.load(\n  \"spaces/eugenesiow/remove-bg\", \n  inputs=[gr.Image(label=\"Input Image\", source=\"webcam\")]\n).launch()\n```\n\nThe code above generates the following demo.\n\n<iframe src=\"https://osanseviero-remove-bg-webcam.hf.space\" frameBorder=\"0\" height=\"600\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nAs you can see, the demo looks the same, but it uses a webcam input instead of user-uploaded images.\n\nYou can learn more about this feature, and how to use it with the new Blocks API in the [Using Gradio Blocks Like Functions guide](/using_blocks_like_functions)\n\n## Using multiple Spaces\n\nSometimes a single model inference will not be enough: you might want to call multiple models by piping them (using the output of model A as the input of model B). `Series` can achieve this. Other times, you might want to run two models in parallel to compare them. `Parallel` can do this!\n\nLet's combine the notion of running things in parallel with the Spaces integration. The [GPT-J-6B](https://huggingface.co/spaces/mrm8488/GPT-J-6B) Space demos a model that generates text using a model called GPT-J. The [T0pp](https://huggingface.co/spaces/akhaliq/T0pp) Space demos another generative model called T0pp. Let's see how to combine both into one.\n\n```python\nimport gradio as gr\n\niface1 = gr.Interface.load(\"spaces/mrm8488/GPT-J-6B\")\niface2 = gr.Interface.load(\"spaces/akhaliq/T0pp\")\n\niface3 = gr.mix.Parallel(\n  iface1, iface2, \n  examples = [\n    ['Which country will win the 2002 World Cup?'],\n    [\"A is the son's of B's uncle. What is the family relationship between A and B?\"],\n    [\"In 2030, \"],\n  ])\n  \niface3.launch()\n```\n\n`iface1` and `iface2` are loading existing Spaces. Then, with `Parallel`, you can run the interfaces parallelly. When you click submit, you will get the output for both interfaces. This is how the demo looks like:\n\n<iframe src=\"https://osanseviero-mix-match-gradio.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nAlthough both models are generative, you can see that the way both models behave is very different. That's a powerful application of `Parallel`!\n\n## Creating Spaces with python\n\nMaking use of the [huggingface_hub client library](https://huggingface.co/docs/huggingface_hub/index) library you can create new Spaces or model repositories. You can do this even in a Gradio Space! You can find an example space [here](https://huggingface.co/spaces/farukozderim/Model-Comparator-Space-Builder). This Space creates a new Space comparing different models or spaces with the support of Gradio `load` and `Parallel`. Now you can try creating cool spaces with all kinds of functionality \ud83d\ude0e.\n\n```python\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file,\n)\ncreate_repo(name=target_space_name, token=hf_token, repo_type=\"space\", space_sdk=\"gradio\")\nrepo_name = get_full_repo_name(model_id=target_space_name, token=hf_token)\nfile_url = upload_file(\n    path_or_fileobj=\"file.txt\",\n    path_in_repo=\"app.py\",\n    repo_id=repo_name,\n    repo_type=\"space\",\n    token=hf_token,\n)\n```\nHere, `create_repo` creates a gradio repo with the target name under a specific account using that account's Write Token. `repo_name` gets the full repo name of the related repo. Finally `upload_file` uploads a file inside the repo with the name `app.py`.\n\n<iframe src=\"https://farukozderim-model-comparator-space-builder.hf.space\" frameBorder=\"0\" height=\"800\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\n## Embedding your Space demo on other websites\n\nThroughout this guide, you've seen there are Gradio demos embedded. You can also do this on own website! The first step is to create a Space with the demo you want to showcase. You can embed it in your HTML code, as shown in the following self-contained example.\n\n```html\n<iframe src=\"https://osanseviero-mix-match-gradio.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe&gt;\n```\n\n## Recap\n\nThat's it! Let's recap what you can do:\n\n1. Host your Gradio demos in Spaces.\n2. Use the Inference API to build demos in two lines of code.\n3. Load existing Spaces and modify them.\n4. Combine multiple Spaces by running them sequentially or parallelly.\n5. Embed your Space demo directly on a website.\n\n\ud83e\udd17\n", "html": "<h1 id=\"using-hugging-face-integrations\">Using Hugging Face Integrations</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>The Hugging Face Hub is a central platform that has over 90,000 <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/models\">models</a>, 14,000 <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/datasets\">datasets</a> and 14,000 <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces\">demos</a>, also known as Spaces. From Natural Language Processing to Computer Vision and Speech, the Hub supports multiple domains. Although Hugging Face is famous for its \ud83e\udd17 transformers and diffusers libraries, the Hub also supports dozens of ML libraries, such as PyTorch, TensorFlow, spaCy, and many others.</p>\n\n<p>Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.</p>\n\n<h2 id=\"using-regular-inference-with-pipeline\">Using regular inference with <code>pipeline</code></h2>\n\n<p>First, let's build a simple interface that translates text from English to Spanish. Between the over a thousand models shared by the University of Helsinki, there is an <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/Helsinki-NLP/opus-mt-en-es\">existing model</a>, <code>opus-mt-en-es</code>, that does precisely this!</p>\n\n<p>The \ud83e\udd17 transformers library has a very easy-to-use abstraction, <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline\"><code>pipeline()</code></a> that handles most of the complex code to offer a simple API for common tasks. By specifying the task and an (optional) model, you can use an existing model with few lines:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n\ndef predict(text):\n  return pipe(text)[0][\"translation_text\"]\n\niface = gr.Interface(\n  fn=predict, \n  inputs='text',\n  outputs='text',\n  examples=[[\"Hello! My name is Omar\"]]\n)\n\niface.launch()\n</code></pre></div>\n\n<p>The previous code produces the following interface, which you can try right here in your browser: </p>\n\n<iframe src=\"https://osanseviero-helsinki-translation-en-es.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>This demo requires installing four libraries: gradio, torch, transformers, and sentencepiece. Apart from that, this is a Gradio with the structure you're used to! The demo is a usual Gradio <code>Interface</code> with a prediction function, a specified input, and a specified output. The prediction function executes the <code>pipeline</code> function with the given input, retrieves the first (and only) translation result, and returns the <code>translation_text</code> field, which you're interested in.</p>\n\n<h2 id=\"using-hugging-face-inference-api\">Using Hugging Face Inference API</h2>\n\n<p>Hugging Face has a free service called the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/inference-api\">Inference API</a>, which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/pricing\">Inference Endpoints</a> when you want to use it in production.</p>\n\n<p>Let's try the same demo as above but using the Inference API instead of loading the model yourself. Given a Hugging Face model supported in the Inference API, Gradio can automatically infer the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function. Here is what the code would look like!</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\niface = gr.Interface.load(\"huggingface/Helsinki-NLP/opus-mt-en-es\",\n  examples=[[\"Hello! My name is Omar\"]]\n)\n\niface.launch()\n</code></pre></div>\n\n<p>Let's go over some of the key differences:</p>\n\n<ul>\n<li><code>Interface.load()</code> is used instead of the usual <code>Interface()</code>.</li>\n<li><code>Interface.load()</code> receives a string with the prefix <code>huggingface/</code>, and then the model repository ID.</li>\n<li>Since the input, output and prediction functions are not needed, you only need to modify the UI parts (such as <code>title</code>, <code>description</code>, and <code>examples</code>).</li>\n<li>There is no need to install any dependencies (except Gradio) since you are not loading the model on your computer.</li>\n</ul>\n\n<p>You might notice that the first inference takes about 20 seconds. This happens since the Inference API is loading the model in the server. You get some benefits afterward:</p>\n\n<ul>\n<li>The inference will be much faster.</li>\n<li>The server caches your requests.</li>\n<li>You get built-in automatic scaling.</li>\n</ul>\n\n<h2 id=\"hosting-your-gradio-demos\">Hosting your Gradio demos</h2>\n\n<p><a rel=\"noopener\" target=\"_blank\" href=\"https://hf.co/spaces\">Hugging Face Spaces</a> allows anyone to host their Gradio demos freely. The community shares oven 2,000 Spaces. Uploading your Gradio demos take a couple of minutes. You can head to <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/new-space\">hf.co/new-space</a>, select the Gradio SDK, create an <code>app.py</code> file, and voila! You have a demo you can share with anyone else.</p>\n\n<h2 id=\"building-demos-based-on-other-demos\">Building demos based on other demos</h2>\n\n<p>You can use the existing Spaces to tweak the UI or combine multiple demos. Let's find how to do this! First, let's take a look at an existing demo that does background removal. </p>\n\n<p>This is a Gradio demo <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/eugenesiow/remove-bg\">already shared</a> by a community member. You can load an existing demo using <code>Interface</code> in a syntax similar to how it's done for the Inference API. It just takes two lines of code and with the prefix <code>spaces</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface.load(\"spaces/eugenesiow/remove-bg\").launch()\n</code></pre></div>\n\n<p>The code snippet above will load the same interface as the corresponding Space demo.</p>\n\n<iframe src=\"https://eugenesiow-remove-bg.hf.space\" frameBorder=\"0\" height=\"900\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>You can change UI elements, such as the title or theme, but also change the expected type. The previous Space expected users to upload images. What if you would like users to have their webcam and remove the background from there? You can load the Space but change the source of input as follows:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface.load(\n  \"spaces/eugenesiow/remove-bg\", \n  inputs=[gr.Image(label=\"Input Image\", source=\"webcam\")]\n).launch()\n</code></pre></div>\n\n<p>The code above generates the following demo.</p>\n\n<iframe src=\"https://osanseviero-remove-bg-webcam.hf.space\" frameBorder=\"0\" height=\"600\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>As you can see, the demo looks the same, but it uses a webcam input instead of user-uploaded images.</p>\n\n<p>You can learn more about this feature, and how to use it with the new Blocks API in the <a rel=\"noopener\" target=\"_blank\" href=\"/using_blocks_like_functions\">Using Gradio Blocks Like Functions guide</a></p>\n\n<h2 id=\"using-multiple-spaces\">Using multiple Spaces</h2>\n\n<p>Sometimes a single model inference will not be enough: you might want to call multiple models by piping them (using the output of model A as the input of model B). <code>Series</code> can achieve this. Other times, you might want to run two models in parallel to compare them. <code>Parallel</code> can do this!</p>\n\n<p>Let's combine the notion of running things in parallel with the Spaces integration. The <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/mrm8488/GPT-J-6B\">GPT-J-6B</a> Space demos a model that generates text using a model called GPT-J. The <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/akhaliq/T0pp\">T0pp</a> Space demos another generative model called T0pp. Let's see how to combine both into one.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\niface1 = gr.Interface.load(\"spaces/mrm8488/GPT-J-6B\")\niface2 = gr.Interface.load(\"spaces/akhaliq/T0pp\")\n\niface3 = gr.mix.Parallel(\n  iface1, iface2, \n  examples = [\n    ['Which country will win the 2002 World Cup?'],\n    [\"A is the son's of B's uncle. What is the family relationship between A and B?\"],\n    [\"In 2030, \"],\n  ])\n\niface3.launch()\n</code></pre></div>\n\n<p><code>iface1</code> and <code>iface2</code> are loading existing Spaces. Then, with <code>Parallel</code>, you can run the interfaces parallelly. When you click submit, you will get the output for both interfaces. This is how the demo looks like:</p>\n\n<iframe src=\"https://osanseviero-mix-match-gradio.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Although both models are generative, you can see that the way both models behave is very different. That's a powerful application of <code>Parallel</code>!</p>\n\n<h2 id=\"creating-spaces-with-python\">Creating Spaces with python</h2>\n\n<p>Making use of the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/docs/huggingface_hub/index\">huggingface_hub client library</a> library you can create new Spaces or model repositories. You can do this even in a Gradio Space! You can find an example space <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/farukozderim/Model-Comparator-Space-Builder\">here</a>. This Space creates a new Space comparing different models or spaces with the support of Gradio <code>load</code> and <code>Parallel</code>. Now you can try creating cool spaces with all kinds of functionality \ud83d\ude0e.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file,\n)\ncreate_repo(name=target_space_name, token=hf_token, repo_type=\"space\", space_sdk=\"gradio\")\nrepo_name = get_full_repo_name(model_id=target_space_name, token=hf_token)\nfile_url = upload_file(\n    path_or_fileobj=\"file.txt\",\n    path_in_repo=\"app.py\",\n    repo_id=repo_name,\n    repo_type=\"space\",\n    token=hf_token,\n)\n</code></pre></div>\n\n<p>Here, <code>create_repo</code> creates a gradio repo with the target name under a specific account using that account's Write Token. <code>repo_name</code> gets the full repo name of the related repo. Finally <code>upload_file</code> uploads a file inside the repo with the name <code>app.py</code>.</p>\n\n<iframe src=\"https://farukozderim-model-comparator-space-builder.hf.space\" frameBorder=\"0\" height=\"800\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"embedding-your-space-demo-on-other-websites\">Embedding your Space demo on other websites</h2>\n\n<p>Throughout this guide, you've seen there are Gradio demos embedded. You can also do this on own website! The first step is to create a Space with the demo you want to showcase. You can embed it in your HTML code, as shown in the following self-contained example.</p>\n\n<div class='codeblock'><pre><code class='lang-html'><iframe src=\"https://osanseviero-mix-match-gradio.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe&gt;\n</code></pre></div>\n\n<h2 id=\"recap\">Recap</h2>\n\n<p>That's it! Let's recap what you can do:</p>\n\n<ol>\n<li>Host your Gradio demos in Spaces.</li>\n<li>Use the Inference API to build demos in two lines of code.</li>\n<li>Load existing Spaces and modify them.</li>\n<li>Combine multiple Spaces by running them sequentially or parallelly.</li>\n<li>Embed your Space demo directly on a website.</li>\n</ol>\n\n<p>\ud83e\udd17</p>\n", "tags": ["HUB", "SPACES", "EMBED"], "spaces": ["https://huggingface.co/spaces/farukozderim/Model-Comparator-Space-Builder", "https://huggingface.co/spaces/osanseviero/helsinki_translation_en_es", "https://huggingface.co/spaces/osanseviero/remove-bg-webcam", "https://huggingface.co/spaces/mrm8488/GPT-J-6B", "https://huggingface.co/spaces/akhaliq/T0pp", "https://huggingface.co/spaces/osanseviero/mix_match_gradio"], "url": "/guides/using-hugging-face-integrations/", "contributor": "<a href=\"https://huggingface.co/osanseviero\">Omar Sanseviero</a> \ud83e\udd99 and <a href=\"https://huggingface.co/farukozderim\">\u00d6mer Faruk \u00d6zdemir</a>"}, {"name": "image-classification-in-pytorch", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 17, "pretty_name": "Image Classification In Pytorch", "content": "# Image Classification in PyTorch\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/pytorch-image-classifier, https://huggingface.co/spaces/pytorch/ResNet, https://huggingface.co/spaces/pytorch/ResNext, https://huggingface.co/spaces/pytorch/SqueezeNet\nTags: VISION, RESNET, PYTORCH\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging. \n\nSuch models are perfect to use with Gradio's *image* input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained image classification model, so you should also have `torch` installed.\n\n## Step 1 \u2014 Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from [PyTorch Hub](https://pytorch.org/hub/pytorch_vision_resnet/). You can use a different pretrained model or train your own. \n\n```python\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n```\n\nBecause we will be using the model for inference, we have called the `.eval()` method.\n\n## Step 2 \u2014 Defining a `predict` function\n\nNext, we will need to define a function that takes in the *user input*, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}    \n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n* `inp`: the input image as a `PIL` image\n\nThen, the function converts the image to a PIL Image and then eventually a PyTorch `tensor`, passes it through the model, and returns:\n\n* `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 \u2014 Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. \n\nIn this case, the input component is a drag-and-drop image component. To create this input, we use `Image(type=\"pil\")` which creates the component and handles the preprocessing to convert that to a `PIL` image. \n\nThe output component will be a `Label`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as `Label(num_top_classes=3)`.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=predict, \n             inputs=gr.Image(type=\"pil\"),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n\n", "html": "<h1 id=\"image-classification-in-pytorch\">Image Classification in PyTorch</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging. </p>\n\n<p>Such models are perfect to use with Gradio's <em>image</em> input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):</p>\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started!</p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. We will be using a pretrained image classification model, so you should also have <code>torch</code> installed.</p>\n\n<h2 id=\"step-1-setting-up-the-image-classification-model\">Step 1 \u2014 Setting up the Image Classification Model</h2>\n\n<p>First, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from <a rel=\"noopener\" target=\"_blank\" href=\"https://pytorch.org/hub/pytorch_vision_resnet/\">PyTorch Hub</a>. You can use a different pretrained model or train your own. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import torch\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n</code></pre></div>\n\n<p>Because we will be using the model for inference, we have called the <code>.eval()</code> method.</p>\n\n<h2 id=\"step-2-defining-a-predict-function\">Step 2 \u2014 Defining a <code>predict</code> function</h2>\n\n<p>Next, we will need to define a function that takes in the <em>user input</em>, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this <a rel=\"noopener\" target=\"_blank\" href=\"https://git.io/JJkYN\">text file</a>.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import requests\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}    \n  return confidences\n</code></pre></div>\n\n<p>Let's break this down. The function takes one parameter:</p>\n\n<ul>\n<li><code>inp</code>: the input image as a <code>PIL</code> image</li>\n</ul>\n\n<p>Then, the function converts the image to a PIL Image and then eventually a PyTorch <code>tensor</code>, passes it through the model, and returns:</p>\n\n<ul>\n<li><code>confidences</code>: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities</li>\n</ul>\n\n<h2 id=\"step-3-creating-a-gradio-interface\">Step 3 \u2014 Creating a Gradio Interface</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio Interface around it. </p>\n\n<p>In this case, the input component is a drag-and-drop image component. To create this input, we use <code>Image(type=\"pil\")</code> which creates the component and handles the preprocessing to convert that to a <code>PIL</code> image. </p>\n\n<p>The output component will be a <code>Label</code>, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as <code>Label(num_top_classes=3)</code>.</p>\n\n<p>Finally, we'll add one more parameter, the <code>examples</code>, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(fn=predict, \n             inputs=gr.Image(type=\"pil\"),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()\n</code></pre></div>\n\n<p>This produces the following interface, which you can try right here in your browser (try uploading your own examples!):</p>\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting <code>share=True</code> when you <code>launch()</code> the Interface!</p>\n", "tags": ["VISION", "RESNET", "PYTORCH"], "spaces": ["https://huggingface.co/spaces/abidlabs/pytorch-image-classifier", "https://huggingface.co/spaces/pytorch/ResNet", "https://huggingface.co/spaces/pytorch/ResNext", "https://huggingface.co/spaces/pytorch/SqueezeNet"], "url": "/guides/image-classification-in-pytorch/", "contributor": null}, {"name": "image-classification-in-tensorflow", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 18, "pretty_name": "Image Classification In Tensorflow", "content": "# Image Classification in TensorFlow and Keras\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/keras-image-classifier\nTags: VISION, MOBILENET, TENSORFLOW\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging. \n\nSuch models are perfect to use with Gradio's *image* input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained Keras image classification model, so you should also have `tensorflow` installed.\n\n## Step 1 \u2014 Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from [Keras](https://keras.io/api/applications/mobilenet/). You can use a different pretrained model or train your own. \n\n```python\nimport tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n```\n\nThis line automatically downloads the MobileNet model and weights using the Keras library.  \n\n## Step 2 \u2014 Defining a `predict` function\n\nNext, we will need to define a function that takes in the *user input*, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n* `inp`: the input image as a `numpy` array\n\nThen, the function adds a batch dimension, passes it through the model, and returns:\n\n* `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 \u2014 Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. \n\nIn this case, the input component is a drag-and-drop image component. To create this input, we can use the `\"gradio.inputs.Image\"` class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\n\nThe output component will be a `\"label\"`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=classify_image, \n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n\n", "html": "<h1 id=\"image-classification-in-tensorflow-and-keras\">Image Classification in TensorFlow and Keras</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging. </p>\n\n<p>Such models are perfect to use with Gradio's <em>image</em> input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):</p>\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started!</p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. We will be using a pretrained Keras image classification model, so you should also have <code>tensorflow</code> installed.</p>\n\n<h2 id=\"step-1-setting-up-the-image-classification-model\">Step 1 \u2014 Setting up the Image Classification Model</h2>\n\n<p>First, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from <a rel=\"noopener\" target=\"_blank\" href=\"https://keras.io/api/applications/mobilenet/\">Keras</a>. You can use a different pretrained model or train your own. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n</code></pre></div>\n\n<p>This line automatically downloads the MobileNet model and weights using the Keras library.  </p>\n\n<h2 id=\"step-2-defining-a-predict-function\">Step 2 \u2014 Defining a <code>predict</code> function</h2>\n\n<p>Next, we will need to define a function that takes in the <em>user input</em>, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this <a rel=\"noopener\" target=\"_blank\" href=\"https://git.io/JJkYN\">text file</a>.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n</code></pre></div>\n\n<p>Let's break this down. The function takes one parameter:</p>\n\n<ul>\n<li><code>inp</code>: the input image as a <code>numpy</code> array</li>\n</ul>\n\n<p>Then, the function adds a batch dimension, passes it through the model, and returns:</p>\n\n<ul>\n<li><code>confidences</code>: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities</li>\n</ul>\n\n<h2 id=\"step-3-creating-a-gradio-interface\">Step 3 \u2014 Creating a Gradio Interface</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio Interface around it. </p>\n\n<p>In this case, the input component is a drag-and-drop image component. To create this input, we can use the <code>\"gradio.inputs.Image\"</code> class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.</p>\n\n<p>The output component will be a <code>\"label\"</code>, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.</p>\n\n<p>Finally, we'll add one more parameter, the <code>examples</code>, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(fn=classify_image, \n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n</code></pre></div>\n\n<p>This produces the following interface, which you can try right here in your browser (try uploading your own examples!):</p>\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting <code>share=True</code> when you <code>launch()</code> the Interface!</p>\n", "tags": ["VISION", "MOBILENET", "TENSORFLOW"], "spaces": ["https://huggingface.co/spaces/abidlabs/keras-image-classifier"], "url": "/guides/image-classification-in-tensorflow/", "contributor": null}], "parent": "gradio", "prev_obj": "Video", "next_obj": "Progress"}, "progress": {"class": null, "name": "Progress", "description": "The Progress class provides a custom progress tracker that is used in a function signature. To attach a Progress tracker to a function, simply add a parameter right after the input parameters that has a default value set to a `gradio.Progress()` instance. The Progress tracker can then be updated in the function by calling the Progress object or using the `tqdm` method on an Iterable. The Progress tracker is currently only available with `queue()`.", "tags": {"demos": "progress"}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "track_tqdm", "annotation": "bool", "doc": "If True, the Progress object will track any tqdm.tqdm iterations with the tqdm library in the function.", "default": "False"}], "returns": {"annotation": null}, "example": "import gradio as gr\nimport time\ndef my_function(x, progress=gr.Progress()):\n    progress(0, desc=\"Starting...\")\n    time.sleep(1)\n    for i in progress.tqdm(range(100)):\n        time.sleep(0.1)\n    return x\ngr.Interface(my_function, gr.Textbox(), gr.Textbox()).queue().launch()", "fns": [{"fn": null, "name": "__call__", "description": "Updates progress tracker with progress and message text.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "progress", "annotation": "float | Tuple[int, int | None] | None", "doc": "If float, should be between 0 and 1 representing completion. If Tuple, first number represents steps completed, and second value represents total steps or None if unknown. If None, hides progress bar."}, {"name": "desc", "annotation": "str | None", "doc": "description to display.", "default": "None"}, {"name": "total", "annotation": "int | None", "doc": "estimated total number of steps.", "default": "None"}, {"name": "unit", "annotation": "str", "doc": "unit of iterations.", "default": "\"steps\""}], "returns": {}, "example": null, "parent": "gradio.Progress"}, {"fn": null, "name": "tqdm", "description": "Attaches progress tracker to iterable, like tqdm.", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "iterable", "annotation": "Iterable | None", "doc": "iterable to attach progress tracker to."}, {"name": "desc", "annotation": "str | None", "doc": "description to display.", "default": "None"}, {"name": "total", "annotation": "int | None", "doc": "estimated total number of steps.", "default": "None"}, {"name": "unit", "annotation": "str", "doc": "unit of iterations.", "default": "\"steps\""}, {"name": "args", "annotation": "<class 'inspect._empty'>", "doc": null}], "returns": {}, "example": null, "parent": "gradio.Progress"}], "demos": [["progress", "import gradio as gr\nimport random\nimport time\nimport tqdm\nfrom datasets import load_dataset\nimport shutil\nfrom uuid import uuid4\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text = gr.Textbox()\n        textb = gr.Textbox()\n    with gr.Row():\n        load_set_btn = gr.Button(\"Load Set\")\n        load_nested_set_btn = gr.Button(\"Load Nested Set\")\n        load_random_btn = gr.Button(\"Load Random\")\n        clean_imgs_btn = gr.Button(\"Clean Images\")\n        wait_btn = gr.Button(\"Wait\")\n        do_all_btn = gr.Button(\"Do All\")\n        track_tqdm_btn = gr.Button(\"Bind TQDM\")\n        bind_internal_tqdm_btn = gr.Button(\"Bind Internal TQDM\")\n\n    text2 = gr.Textbox()\n\n    # track list\n    def load_set(text, text2, progress=gr.Progress()):\n        imgs = [None] * 24\n        for img in progress.tqdm(imgs, desc=\"Loading from list\"):\n            time.sleep(0.1)\n        return \"done\"\n    load_set_btn.click(load_set, [text, textb], text2)\n\n    # track nested list\n    def load_nested_set(text, text2, progress=gr.Progress()):\n        imgs = [[None] * 8] * 3\n        for img_set in progress.tqdm(imgs, desc=\"Nested list\"):\n            time.sleep(2)\n            for img in progress.tqdm(img_set, desc=\"inner list\"):\n                time.sleep(0.1)\n        return \"done\"\n    load_nested_set_btn.click(load_nested_set, [text, textb], text2)\n\n    # track iterable of unknown length\n    def load_random(data, progress=gr.Progress()):\n        def yielder():\n            for i in range(0, random.randint(15, 20)):\n                time.sleep(0.1)\n                yield None\n        for img in progress.tqdm(yielder()):\n            pass\n        return \"done\"\n    load_random_btn.click(load_random, {text, textb}, text2)\n        \n    # manual progress\n    def clean_imgs(text, progress=gr.Progress()):\n        progress(0.2, desc=\"Collecting Images\")\n        time.sleep(1)\n        progress(0.5, desc=\"Cleaning Images\")\n        time.sleep(1.5)\n        progress(0.8, desc=\"Sending Images\")\n        time.sleep(1.5)\n        return \"done\"\n    clean_imgs_btn.click(clean_imgs, text, text2)\n\n    # no progress\n    def wait(text):\n        time.sleep(4)\n        return \"done\"\n    wait_btn.click(wait, text, text2)\n\n    # multiple progressions\n    def do_all(data, progress=gr.Progress()):\n        load_set(data[text], data[textb], progress)\n        load_random(data, progress)\n        clean_imgs(data[text], progress)\n        progress(None)\n        wait(text)\n        return \"done\"\n    do_all_btn.click(do_all, {text, textb}, text2)\n\n    def track_tqdm(data, progress=gr.Progress(track_tqdm=True)):\n        for i in tqdm.tqdm(range(5), desc=\"outer\"):\n            for j in tqdm.tqdm(range(4), desc=\"inner\"):\n                time.sleep(1)\n        return \"done\"\n    track_tqdm_btn.click(track_tqdm, {text, textb}, text2)\n\n    def bind_internal_tqdm(data, progress=gr.Progress(track_tqdm=True)):\n        outdir = \"__tmp/\" + str(uuid4())\n        dataset = load_dataset(\"beans\", split=\"train\", cache_dir=outdir)\n        shutil.rmtree(outdir)\n        return \"done\"\n    bind_internal_tqdm_btn.click(bind_internal_tqdm, {text, textb}, text2)\n\n\nif __name__ == \"__main__\":\n    demo.queue(concurrency_count=20).launch()\n"]], "parent": "gradio", "prev_obj": "Examples", "next_obj": "update"}, "update": {"class": null, "name": "update", "description": "Updates component properties. When a function passed into a Gradio Interface or a Blocks events returns a typical value, it updates the value of the output component. But it is also possible to update the properties of an output component (such as the number of lines of a `Textbox` or the visibility of an `Image`) by returning the component's `update()` function, which takes as parameters any of the constructor parameters for that component. This is a shorthand for using the update method on a component. For example, rather than using gr.Number.update(...) you can just use gr.update(...). Note that your editor's autocompletion will suggest proper parameters if you use the update method on the component. <br>", "tags": {"demos": "blocks_essay, blocks_update, blocks_essay_update", "parameters": "kwargs: Key-word arguments used to update the component's properties."}, "parameters": [{"name": "kwargs", "annotation": "<class 'inspect._empty'>", "doc": "Key-word arguments used to update the component's properties."}], "returns": {"annotation": null}, "example": "# Blocks Example\nimport gradio as gr\nwith gr.Blocks() as demo:\n    radio = gr.Radio([1, 2, 4], label=\"Set the value of the number\")\n    number = gr.Number(value=2, interactive=True)\n    radio.change(fn=lambda value: gr.update(value=value), inputs=radio, outputs=number)\ndemo.launch()\n\n# Interface example\nimport gradio as gr\ndef change_textbox(choice):\n  if choice == \"short\":\n      return gr.Textbox.update(lines=2, visible=True)\n  elif choice == \"long\":\n      return gr.Textbox.update(lines=8, visible=True)\n  else:\n      return gr.Textbox.update(visible=False)\ngr.Interface(\n  change_textbox,\n  gr.Radio(\n      [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\n  ),\n  gr.Textbox(lines=2),\n  live=True,\n).launch()", "fns": [], "demos": [["blocks_essay", "import gradio as gr\n\n\ndef change_textbox(choice):\n    if choice == \"short\":\n        return gr.Textbox.update(lines=2, visible=True)\n    elif choice == \"long\":\n        return gr.Textbox.update(lines=8, visible=True)\n    else:\n        return gr.Textbox.update(visible=False)\n\n\nwith gr.Blocks() as demo:\n    radio = gr.Radio(\n        [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\n    )\n    text = gr.Textbox(lines=2, interactive=True)\n\n    radio.change(fn=change_textbox, inputs=radio, outputs=text)\n\n\nif __name__ == \"__main__\":\n    demo.launch()\n"], ["blocks_update", "import gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n        \"\"\"\n    # Animal Generator\n    Once you select a species, the detail panel should be visible.\n    \"\"\"\n    )\n\n    species = gr.Radio(label=\"Animal Class\", choices=[\"Mammal\", \"Fish\", \"Bird\"])\n    animal = gr.Dropdown(label=\"Animal\", choices=[])\n\n    with gr.Column(visible=False) as details_col:\n        weight = gr.Slider(0, 20)\n        details = gr.Textbox(label=\"Extra Details\")\n        generate_btn = gr.Button(\"Generate\")\n        output = gr.Textbox(label=\"Output\")\n\n    species_map = {\n        \"Mammal\": [\"Elephant\", \"Giraffe\", \"Hamster\"],\n        \"Fish\": [\"Shark\", \"Salmon\", \"Tuna\"],\n        \"Bird\": [\"Chicken\", \"Eagle\", \"Hawk\"],\n    }\n\n    def filter_species(species):\n        return gr.Dropdown.update(\n            choices=species_map[species], value=species_map[species][1]\n        ), gr.update(visible=True)\n\n    species.change(filter_species, species, [animal, details_col])\n\n    def filter_weight(animal):\n        if animal in (\"Elephant\", \"Shark\", \"Giraffe\"):\n            return gr.update(maximum=100)\n        else:\n            return gr.update(maximum=20)\n\n    animal.change(filter_weight, animal, weight)\n    weight.change(lambda w: gr.update(lines=int(w / 10) + 1), weight, details)\n\n    generate_btn.click(lambda x: x, details, output)\n\n\nif __name__ == \"__main__\":\n    demo.launch()"], ["blocks_essay_update", "import gradio as gr\n\ndef change_textbox(choice):\n    if choice == \"short\":\n        return gr.update(lines=2, visible=True, value=\"Short story: \")\n    elif choice == \"long\":\n        return gr.update(lines=8, visible=True, value=\"Long story...\")\n    else:\n        return gr.update(visible=False)\n\nwith gr.Blocks() as demo:\n    radio = gr.Radio(\n        [\"short\", \"long\", \"none\"], label=\"Essay Length to Write?\"\n    )\n    text = gr.Textbox(lines=2, interactive=True)\n    radio.change(fn=change_textbox, inputs=radio, outputs=text)\n\nif __name__ == \"__main__\":\n    demo.launch()"]], "parent": "gradio", "prev_obj": "Progress", "next_obj": "make_waveform"}, "make_waveform": {"class": null, "name": "make_waveform", "description": "Generates a waveform video from an audio file. Useful for creating an easy to share audio visualization. The output should be passed into a `gr.Video` component.", "tags": {"parameters": "audio: Audio file path or tuple of (sample_rate, audio_data)<br>bg_color: Background color of waveform (ignored if bg_image is provided)<br>bg_image: Background image of waveform<br>fg_alpha: Opacity of foreground waveform<br>bars_color: Color of waveform bars. Can be a single color or a tuple of (start_color, end_color) of gradient<br>bar_count: Number of bars in waveform<br>bar_width: Width of bars in waveform. 1 represents full width, 0.5 represents half width, etc.", "returns": "A filepath to the output video."}, "parameters": [{"name": "audio", "annotation": "str | Tuple[int, np.ndarray]", "doc": "Audio file path or tuple of (sample_rate, audio_data)"}, {"name": "bg_color", "annotation": "str", "doc": "Background color of waveform (ignored if bg_image is provided)", "default": "\"#f3f4f6\""}, {"name": "bg_image", "annotation": "str | None", "doc": "Background image of waveform", "default": "None"}, {"name": "fg_alpha", "annotation": "float", "doc": "Opacity of foreground waveform", "default": "0.75"}, {"name": "bars_color", "annotation": "str | Tuple[str, str]", "doc": "Color of waveform bars. Can be a single color or a tuple of (start_color, end_color) of gradient", "default": "('#fbbf24', '#ea580c')"}, {"name": "bar_count", "annotation": "int", "doc": "Number of bars in waveform", "default": "50"}, {"name": "bar_width", "annotation": "float", "doc": "Width of bars in waveform. 1 represents full width, 0.5 represents half width, etc.", "default": "0.6"}], "returns": {"annotation": null, "doc": "A filepath to the output video."}, "example": null, "fns": [], "parent": "gradio", "prev_obj": "update", "next_obj": "Request"}}, "routes": {"request": {"class": null, "name": "Request", "description": "A Gradio request object that can be used to access the request headers, cookies, query parameters and other information about the request from within the prediction function. The class is a thin wrapper around the fastapi.Request class. Attributes of this class include: `headers`, `client`, `query_params`, and `path_params`,", "tags": {}, "parameters": [{"name": "self", "annotation": "<class 'inspect._empty'>", "doc": null}, {"name": "request", "annotation": "fastapi.Request | None", "doc": "A fastapi.Request", "default": "None"}], "returns": {"annotation": null}, "example": "import gradio as gr\ndef echo(name, request: gr.Request):\n    print(\"Request headers dictionary:\", request.headers)\n    print(\"IP address:\", request.client.host)\n    return name\nio = gr.Interface(echo, \"textbox\", \"textbox\").launch()", "fns": [], "parent": "gradio", "prev_obj": "make_waveform", "next_obj": "mount_gradio_app"}, "mount_gradio_app": {"class": null, "name": "mount_gradio_app", "description": "Mount a gradio.Blocks to an existing FastAPI application. <br>", "tags": {"parameters": "app: The parent FastAPI application.<br>blocks: The blocks object we want to mount to the parent app.<br>path: The path at which the gradio application will be mounted.<br>gradio_api_url: The full url at which the gradio app will run. This is only needed if deploying to Huggingface spaces of if the websocket endpoints of your deployed app are on a different network location than the gradio app. If deploying to spaces, set gradio_api_url to 'http://localhost:7860/'"}, "parameters": [{"name": "app", "annotation": "fastapi.FastAPI", "doc": "The parent FastAPI application."}, {"name": "blocks", "annotation": "gradio.Blocks", "doc": "The blocks object we want to mount to the parent app."}, {"name": "path", "annotation": "str", "doc": "The path at which the gradio application will be mounted."}, {"name": "gradio_api_url", "annotation": "str | None", "doc": "The full url at which the gradio app will run. This is only needed if deploying to Huggingface spaces of if the websocket endpoints of your deployed app are on a different network location than the gradio app. If deploying to spaces, set gradio_api_url to 'http://localhost:7860/'", "default": "None"}], "returns": {"annotation": null}, "example": "from fastapi import FastAPI\nimport gradio as gr\napp = FastAPI()\n@app.get(\"/\")\ndef read_main():\n    return {\"message\": \"This is your main app\"}\nio = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")\napp = gr.mount_gradio_app(app, io, path=\"/gradio\")\n# Then run `uvicorn run:app` from the terminal and navigate to http://localhost:8000/gradio.", "fns": [], "parent": "gradio", "prev_obj": "Request"}}, "events": {}, "ordered_events": ["Change()", "Click()", "Submit()", "Edit()", "Clear()", "Play()", "Pause()", "Stream()", "Blur()", "Upload()"]}, "pages": ["simplecsvlogger", "csvlogger", "huggingfacedatasetsaver", "examples", "progress", "update", "make_waveform", "request", "mount_gradio_app", "queue", "blocks", "row", "column", "tab", "box", "accordion", "audio", "barplot", "button", "chatbot", "checkbox", "checkboxgroup", "colorpicker", "dataframe", "dataset", "dropdown", "file", "gallery", "html", "highlightedtext", "image", "interpretation", "json", "label", "lineplot", "markdown", "model3d", "number", "plot", "radio", "scatterplot", "slider", "state", "textbox", "timeseries", "uploadbutton", "video", "interface", "tabbedinterface", "parallel", "series"]}