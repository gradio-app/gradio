{"guide": {"name": "Gradio-and-Wandb-Integration", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 16, "pretty_name": "Gradio And Wandb Integration", "content": "# Gradio and W&B Integration\n\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\nTags: WANDB, SPACES\nContributed by Gradio team\n\n## Introduction\n\nIn this Guide, we'll walk you through:\n\n* Introduction of Gradio, and Hugging Face Spaces, and Wandb\n* How to setup a Gradio demo using the Wandb integration for JoJoGAN\n* How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\n\nHere's an example of an model trained and experiments tracked on wandb, try out the JoJoGAN demo below.\n\n<iframe src=\"https://akhaliq-jojogan.hf.space\" frameBorder=\"0\" height=\"810\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n## What is Wandb?\n\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:\n\n<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">\n\n\n## What are Hugging Face Spaces & Gradio?\n\n### Gradio\n\nGradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user inferface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\n\nGet started [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces\n\nHugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).\n\n\n## Setting up a Gradio Demo for JoJoGAN\n\nNow, let's walk you through how to do this on your own. We'll make the assumption that you're new to W&B and Gradio for the purposes of this tutorial. \n\nLet's get started!\n\n1. Create a W&B account\n\n    Follow [these quick instructions](https://app.wandb.ai/login) to create your free account if you don\u2019t have one already. It shouldn't take more than a couple minutes. Once you're done (or if you've already got an account), next, we'll run a quick colab. \n\n2. Open Colab Install Gradio and W&B\n\n    We'll be following along with the colab provided in the JoJoGAN repo with some minor modifications to use Wandb and Gradio more effectively. \n\n    [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb)\n\n    Install Gradio and Wandb at the top:\n\n```sh\n\npip install gradio wandb\n```\n\n3. Finetune StyleGAN and W&B experiment tracking\n\n    This next step will open a W&B dashboard to track your experiments and a gradio panel showing pretrained models to choose from a drop down menu from a Gradio Demo hosted on Huggingface Spaces. Here's the code you need for that:\n\n```python\n\nalpha =  1.0 \nalpha = 1-alpha\n\npreserve_color = True \nnum_iter = 100 \nlog_interval = 50 \n\n\nsamples = []\ncolumn_names = [\"Referece (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\n\nwandb.init(project=\"JoJoGAN\")\nconfig = wandb.config\nconfig.num_iter = num_iter\nconfig.preserve_color = preserve_color\nwandb.log(\n{\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\nstep=0)\n\n# load discriminator for perceptual loss\ndiscriminator = Discriminator(1024, 2).eval().to(device)\nckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\ndiscriminator.load_state_dict(ckpt[\"d\"], strict=False)\n\n# reset generator\ndel generator\ngenerator = deepcopy(original_generator)\n\ng_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\n\n# Which layers to swap for generating a family of plausible real images -> fake image\nif preserve_color:\n    id_swap = [9,11,15,16,17]\nelse:\n    id_swap = list(range(7, generator.n_latent))\n\nfor idx in tqdm(range(num_iter)):\n    mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\n    in_latent = latents.clone()\n    in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\n\n    img = generator(in_latent, input_is_latent=True)\n\n    with torch.no_grad():\n        real_feat = discriminator(targets)\n    fake_feat = discriminator(img)\n\n    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\n    \n\n    wandb.log({\"loss\": loss}, step=idx)\n    if idx % log_interval == 0:\n        generator.eval()\n        my_sample = generator(my_w, input_is_latent=True)\n        generator.train()\n        my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\n        wandb.log(\n        {\"Current stylization\": [wandb.Image(my_sample)]},\n        step=idx)\n    table_data = [\n            wandb.Image(transforms.ToPILImage()(target_im)),\n            wandb.Image(img),\n            wandb.Image(my_sample),\n        ]\n    samples.append(table_data)\n\n    g_optim.zero_grad()\n    loss.backward()\n    g_optim.step()\n\nout_table = wandb.Table(data=samples, columns=column_names)\nwandb.log({\"Current Samples\": out_table})\n```\n\n4. Save, Download, and Load Model\n\n    Here's how to save and download your model.\n\n```python\n\nfrom PIL import Image\nimport torch\ntorch.backends.cudnn.benchmark = True\nfrom torchvision import transforms, utils\nfrom util import *\nimport math\nimport random\nimport numpy as np\nfrom torch import nn, autograd, optim\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nimport lpips\nfrom model import *\nfrom e4e_projection import projection as e4e_projection\n\nfrom copy import deepcopy\nimport imageio\n\nimport os\nimport sys\nimport torchvision.transforms as transforms\nfrom argparse import Namespace\nfrom e4e.models.psp import pSp\nfrom util import *\nfrom huggingface_hub import hf_hub_download\nfrom google.colab import files\n\ntorch.save({\"g\": generator.state_dict()}, \"your-model-name.pt\")\n\nfiles.download('your-model-name.pt') \n\nlatent_dim = 512\ndevice=\"cuda\"\nmodel_path_s = hf_hub_download(repo_id=\"akhaliq/jojogan-stylegan2-ffhq-config-f\", filename=\"stylegan2-ffhq-config-f.pt\")\noriginal_generator = Generator(1024, latent_dim, 8, 2).to(device)\nckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)\noriginal_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\nmean_latent = original_generator.mean_latent(10000)\n\ngenerator = deepcopy(original_generator)\n\nckpt = torch.load(\"/content/JoJoGAN/your-model-name.pt\", map_location=lambda storage, loc: storage)\ngenerator.load_state_dict(ckpt[\"g\"], strict=False)\ngenerator.eval()\n\nplt.rcParams['figure.dpi'] = 150\n\n\n\ntransform = transforms.Compose(\n    [\n        transforms.Resize((1024, 1024)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\n\n\ndef inference(img):  \n    img.save('out.jpg')  \n    aligned_face = align_face('out.jpg')\n\n    my_w = e4e_projection(aligned_face, \"out.pt\", device).unsqueeze(0)  \n    with torch.no_grad():\n        my_sample = generator(my_w, input_is_latent=True)\n            \n    \n    npimage = my_sample[0].cpu().permute(1, 2, 0).detach().numpy()\n    imageio.imwrite('filename.jpeg', npimage)\n    return 'filename.jpeg'\n```\n\n5. Build a Gradio Demo\n\n```python\n\nimport gradio as gr\n\ntitle = \"JoJoGAN\"\ndescription = \"Gradio Demo for JoJoGAN: One Shot Face Stylization. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\n\ndemo = gr.Interface(\n    inference, \n    gr.Image(type=\"pil\"), \n    gr.Image(type=\"file\"),\n    title=title,\n    description=description\n)\n\ndemo.launch(share=True)\n```\n\n6. Integrate Gradio into your W&B Dashboard\n\n    The last step\u2014integrating your Gradio demo with your W&B dashboard\u2014is just one extra line:\n\n```python\n\ndemo.integrate(wandb=wandb)\n```\n\n    Once you call integrate, a demo will be created and you can integrate it into your dashboard or report\n\n    Outside of W&B with Web components, using the gradio-app tags allows anyone can embed Gradio demos on HF spaces directly into their blogs, websites, documentation, etc.:\n\n```html\n\n<gradio-app space=\"akhaliq/JoJoGAN\"> </gradio-app>\n```\n\n\n7. (Optional) Embed W&B plots in your Gradio App\n\n    It's also possible to embed W&B plots within Gradio apps. To do so, you can create a W&B Report of your plots and \n    embed them within your Gradio app within a `gr.HTML` block. \n\n    The Report will need to be public and you will need to wrap the URL within an iFrame like this: \n```python\n\nimport gradio as gr\n\ndef wandb_report(url):\n    iframe = f'<iframe src={url} style=\"border:none;height:1024px;width:100%\">'\n    return gr.HTML(iframe)\n\nwith gr.Blocks() as demo:\n    report_url = 'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx'\n    report = wandb_report(report_url)\n\ndemo.launch(share=True)\n```\n\n\n## Conclusion\n\nWe hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! Thanks for making it to the end. To recap:\n\n* Only one single reference image is needed for fine-tuning JoJoGAN which usually takes about 1 minute on a GPU in colab. After training, style can be applied to any input image. Read more in the paper.\n\n* W&B tracks experiments with just a few lines of code added to a colab and you can visualize, sort, and understand your experiments in a single, centralized dashboard.\n\n* Gradio, meanwhile, demos the model in a user friendly interface to share anywhere on the web. \n\n## How to contribute Gradio demos on HF spaces on the Wandb organization\n\n* Create an account on Hugging Face [here](https://huggingface.co/join).\n* Add Gradio Demo under your username, see this [course](https://huggingface.co/course/chapter9/4?fw=pt) for setting up Gradio Demo on Hugging Face. \n* Request to join wandb organization [here](https://huggingface.co/wandb).\n* Once approved transfer model from your username to Wandb organization\n", "html": "<h1 id=\"gradio-and-wb-integration\">Gradio and W&amp;B Integration</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>In this Guide, we'll walk you through:</p>\n\n<ul>\n<li>Introduction of Gradio, and Hugging Face Spaces, and Wandb</li>\n<li>How to setup a Gradio demo using the Wandb integration for JoJoGAN</li>\n<li>How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face</li>\n</ul>\n\n<p>Here's an example of an model trained and experiments tracked on wandb, try out the JoJoGAN demo below.</p>\n\n<iframe src=\"https://akhaliq-jojogan.hf.space\" frameBorder=\"0\" height=\"810\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"what-is-wandb\">What is Wandb?</h2>\n\n<p>Weights and Biases (W&amp;B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:</p>\n\n<p><img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\"></p>\n\n<h2 id=\"what-are-hugging-face-spaces-gradio\">What are Hugging Face Spaces &amp; Gradio?</h2>\n\n<h3 id=\"gradio\">Gradio</h3>\n\n<p>Gradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user inferface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.</p>\n\n<p>Get started <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/getting_started\">here</a></p>\n\n<h3 id=\"hugging-face-spaces\">Hugging Face Spaces</h3>\n\n<p>Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/launch\">here</a>.</p>\n\n<h2 id=\"setting-up-a-gradio-demo-for-jojogan\">Setting up a Gradio Demo for JoJoGAN</h2>\n\n<p>Now, let's walk you through how to do this on your own. We'll make the assumption that you're new to W&amp;B and Gradio for the purposes of this tutorial. </p>\n\n<p>Let's get started!</p>\n\n<ol>\n<li><p>Create a W&amp;B account</p>\n\n<p>Follow <a rel=\"noopener\" target=\"_blank\" href=\"https://app.wandb.ai/login\">these quick instructions</a> to create your free account if you don\u2019t have one already. It shouldn't take more than a couple minutes. Once you're done (or if you've already got an account), next, we'll run a quick colab. </p></li>\n<li><p>Open Colab Install Gradio and W&amp;B</p>\n\n<p>We'll be following along with the colab provided in the JoJoGAN repo with some minor modifications to use Wandb and Gradio more effectively. </p>\n\n<p><a rel=\"noopener\" target=\"_blank\" href=\"https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" /></a></p>\n\n<p>Install Gradio and Wandb at the top:</p></li>\n</ol>\n\n<div class='codeblock'><pre><code class='lang-sh'>\npip install gradio wandb\n</code></pre></div>\n\n<ol>\n<li><p>Finetune StyleGAN and W&amp;B experiment tracking</p>\n\n<p>This next step will open a W&amp;B dashboard to track your experiments and a gradio panel showing pretrained models to choose from a drop down menu from a Gradio Demo hosted on Huggingface Spaces. Here's the code you need for that:</p></li>\n</ol>\n\n<div class='codeblock'><pre><code class='lang-python'>\nalpha =  1.0 \nalpha = 1-alpha\n\npreserve_color = True \nnum_iter = 100 \nlog_interval = 50 \n\n\nsamples = []\ncolumn_names = [\"Referece (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\n\nwandb.init(project=\"JoJoGAN\")\nconfig = wandb.config\nconfig.num_iter = num_iter\nconfig.preserve_color = preserve_color\nwandb.log(\n{\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\nstep=0)\n\n# load discriminator for perceptual loss\ndiscriminator = Discriminator(1024, 2).eval().to(device)\nckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\ndiscriminator.load_state_dict(ckpt[\"d\"], strict=False)\n\n# reset generator\ndel generator\ngenerator = deepcopy(original_generator)\n\ng_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\n\n# Which layers to swap for generating a family of plausible real images -> fake image\nif preserve_color:\n    id_swap = [9,11,15,16,17]\nelse:\n    id_swap = list(range(7, generator.n_latent))\n\nfor idx in tqdm(range(num_iter)):\n    mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\n    in_latent = latents.clone()\n    in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\n\n    img = generator(in_latent, input_is_latent=True)\n\n    with torch.no_grad():\n        real_feat = discriminator(targets)\n    fake_feat = discriminator(img)\n\n    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\n\n\n    wandb.log({\"loss\": loss}, step=idx)\n    if idx % log_interval == 0:\n        generator.eval()\n        my_sample = generator(my_w, input_is_latent=True)\n        generator.train()\n        my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\n        wandb.log(\n        {\"Current stylization\": [wandb.Image(my_sample)]},\n        step=idx)\n    table_data = [\n            wandb.Image(transforms.ToPILImage()(target_im)),\n            wandb.Image(img),\n            wandb.Image(my_sample),\n        ]\n    samples.append(table_data)\n\n    g_optim.zero_grad()\n    loss.backward()\n    g_optim.step()\n\nout_table = wandb.Table(data=samples, columns=column_names)\nwandb.log({\"Current Samples\": out_table})\n</code></pre></div>\n\n<ol>\n<li><p>Save, Download, and Load Model</p>\n\n<p>Here's how to save and download your model.</p></li>\n</ol>\n\n<div class='codeblock'><pre><code class='lang-python'>\nfrom PIL import Image\nimport torch\ntorch.backends.cudnn.benchmark = True\nfrom torchvision import transforms, utils\nfrom util import *\nimport math\nimport random\nimport numpy as np\nfrom torch import nn, autograd, optim\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nimport lpips\nfrom model import *\nfrom e4e_projection import projection as e4e_projection\n\nfrom copy import deepcopy\nimport imageio\n\nimport os\nimport sys\nimport torchvision.transforms as transforms\nfrom argparse import Namespace\nfrom e4e.models.psp import pSp\nfrom util import *\nfrom huggingface_hub import hf_hub_download\nfrom google.colab import files\n\ntorch.save({\"g\": generator.state_dict()}, \"your-model-name.pt\")\n\nfiles.download('your-model-name.pt') \n\nlatent_dim = 512\ndevice=\"cuda\"\nmodel_path_s = hf_hub_download(repo_id=\"akhaliq/jojogan-stylegan2-ffhq-config-f\", filename=\"stylegan2-ffhq-config-f.pt\")\noriginal_generator = Generator(1024, latent_dim, 8, 2).to(device)\nckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)\noriginal_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\nmean_latent = original_generator.mean_latent(10000)\n\ngenerator = deepcopy(original_generator)\n\nckpt = torch.load(\"/content/JoJoGAN/your-model-name.pt\", map_location=lambda storage, loc: storage)\ngenerator.load_state_dict(ckpt[\"g\"], strict=False)\ngenerator.eval()\n\nplt.rcParams['figure.dpi'] = 150\n\n\n\ntransform = transforms.Compose(\n    [\n        transforms.Resize((1024, 1024)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\n\n\ndef inference(img):  \n    img.save('out.jpg')  \n    aligned_face = align_face('out.jpg')\n\n    my_w = e4e_projection(aligned_face, \"out.pt\", device).unsqueeze(0)  \n    with torch.no_grad():\n        my_sample = generator(my_w, input_is_latent=True)\n\n\n    npimage = my_sample[0].cpu().permute(1, 2, 0).detach().numpy()\n    imageio.imwrite('filename.jpeg', npimage)\n    return 'filename.jpeg'\n</code></pre></div>\n\n<ol>\n<li>Build a Gradio Demo</li>\n</ol>\n\n<div class='codeblock'><pre><code class='lang-python'>\nimport gradio as gr\n\ntitle = \"JoJoGAN\"\ndescription = \"Gradio Demo for JoJoGAN: One Shot Face Stylization. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\n\ndemo = gr.Interface(\n    inference, \n    gr.Image(type=\"pil\"), \n    gr.Image(type=\"file\"),\n    title=title,\n    description=description\n)\n\ndemo.launch(share=True)\n</code></pre></div>\n\n<ol>\n<li><p>Integrate Gradio into your W&amp;B Dashboard</p>\n\n<p>The last step\u2014integrating your Gradio demo with your W&amp;B dashboard\u2014is just one extra line:</p></li>\n</ol>\n\n<div class='codeblock'><pre><code class='lang-python'>\ndemo.integrate(wandb=wandb)\n</code></pre></div>\n\n<pre><code>Once you call integrate, a demo will be created and you can integrate it into your dashboard or report\n\nOutside of W&amp;B with Web components, using the gradio-app tags allows anyone can embed Gradio demos on HF spaces directly into their blogs, websites, documentation, etc.:\n</code></pre>\n\n<div class='codeblock'><pre><code class='lang-html'>\n<gradio-app space=\"akhaliq/JoJoGAN\"> </gradio-app>\n</code></pre></div>\n\n<ol>\n<li><p>(Optional) Embed W&amp;B plots in your Gradio App</p>\n\n<p>It's also possible to embed W&amp;B plots within Gradio apps. To do so, you can create a W&amp;B Report of your plots and \nembed them within your Gradio app within a <code>gr.HTML</code> block. </p>\n\n<p>The Report will need to be public and you will need to wrap the URL within an iFrame like this: </p></li>\n</ol>\n\n<div class='codeblock'><pre><code class='lang-python'>\nimport gradio as gr\n\ndef wandb_report(url):\n    iframe = f'<iframe src={url} style=\"border:none;height:1024px;width:100%\">'\n    return gr.HTML(iframe)\n\nwith gr.Blocks() as demo:\n    report_url = 'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx'\n    report = wandb_report(report_url)\n\ndemo.launch(share=True)\n</code></pre></div>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>We hope you enjoyed this brief demo of embedding a Gradio demo to a W&amp;B report! Thanks for making it to the end. To recap:</p>\n\n<ul>\n<li><p>Only one single reference image is needed for fine-tuning JoJoGAN which usually takes about 1 minute on a GPU in colab. After training, style can be applied to any input image. Read more in the paper.</p></li>\n<li><p>W&amp;B tracks experiments with just a few lines of code added to a colab and you can visualize, sort, and understand your experiments in a single, centralized dashboard.</p></li>\n<li><p>Gradio, meanwhile, demos the model in a user friendly interface to share anywhere on the web. </p></li>\n</ul>\n\n<h2 id=\"how-to-contribute-gradio-demos-on-hf-spaces-on-the-wandb-organization\">How to contribute Gradio demos on HF spaces on the Wandb organization</h2>\n\n<ul>\n<li>Create an account on Hugging Face <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/join\">here</a>.</li>\n<li>Add Gradio Demo under your username, see this <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/course/chapter9/4?fw=pt\">course</a> for setting up Gradio Demo on Hugging Face. </li>\n<li>Request to join wandb organization <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/wandb\">here</a>.</li>\n<li>Once approved transfer model from your username to Wandb organization</li>\n</ul>\n", "tags": ["WANDB", "SPACES"], "spaces": ["https://huggingface.co/spaces/akhaliq/JoJoGAN"], "url": "/guides/Gradio-and-Wandb-Integration/", "contributor": "Gradio team"}}