{"cells": [{"cell_type": "markdown", "id": "302934307671667531413257853548643485645", "metadata": {}, "source": ["# Gradio Demo: llm_langchain"]}, {"cell_type": "code", "execution_count": null, "id": "272996653310673477252411125948039410165", "metadata": {}, "outputs": [], "source": ["!pip install -q gradio langchain langchain-openai"]}, {"cell_type": "code", "execution_count": null, "id": "288918539441861185822528903084949547379", "metadata": {}, "outputs": [], "source": ["# This is a simple general-purpose chatbot built on top of LangChain and Gradio.\n", "# Before running this, make sure you have exported your OpenAI API key as an environment variable:\n", "# export OPENAI_API_KEY=\"your-openai-api-key\"\n", "\n", "from langchain_openai import ChatOpenAI\n", "from langchain.schema import AIMessage, HumanMessage\n", "import gradio as gr\n", "\n", "model = ChatOpenAI(model=\"gpt-4o-mini\")\n", "\n", "def predict(message, history):\n", "    history_langchain_format = []\n", "    for msg in history:\n", "        if msg['role'] == \"user\":\n", "            history_langchain_format.append(HumanMessage(content=msg['content']))\n", "        elif msg['role'] == \"assistant\":\n", "            history_langchain_format.append(AIMessage(content=msg['content']))\n", "    history_langchain_format.append(HumanMessage(content=message))\n", "    gpt_response = model.invoke(history_langchain_format)\n", "    return gpt_response.content\n", "\n", "demo = gr.ChatInterface(\n", "    predict,\n", "    type=\"messages\"\n", ")\n", "\n", "if __name__ == \"__main__\":\n", "    demo.launch()\n"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}